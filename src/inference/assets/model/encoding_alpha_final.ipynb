{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjKToYfc6adY",
    "outputId": "e010d05c-aab8-4947-c72f-255032e61ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24220\\2213573283.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lASrrK4Z6msI",
    "outputId": "a79cb80c-5c32-4ecc-82b2-1689a695c3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50041 entries, 0 to 50040\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   name             50040 non-null  object \n",
      " 1   type             50041 non-null  object \n",
      " 2   price            50041 non-null  float64\n",
      " 3   description      50041 non-null  object \n",
      " 4   manufacturer     49975 non-null  object \n",
      " 5   url              50040 non-null  object \n",
      " 6   parent_category  50041 non-null  object \n",
      " 7   sub_category_1   49294 non-null  object \n",
      " 8   sub_category_2   43948 non-null  object \n",
      " 9   sub_category_3   27955 non-null  object \n",
      " 10  sub_category_4   9788 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\assignment\\alpha2_dataset_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(pd.NA)\n",
    "\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OUzBSTQi6p8v"
   },
   "outputs": [],
   "source": [
    "# stemmer, lemmatizer and stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import Optional\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8martjTt6sdd",
    "outputId": "04595e57-b78b-4702-9291-3d7251bcc274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50041, 13)\n"
     ]
    }
   ],
   "source": [
    "normalization = ['name', 'description']\n",
    "for column in normalization:\n",
    "    df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                                      Duracell - AAA Batteries (4-Pack)\n",
      "type                                                               HardGood\n",
      "price                                                                  5.49\n",
      "description               Compatible with select electronic devices; AAA...\n",
      "manufacturer                                                       Duracell\n",
      "url                                           duracell aaa batteries 4 pack\n",
      "parent_category                                 Connected Home & Housewares\n",
      "sub_category_1                                                   Housewares\n",
      "sub_category_2                                          Household Batteries\n",
      "sub_category_3                                           Alkaline Batteries\n",
      "sub_category_4                                                         <NA>\n",
      "name_normalized                                        duracell aaa battery\n",
      "description_normalized    compatible select electronic device aaa size d...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HYTfY_RArpEX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Connected Home & Housewares' 'other' 'Car Electronics & GPS'\n",
      " 'In-Store Only' 'Musical Instruments' 'Toys' 'Video Games'\n",
      " 'Cameras & Camcorders' 'Computers & Tablets' 'Appliances' 'Audio'\n",
      " 'TV & Home Theater' 'Health' 'Name Brands' 'Cell Phones' 'Movies & Music'\n",
      " 'Magnolia Home Theater' 'Geek Squad' 'Best Buy Gift Cards'\n",
      " 'H/VG_X360/Games/B2G1_20130602' 'MP Exclusives' 'Wearable Technology'\n",
      " 'Custom Parts']\n"
     ]
    }
   ],
   "source": [
    "#df['sub_category_1'].fillna('0', inplace=True)\n",
    "#df['sub_category_2'].fillna('0', inplace=True)\n",
    "#df['sub_category_3'].fillna('0', inplace=True)\n",
    "#df['sub_category_4'].fillna('0', inplace=True)\n",
    "print(df['parent_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwLSa6uB6upR",
    "outputId": "3b01d80e-984e-4e8f-ffb0-73b48cd31694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "print(df.head())\n",
    "df.shape\n",
    "X = df.drop(columns=['parent_category'])\n",
    "y = df['parent_category']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "with h5py.File('label_encoder.h5', 'w') as hf:\n",
    "    hf.create_dataset('label_encoder', data=label_encoder.classes_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "woBvshjI-gHS"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "X_1 = df.drop(columns=['sub_category_1'])\n",
    "y_1 = df['sub_category_1']\n",
    "y_1.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder_1 = LabelEncoder()\n",
    "y_1_encoded = label_encoder_1.fit_transform(y_1)\n",
    "\n",
    "with h5py.File('label_encoder_1.h5', 'w') as hf:\n",
    "    hf.create_dataset('label_encoder_1', data=label_encoder_1.classes_)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wYMN3-Aj-f-n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_2 = df.drop(columns=['sub_category_2'])\n",
    "y_2 = df['sub_category_2']\n",
    "y_2.fillna('missing', inplace=True)\n",
    "label_encoder_2 = LabelEncoder()\n",
    "y_2_encoded = label_encoder_2.fit_transform(y_2)\n",
    "\n",
    "with h5py.File('label_encoder_2.h5', 'w') as hf:\n",
    "    hf.create_dataset('label_encoder_2', data=label_encoder_2.classes_)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sLrnJPS1-f3n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_3 = df.drop(columns=['sub_category_3'])\n",
    "y_3 = df['sub_category_3']\n",
    "y_3.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder_3 = LabelEncoder()\n",
    "y_3_encoded = label_encoder_3.fit_transform(y_3)\n",
    "\n",
    "with h5py.File('label_encoder_3.h5', 'w') as hf:\n",
    "    hf.create_dataset('label_encoder_3', data=label_encoder_3.classes_)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MmO58b37-foH"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_4 = df.drop(columns=['sub_category_4'])\n",
    "y_4 = df['sub_category_4']\n",
    "y_4.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder_4 = LabelEncoder()\n",
    "y_4_encoded = label_encoder_4.fit_transform(y_4)\n",
    "\n",
    "with h5py.File('label_encoder_4.h5', 'w') as hf:\n",
    "    hf.create_dataset('label_encoder_4', data=label_encoder_4.classes_)\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4, y_4_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxJvoJcBsGfp",
    "outputId": "8064db2d-ac1c-4896-9349-64285425f5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "print(X_4.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_ArCp4U6xB_",
    "outputId": "d5cd1931-8036-4eea-de8f-8f3daa08bcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40032, 12)\n",
      "(10009, 12)\n",
      "(40032,)\n",
      "(10009,)\n",
      "9233                            star fox preowned nintendo\n",
      "25631    pioneer networkready ultra hd passthrough av h...\n",
      "19030                     evolve ultimate edition xbox one\n",
      "12044    joby pro series ultraplate quickrelease plate ...\n",
      "18967    aluratek bump w home audio speaker system ipod...\n",
      "                               ...                        \n",
      "11284    samsung class diag lead curved smart ultra hd ...\n",
      "44732    hifonics brutus class mono mosfet subwoofer am...\n",
      "38158    mobile edge premium laptop backpack apple macb...\n",
      "860                                 presonus presonus gray\n",
      "15795      insignia portable bluetooth stereo speaker blue\n",
      "Name: name_normalized, Length: 40032, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train['name_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXktfLf6y1M",
    "outputId": "4b9e8a29-b99b-4ab7-f153-2346eeee9663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  1, 20, ...,  6, 16,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdo3ct0S6zga"
   },
   "source": [
    "One Hot Encoder y Scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMYODyT5cLA"
   },
   "source": [
    "Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHamdvAY64SL",
    "outputId": "c312cd27-8ae2-4f37-bdde-f90f6b6e38ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded: (40032, 2195)\n",
      "Data type of X_train_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded: float64\n",
      "Shape of X_test_encoded: (10009, 2195)\n",
      "Data type of X_test_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded: float64\n",
      "Shape of X_train_scaled: (40032, 1)\n",
      "Data type of elements in X_train_scaled: float64\n",
      "Shape of X_test_scaled: (10009, 1)\n",
      "Data type of elements in X_test_scaled: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "numerical_columns = ['price']\n",
    "text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "\n",
    "# Save the encoder\n",
    "with open('encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n",
    "\n",
    "# Information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns]) \n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Data type of elements in X_train_scaled:\", X_train_scaled.dtype)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14kj_745e3A"
   },
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rrKOHZ_4_AK",
    "outputId": "bd1b3f38-75f9-494e-dbf6-9bd87d1f6832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_1: (40032, 2218)\n",
      "Data type of X_train_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_1: float64\n",
      "Shape of X_test_encoded_1: (10009, 2218)\n",
      "Data type of X_test_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_1: float64\n",
      "Shape of X_train_scaled_1: (40032, 1)\n",
      "Data type of elements in X_train_scaled_1: float64\n",
      "Shape of X_test_scaled_1: (10009, 1)\n",
      "Data type of elements in X_test_scaled_1: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_1 = ['type', 'manufacturer', 'parent_category']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_1[categorical_columns_1] = X_train_1[categorical_columns_1].fillna('missing')\n",
    "X_test_1[categorical_columns_1] = X_test_1[categorical_columns_1].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_1 = encoder_1.fit_transform(X_train_1[categorical_columns_1])\n",
    "X_test_encoded_1 = encoder_1.transform(X_test_1[categorical_columns_1])\n",
    "\n",
    "with open('encoder_1.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder_1, file)\n",
    "\n",
    "\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_1:\", X_train_encoded_1.shape)\n",
    "print(\"Data type of X_train_encoded_1:\", type(X_train_encoded_1))\n",
    "print(\"Data type of elements in X_train_encoded_1:\", X_train_encoded_1.dtype)\n",
    "print(\"Shape of X_test_encoded_1:\", X_test_encoded_1.shape)\n",
    "print(\"Data type of X_test_encoded_1:\", type(X_test_encoded_1))\n",
    "print(\"Data type of elements in X_test_encoded_1:\", X_test_encoded_1.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled_1 = scaler_1.fit_transform(X_train_1[numerical_columns])\n",
    "X_test_scaled_1 = scaler_1.transform(X_test_1[numerical_columns])\n",
    "\n",
    "with open('scaler_1.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_1, file)\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_1:\", X_train_scaled_1.shape)\n",
    "print(\"Data type of elements in X_train_scaled_1:\", X_train_scaled_1.dtype)\n",
    "print(\"Shape of X_test_scaled_1:\", X_test_scaled_1.shape)\n",
    "print(\"Data type of elements in X_test_scaled_1:\", X_test_scaled_1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4uI4css5hqx"
   },
   "source": [
    "Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSlE-ZXJ4-5Q",
    "outputId": "cb5f926f-438b-42f9-ab8b-6c1f9f7ba5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_2: (40032, 2332)\n",
      "Data type of X_train_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_2: float64\n",
      "Shape of X_test_encoded_2: (10009, 2332)\n",
      "Data type of X_test_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_2: float64\n",
      "Shape of X_train_scaled_2: (40032, 1)\n",
      "Data type of elements in X_train_scaled_2: float64\n",
      "Shape of X_test_scaled_2: (10009, 1)\n",
      "Data type of elements in X_test_scaled_2: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_2 = ['type', 'manufacturer', 'parent_category', 'sub_category_1']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_2[categorical_columns_2] = X_train_2[categorical_columns_2].fillna('missing')\n",
    "X_test_2[categorical_columns_2] = X_test_2[categorical_columns_2].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_2 = encoder_2.fit_transform(X_train_2[categorical_columns_2])\n",
    "X_test_encoded_2 = encoder_2.transform(X_test_2[categorical_columns_2])\n",
    "\n",
    "with open('encoder_2.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder_2, file)\n",
    "\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_2:\", X_train_encoded_2.shape)\n",
    "print(\"Data type of X_train_encoded_2:\", type(X_train_encoded_2))\n",
    "print(\"Data type of elements in X_train_encoded_2:\", X_train_encoded_2.dtype)\n",
    "print(\"Shape of X_test_encoded_2:\", X_test_encoded_2.shape)\n",
    "print(\"Data type of X_test_encoded_2:\", type(X_test_encoded_2))\n",
    "print(\"Data type of elements in X_test_encoded_2:\", X_test_encoded_2.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scaled_2 = scaler_2.fit_transform(X_train_2[numerical_columns])\n",
    "X_test_scaled_2 = scaler_2.transform(X_test_2[numerical_columns])\n",
    "\n",
    "with open('scaler_2.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_2, file)\n",
    "\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_2:\", X_train_scaled_2.shape)\n",
    "print(\"Data type of elements in X_train_scaled_2:\", X_train_scaled_2.dtype)\n",
    "print(\"Shape of X_test_scaled_2:\", X_test_scaled_2.shape)\n",
    "print(\"Data type of elements in X_test_scaled_2:\", X_test_scaled_2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjEeJaua5jYm"
   },
   "source": [
    "Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UWrFjTJ4-y7",
    "outputId": "14c5078c-21d3-4614-8393-a3daa5372ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_3: (40032, 2681)\n",
      "Data type of X_train_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_3: float64\n",
      "Shape of X_test_encoded_3: (10009, 2681)\n",
      "Data type of X_test_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_3: float64\n",
      "Shape of X_train_scaled_3: (40032, 1)\n",
      "Data type of elements in X_train_scaled_3: float64\n",
      "Shape of X_test_scaled_3: (10009, 1)\n",
      "Data type of elements in X_test_scaled_3: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_3 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_3[categorical_columns_3] = X_train_3[categorical_columns_3].fillna('missing')\n",
    "X_test_3[categorical_columns_3] = X_test_3[categorical_columns_3].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_3 = encoder_3.fit_transform(X_train_3[categorical_columns_3])\n",
    "X_test_encoded_3 = encoder_3.transform(X_test_3[categorical_columns_3])\n",
    "\n",
    "\n",
    "with open('encoder_3.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder_3, file)\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_3:\", X_train_encoded_3.shape)\n",
    "print(\"Data type of X_train_encoded_3:\", type(X_train_encoded_3))\n",
    "print(\"Data type of elements in X_train_encoded_3:\", X_train_encoded_3.dtype)\n",
    "print(\"Shape of X_test_encoded_3:\", X_test_encoded_3.shape)\n",
    "print(\"Data type of X_test_encoded_3:\", type(X_test_encoded_3))\n",
    "print(\"Data type of elements in X_test_encoded_3:\", X_test_encoded_3.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_3 = StandardScaler()\n",
    "X_train_scaled_3 = scaler_3.fit_transform(X_train_3[numerical_columns])\n",
    "X_test_scaled_3 = scaler_3.transform(X_test_3[numerical_columns])\n",
    "\n",
    "with open('scaler_3.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_3, file)\n",
    "\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_3:\", X_train_scaled_3.shape)\n",
    "print(\"Data type of elements in X_train_scaled_3:\", X_train_scaled_3.dtype)\n",
    "print(\"Shape of X_test_scaled_3:\", X_test_scaled_3.shape)\n",
    "print(\"Data type of elements in X_test_scaled_3:\", X_test_scaled_3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX9LC3rA5k-T"
   },
   "source": [
    "Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E-BaTiE4-rh",
    "outputId": "0fc98aa5-895c-4d65-e0c7-d19d5a80f750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_4: (40032, 2997)\n",
      "Data type of X_train_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_4: float64\n",
      "Shape of X_test_encoded_4: (10009, 2997)\n",
      "Data type of X_test_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_4: float64\n",
      "Shape of X_train_scaled_4: (40032, 1)\n",
      "Data type of elements in X_train_scaled_4: float64\n",
      "Shape of X_test_scaled_4: (10009, 1)\n",
      "Data type of elements in X_test_scaled_4: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_4 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2', 'sub_category_3']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_4[categorical_columns_4] = X_train_4[categorical_columns_4].fillna('missing')\n",
    "X_test_4[categorical_columns_4] = X_test_4[categorical_columns_4].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_4 = encoder_4.fit_transform(X_train_4[categorical_columns_4])\n",
    "X_test_encoded_4 = encoder_4.transform(X_test_4[categorical_columns_4])\n",
    "\n",
    "with open('encoder_4.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder_4, file)\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_4:\", X_train_encoded_4.shape)\n",
    "print(\"Data type of X_train_encoded_4:\", type(X_train_encoded_4))\n",
    "print(\"Data type of elements in X_train_encoded_4:\", X_train_encoded_4.dtype)\n",
    "print(\"Shape of X_test_encoded_4:\", X_test_encoded_4.shape)\n",
    "print(\"Data type of X_test_encoded_4:\", type(X_test_encoded_4))\n",
    "print(\"Data type of elements in X_test_encoded_4:\", X_test_encoded_4.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_4 = StandardScaler()\n",
    "X_train_scaled_4 = scaler_4.fit_transform(X_train_4[numerical_columns])\n",
    "X_test_scaled_4 = scaler_4.transform(X_test_4[numerical_columns])\n",
    "\n",
    "with open('scaler_4.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_4, file)\n",
    "\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_4:\", X_train_scaled_4.shape)\n",
    "print(\"Data type of elements in X_train_scaled_4:\", X_train_scaled_4.dtype)\n",
    "print(\"Shape of X_test_scaled_4:\", X_test_scaled_4.shape)\n",
    "print(\"Data type of elements in X_test_scaled_4:\", X_test_scaled_4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CgYBDhCwHKQA"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Stage 1\n",
    "X_train_processed = hstack([X_train_encoded, X_train_scaled]).astype(np.float32).toarray()\n",
    "X_test_processed = hstack([X_test_encoded, X_test_scaled]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 2\n",
    "X_train_processed_1 = hstack([X_train_encoded_1, X_train_scaled_1]).astype(np.float32).toarray()\n",
    "X_test_processed_1 = hstack([X_test_encoded_1, X_test_scaled_1]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 3\n",
    "X_train_processed_2 = hstack([X_train_encoded_2, X_train_scaled_2]).astype(np.float32).toarray()\n",
    "X_test_processed_2 = hstack([X_test_encoded_2, X_test_scaled_2]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 4\n",
    "X_train_processed_3 = hstack([X_train_encoded_3, X_train_scaled_3]).astype(np.float32).toarray()\n",
    "X_test_processed_3 = hstack([X_test_encoded_3, X_test_scaled_3]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 5\n",
    "X_train_processed_4 = hstack([X_train_encoded_4, X_train_scaled_4]).astype(np.float32).toarray()\n",
    "X_test_processed_4 = hstack([X_test_encoded_4, X_test_scaled_4]).astype(np.float32).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvYoUCeHHjx_",
    "outputId": "61b6f3c1-d6bf-4b4a-fcd5-cc6f7e54ab90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed: float32\n",
      "Data type of X_test_processed: float32\n",
      "X_train_processed shape: (40032, 2196)\n",
      "X_test_processed shape: (10009, 2196)\n"
     ]
    }
   ],
   "source": [
    "# Dim 1\n",
    "print(\"Data type of X_train_processed:\", X_train_processed.dtype)\n",
    "print(\"Data type of X_test_processed:\", X_test_processed.dtype)\n",
    "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhfSkN9I1vK",
    "outputId": "180da8c5-683c-4361-98ce-f51e50cd01ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_1: float32\n",
      "Data type of X_test_processed_1: float32\n",
      "X_train_processed_1 shape: (40032, 2219)\n",
      "X_test_processed_1 shape: (10009, 2219)\n"
     ]
    }
   ],
   "source": [
    "# Dim 2\n",
    "print(\"Data type of X_train_processed_1:\", X_train_processed_1.dtype)\n",
    "print(\"Data type of X_test_processed_1:\", X_test_processed_1.dtype)\n",
    "print(\"X_train_processed_1 shape:\", X_train_processed_1.shape)\n",
    "print(\"X_test_processed_1 shape:\", X_test_processed_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Az3OeCXI1rI",
    "outputId": "3b70f69e-056d-4351-9485-bd9313c99b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_2: float32\n",
      "Data type of X_test_processed_2: float32\n",
      "X_train_processed_2 shape: (40032, 2333)\n",
      "X_test_processed_2 shape: (10009, 2333)\n"
     ]
    }
   ],
   "source": [
    "# Dim 3\n",
    "print(\"Data type of X_train_processed_2:\", X_train_processed_2.dtype)\n",
    "print(\"Data type of X_test_processed_2:\", X_test_processed_2.dtype)\n",
    "print(\"X_train_processed_2 shape:\", X_train_processed_2.shape)\n",
    "print(\"X_test_processed_2 shape:\", X_test_processed_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBSG8gbeI1ng",
    "outputId": "902e994e-05fa-4a26-997f-66ff75d4eef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_3: float32\n",
      "Data type of X_test_processed_3: float32\n",
      "X_train_processed_3 shape: (40032, 2682)\n",
      "X_test_processed_3 shape: (10009, 2682)\n"
     ]
    }
   ],
   "source": [
    "# Dim 4\n",
    "print(\"Data type of X_train_processed_3:\", X_train_processed_3.dtype)\n",
    "print(\"Data type of X_test_processed_3:\",X_test_processed_3.dtype)\n",
    "print(\"X_train_processed_3 shape:\", X_train_processed_3.shape)\n",
    "print(\"X_test_processed_3 shape:\", X_test_processed_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIkzAILmI5Iv",
    "outputId": "945746cf-2be2-42d4-f3d9-49007732e383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_4: float32\n",
      "Data type of X_test_processed_4: float32\n",
      "X_train_processed_4 shape: (40032, 2998)\n",
      "X_test_processed_4 shape: (10009, 2998)\n"
     ]
    }
   ],
   "source": [
    "# Dim 5\n",
    "print(\"Data type of X_train_processed_4:\", X_train_processed_4.dtype)\n",
    "print(\"Data type of X_test_processed_4:\", X_test_processed_4.dtype)\n",
    "print(\"X_train_processed_4 shape:\", X_train_processed_4.shape)\n",
    "print(\"X_test_processed_4 shape:\", X_test_processed_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define file paths to save the arrays\n",
    "file_paths = {\n",
    "    'X_train_processed.npy': X_train_processed,\n",
    "    'X_test_processed.npy': X_test_processed,\n",
    "    'X_train_processed_1.npy': X_train_processed_1,\n",
    "    'X_test_processed_1.npy': X_test_processed_1,\n",
    "    'X_train_processed_2.npy': X_train_processed_2,\n",
    "    'X_test_processed_2.npy': X_test_processed_2,\n",
    "    'X_train_processed_3.npy': X_train_processed_3,\n",
    "    'X_test_processed_3.npy': X_test_processed_3,\n",
    "    'X_train_processed_4.npy': X_train_processed_4,\n",
    "    'X_test_processed_4.npy': X_test_processed_4\n",
    "}\n",
    "\n",
    "# Save each array\n",
    "for file_name, array in file_paths.items():\n",
    "    np.save(file_name, array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EVfWlKtNKCOR"
   },
   "outputs": [],
   "source": [
    "X_train_name_embeddings_loaded = np.load('X_train_name_embeddings.npy', allow_pickle= True)\n",
    "X_train_description_embeddings_loaded = np.load('X_train_description_embeddings.npy',allow_pickle= True)\n",
    "X_test_name_embeddings_loaded = np.load('X_test_name_embeddings.npy', allow_pickle= True)\n",
    "X_test_description_embeddings_loaded = np.load('X_test_description_embeddings.npy', allow_pickle= True)\n",
    "\n",
    "# Load the saved NumPy arrays\n",
    "X_train_processed_loaded = np.load('X_train_processed.npy', allow_pickle=True)\n",
    "X_test_processed_loaded = np.load('X_test_processed.npy', allow_pickle=True)\n",
    "X_train_processed_1_loaded = np.load('X_train_processed_1.npy', allow_pickle=True)\n",
    "X_test_processed_1_loaded = np.load('X_test_processed_1.npy', allow_pickle=True)\n",
    "X_train_processed_2_loaded = np.load('X_train_processed_2.npy', allow_pickle=True)\n",
    "X_test_processed_2_loaded = np.load('X_test_processed_2.npy', allow_pickle=True)\n",
    "X_train_processed_3_loaded = np.load('X_train_processed_3.npy', allow_pickle=True)\n",
    "X_test_processed_3_loaded = np.load('X_test_processed_3.npy', allow_pickle=True)\n",
    "X_train_processed_4_loaded = np.load('X_train_processed_4.npy', allow_pickle=True)\n",
    "X_test_processed_4_loaded = np.load('X_test_processed_4.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name_last_hidden = np.array([x[0][-1] for x in X_train_name_embeddings_loaded])\n",
    "X_train_description_last_hidden = np.array([x[0][-1] for x in X_train_description_embeddings_loaded])\n",
    "X_test_name_last_hidden = np.array([x[0][-1] for x in X_test_name_embeddings_loaded])\n",
    "X_test_description_last_hidden = np.array([x[0][-1] for x in X_test_description_embeddings_loaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of X_train_name_last_hidden: [ 6.20537519e-01 -1.57028392e-01 -4.39105332e-01  5.87245941e-01\n",
      " -4.92567480e-01 -6.75922573e-01  3.66373003e-01 -7.91122556e-01\n",
      "  6.63730741e-01  4.76211309e-02  4.97472771e-02 -3.77824754e-01\n",
      " -5.29451743e-02 -3.52970883e-03 -7.33473897e-01 -2.74465412e-01\n",
      " -8.19092095e-02 -1.52533606e-01  7.26244077e-02 -4.86062840e-02\n",
      "  3.07591200e-01 -1.32735014e-01  8.47112298e-01  2.39423364e-01\n",
      "  1.34860486e-01  3.51377934e-01 -4.78440404e-01 -1.80202276e-02\n",
      " -2.26639926e-01 -3.52599382e-01 -6.21600211e-01 -2.67526209e-01\n",
      " -1.92636084e-02  3.99675339e-01  1.55551210e-01 -8.61436129e-02\n",
      "  3.39647025e-01 -4.60110493e-02 -5.71235001e-01 -4.14561182e-01\n",
      " -3.34018916e-01 -1.61173679e-02 -1.93653673e-01  6.32311583e-01\n",
      "  5.47741950e-02 -6.00057304e-01  6.22461319e-01  3.42181236e-01\n",
      " -2.25496083e-01  4.89310294e-01  2.93806463e-01  1.60992384e-01\n",
      " -7.31535181e-02  8.67539346e-02  5.76151237e-02  1.21880889e-01\n",
      "  2.94119507e-01 -4.34998453e-01  1.54902250e-01  2.93051004e-01\n",
      " -5.07322550e-02  6.77793384e-01 -2.17716694e-01 -1.94208890e-01\n",
      "  6.51268899e-01 -1.67188928e-01  2.17404403e-02 -3.18291545e-01\n",
      " -6.07384741e-01 -3.05744082e-01 -2.39384830e-01 -1.02852356e+00\n",
      "  6.55281067e-01  3.35646331e-01  3.62706840e-01  2.41505504e-01\n",
      " -3.31891775e-01  7.45274603e-01  1.18391871e-01  3.59835505e-01\n",
      "  3.76000077e-01 -3.06956947e-01 -1.38037503e-01  7.29502961e-02\n",
      "  5.02546012e-01  1.56718288e-02 -2.42989823e-01  4.20575216e-02\n",
      " -4.59762454e-01 -1.26091674e-01  3.46461594e-01  2.71966606e-01\n",
      "  3.90934289e-01 -3.59424323e-01 -1.19082555e-01  3.55039179e-01\n",
      " -6.93194717e-02 -9.02826637e-02 -2.01892227e-01 -1.14928253e-01\n",
      " -3.77589911e-01 -1.72685519e-01 -5.20075373e-02  8.87612224e-01\n",
      " -1.62285715e-01 -2.95471847e-01  4.46627766e-01  5.57538629e-01\n",
      "  2.56792873e-01  1.03553367e+00  5.43226182e-01  2.22238839e-01\n",
      "  1.73543707e-01 -6.79688826e-02 -4.89283442e-01 -5.52646935e-01\n",
      "  2.85861969e-01  2.83273697e-01  3.80976677e-01  2.35821024e-01\n",
      " -6.67967856e-01 -4.66061831e-01  5.99375069e-01  1.08154857e+00\n",
      " -1.25421882e-01 -3.18098664e-02  2.50165947e-02 -7.18430936e-01\n",
      "  9.47480053e-02 -7.66403854e-01 -3.98136169e-01  6.48566902e-01\n",
      "  2.57072091e-01  8.71164203e-01 -1.05473615e-01  2.04653174e-01\n",
      " -3.08098674e-01  5.59956357e-02 -6.77700162e-01  1.01367123e-01\n",
      " -3.72281134e-01  9.31154370e-01  6.59990549e-01 -1.15582418e+00\n",
      "  5.36541790e-02  6.25010610e-01  5.59506238e-01 -3.16040702e-02\n",
      "  4.84125316e-02 -3.06834847e-01  7.54427016e-01 -1.69089168e-01\n",
      " -2.54145682e-01 -2.30913237e-02 -5.35159945e-01  2.63796747e-01\n",
      " -1.52081802e-01  1.89791620e-02  4.29030150e-01  7.86521673e-01\n",
      "  2.92653799e-01  3.41052055e-01  3.65956217e-01  5.37699759e-01\n",
      " -5.17349124e-01  2.24996656e-01 -1.17838478e+00 -1.76467597e-01\n",
      "  1.01287454e-01  4.32819426e-01 -4.23639059e-01 -4.54228640e-01\n",
      "  7.73141310e-02  3.79181325e-01 -3.45093966e-01  3.52966428e-01\n",
      " -1.62603110e-01  8.23184326e-02 -2.84529865e-01 -6.81628287e-01\n",
      " -8.46690559e+00 -2.35476375e-01 -9.76541191e-02  2.22685993e-01\n",
      " -3.64512168e-02 -3.91305238e-01 -8.25014532e-01 -1.73412472e-01\n",
      "  3.74747545e-01 -8.75730038e-01 -1.22479826e-01 -1.77380100e-01\n",
      " -9.63426471e-01  8.22258413e-01  3.41257453e-01 -2.38749608e-01\n",
      "  2.57653482e-02  2.47886866e-01 -3.30268562e-01  3.26861292e-01\n",
      " -1.97851792e-01 -3.42524916e-01  9.84942913e-02  6.40032470e-01\n",
      " -2.42812037e-01 -1.43791389e+00  2.46120483e-01 -2.29564041e-01\n",
      "  5.11401772e-01  5.85837178e-02 -1.31213963e+00 -1.12423360e-01\n",
      " -3.62519175e-03 -7.86731243e-01  1.56666100e-01 -5.34660220e-01\n",
      "  4.07156013e-02 -5.64395070e-01 -9.06348109e-01 -2.95303822e-01\n",
      " -5.59946835e-01 -3.20167810e-01 -1.84398927e-02  2.15938147e-02\n",
      "  5.29996753e-01 -1.46843731e+00  3.76851022e-01  7.21755743e-01\n",
      "  6.36393428e-01  2.52907544e-01 -8.59751627e-02 -1.71806812e-01\n",
      "  7.80056894e-01  1.51630104e-01  2.96293616e-01  1.60506576e-01\n",
      " -6.33846670e-02 -5.69900796e-02 -5.88919036e-02 -6.84572160e-01\n",
      " -7.04938546e-04  2.46744156e-01  2.25677311e-01 -8.23454112e-02\n",
      " -5.27979791e-01 -5.08032322e-01 -1.84435844e-01  9.23074484e-01\n",
      "  7.35809445e-01 -4.10479635e-01 -2.21967623e-01 -5.50180316e-01\n",
      "  3.03729922e-01 -4.96013671e-01  5.50130188e-01  7.58742869e-01\n",
      "  5.94226606e-02 -4.39812601e-01  1.16967094e+00 -5.56134701e-01\n",
      "  4.20879245e-01  7.08146334e-01  3.12095106e-01  1.29798785e-01\n",
      " -6.85978770e-01  6.66718185e-01  4.30409670e-01  2.85361290e-01\n",
      " -3.43734175e-01  3.40557098e-01  4.58756328e-01 -1.30768120e-01\n",
      " -4.67268795e-01  5.87800920e-01 -1.30464152e-01 -1.07199299e+00\n",
      "  6.74340010e-01 -8.66399184e-02  3.83873284e-01 -1.83765680e-01\n",
      " -3.93801302e-01  2.69577444e-01 -5.39098233e-02  1.67211309e-01\n",
      "  2.02100113e-01 -4.57387537e-01 -8.52275074e-01 -1.42785877e-01\n",
      "  3.35738182e-01 -6.06438577e-01 -1.52843416e-01 -3.03091884e-01\n",
      " -8.47115964e-02  6.67190492e-01 -6.68784678e-02  6.33220226e-02\n",
      "  5.23253739e-01 -1.75359398e-02 -4.24083918e-01 -2.04071775e-01\n",
      "  2.14214072e-01 -7.02822745e-01  1.12920888e-01 -3.13348323e-02\n",
      " -2.11466819e-01 -1.50750086e-01  4.50119257e-01  1.24140903e-01\n",
      "  7.55189002e-01 -1.10170841e-01  2.85554498e-01 -6.85934961e-01\n",
      "  2.90729553e-01 -1.57349512e-01 -2.48254389e-02  7.73136169e-02\n",
      " -1.95251510e-01 -1.05724163e-01  1.81615889e-01 -4.38701361e-01\n",
      "  2.71253109e-01  2.69789636e-01  1.97617477e-03  4.88260210e-01\n",
      " -6.08754009e-02 -6.35233000e-02 -8.48654211e-01 -4.67859924e-01\n",
      " -4.04063538e-02  3.92924100e-01  4.71455455e-01 -1.16113812e-01\n",
      "  6.34941638e-01 -2.62468249e-01 -4.95653987e-01 -3.08744982e-02\n",
      "  3.34599704e-01 -8.20729434e-02 -3.69854458e-02 -4.42685336e-01\n",
      " -5.49810231e-01  7.30598047e-02  2.70330966e-01 -1.09698489e-01\n",
      " -2.68085241e-01  5.26564658e-01  2.56138742e-01 -1.54329166e-01\n",
      "  2.68264174e-01  6.50385201e-01  1.24321252e-01 -8.18036258e-01\n",
      " -7.96757817e-01 -3.77051294e-01 -2.34010071e-01 -4.08915311e-01\n",
      " -4.76937480e-02  3.31464559e-01  7.78878927e-01  2.71527432e-02\n",
      " -3.22321892e-01  6.98770434e-02 -4.85345066e-01 -7.42784023e-01\n",
      " -4.95838150e-02  2.48317659e-01 -6.83598459e-01 -2.63965130e-01\n",
      " -3.53393927e-02 -4.28863764e-01  3.14770162e-01  1.37096956e-01\n",
      " -3.14997852e-01  8.23536664e-02  2.95320004e-02  5.10599613e-01\n",
      " -3.71191561e-01 -7.13964030e-02  2.93738581e-02 -3.19731176e-01\n",
      " -5.60358286e-01 -6.47917867e-01 -4.73039091e-01  1.92232206e-01\n",
      " -4.33800578e-01  1.69615537e-01  2.53361315e-01  1.28410399e-01\n",
      "  2.39444315e-01 -5.11149056e-02  2.00972959e-01  4.52268481e-01\n",
      " -9.15191919e-02  7.91754797e-02  6.82296529e-02 -2.93338329e-01\n",
      "  4.06531304e-01 -4.03168797e-01  3.09270024e-01  6.50663003e-02\n",
      "  1.05046615e-01 -8.64279717e-02 -1.53797373e-01  4.02077079e-01\n",
      "  5.03367066e-01  7.63851777e-02  2.09626377e-01  1.53182715e-01\n",
      " -5.65243840e-01 -1.32839739e-01  6.27765238e-01 -3.82222414e-01\n",
      " -3.88237000e-01  4.48667586e-01 -4.94298071e-01 -4.10451323e-01\n",
      " -4.89565134e-01 -4.12086457e-01  5.01163781e-01 -6.34864271e-01\n",
      "  4.93994743e-01  3.40991318e-02 -7.70612210e-02 -5.23390770e-01\n",
      " -3.90536785e-01  9.76225793e-01 -2.31955290e-01  4.92922813e-01\n",
      "  2.07122847e-01  3.43242586e-01  6.75997019e-01  3.43689382e-01\n",
      " -7.34328479e-03 -7.62901306e-02 -1.36262581e-01 -3.49554360e-01\n",
      "  2.33103514e-01 -2.97866374e-01  1.42095387e-01  2.46145166e-02\n",
      "  6.91457465e-02 -7.26145744e-01 -2.68725991e-01  4.27473038e-01\n",
      "  8.14948857e-01  1.12877059e+00  5.72656631e-01  6.83157027e-01\n",
      "  2.91611552e-01  4.32616830e-01 -8.02971900e-01 -2.75246471e-01\n",
      "  1.94451630e-01  6.13069296e-01 -5.72932422e-01  2.11954072e-01\n",
      "  1.04959095e+00 -7.37529248e-02  1.31617635e-01  5.35595417e-01\n",
      "  5.16681373e-01 -9.72960353e-01  2.33713865e-01 -6.80972874e-01\n",
      " -4.43347394e-02  6.06121868e-02 -8.95191729e-01  6.56643152e-01\n",
      " -4.67941612e-01  1.72935054e-01  3.06211114e-01 -2.09628448e-01\n",
      " -4.73560929e-01 -7.77069032e-01  5.25709808e-01 -5.47120214e-01\n",
      " -4.76924509e-01  3.44251424e-01  5.33317804e-01 -1.14928357e-01\n",
      "  8.92056108e-01 -1.09298661e-01 -1.78937271e-01  4.77421999e-01\n",
      "  1.20972060e-02 -6.01830721e-01  1.15526125e-01  1.84282243e-01\n",
      "  1.20737918e-01 -8.41489851e-01  2.74324179e-01  6.53214380e-02\n",
      "  3.10614675e-01  1.87517211e-01 -3.63761693e-01 -4.86926019e-01\n",
      "  2.67216444e-01  7.08758384e-02  4.93382275e-01 -6.62216306e-01\n",
      " -8.03277671e-01  5.06674275e-02 -4.52651903e-02  8.58727932e-01\n",
      "  3.86713773e-01 -2.26233333e-01  2.94005901e-01  8.16828609e-02\n",
      "  4.67826068e-01  9.88701731e-02 -5.52253127e-01 -6.25416636e-02\n",
      " -4.36638117e-01 -1.23846389e-01  3.10184360e-01  1.10362805e-01\n",
      " -3.76352072e-01 -6.95727170e-01 -2.82260239e-01 -2.98604459e-01\n",
      " -5.82896113e-01 -7.70874918e-02  6.86300695e-01  4.38593328e-01\n",
      "  2.09501594e-01  1.93385899e-01  1.10943151e+00 -6.18437052e-01\n",
      " -5.46705842e-01 -3.53303879e-01 -4.70864028e-03  2.02219620e-01\n",
      "  1.96173102e-01 -8.64403367e-01 -6.46913230e-01  1.45509869e-01\n",
      " -6.57370865e-01  2.44000092e-01  6.11473203e-01  5.38556337e-01\n",
      " -6.71112657e-01 -4.79236394e-02  1.10620749e+00  2.27204785e-01\n",
      " -2.44950041e-01 -4.12875004e-02 -4.92067188e-02 -3.21430787e-02\n",
      " -3.99663895e-01 -2.86540627e-01 -4.12346452e-01  3.79928648e-02\n",
      "  3.89240086e-01 -1.03023134e-01  3.54121447e-01  1.34739459e+00\n",
      " -1.63724944e-02 -6.84470415e-01 -2.10255329e-02 -1.12153515e-01\n",
      "  4.70662117e-01  5.75038865e-02 -6.25386477e-01 -3.15227062e-01\n",
      " -4.69694585e-01 -4.81898487e-02 -1.43374532e-01 -3.73101383e-01\n",
      "  1.92334324e-01 -1.23024061e-01  1.10399842e+00 -8.03215504e-01\n",
      "  3.67378086e-01  8.49291235e-02 -2.87107050e-01 -3.32370818e-01\n",
      " -2.46271253e-01  4.15957689e-01  1.03018686e-01  4.71279770e-01\n",
      " -1.91447139e-01  3.02230835e-01 -6.40176773e-01  4.34333146e-01\n",
      "  4.13303167e-01 -2.25464195e-01 -2.43023872e-01 -2.54389524e-01\n",
      "  1.05514765e-01  4.62910652e-01 -4.52698588e-01  4.98102456e-02\n",
      " -2.12586150e-01 -5.03674865e-01  1.03526384e-01  2.60310173e-01\n",
      " -1.91697702e-01 -4.90235597e-01  9.73624110e-01  1.67644136e-02\n",
      " -3.09523135e-01 -5.45606792e-01 -3.38727415e-01  1.95590585e-01\n",
      " -3.95997971e-01  4.34039265e-01 -4.79470164e-01 -1.79774463e-01\n",
      " -1.33216277e-01  2.92425573e-01  1.17567241e-01 -2.01351941e-04\n",
      " -2.28282869e-01 -2.98369490e-03  1.14583872e-01  1.87827110e-01\n",
      " -6.16419092e-02 -2.99429864e-01 -5.49328566e-01  1.95448071e-01\n",
      "  3.30738798e-02  4.45143133e-01 -1.45918047e+00  2.76990116e-01\n",
      "  1.06867522e-01 -6.36705995e-01  1.49721503e-02  6.21623933e-01\n",
      "  4.35345471e-01  3.67273450e-01  2.21252292e-01 -1.68745726e-01\n",
      "  6.34661615e-01  3.05576414e-01 -6.60773218e-01 -3.48220825e-01\n",
      " -6.01494908e-01  1.62416726e-01  8.51277590e-01  5.64038873e-01\n",
      " -1.32888541e-01  5.78130960e-01  4.37329769e-01 -2.50419378e-01\n",
      "  1.27389640e-01  3.04841608e-01  1.69228345e-01 -6.80671453e-01\n",
      " -7.91647136e-02 -3.90768163e-02 -5.89853674e-02  3.26032192e-01\n",
      "  2.79810995e-01  2.01112419e-01  2.79959261e-01 -3.94225419e-01\n",
      " -5.61534055e-02 -2.80866593e-01  4.21973437e-01 -1.25217527e-01\n",
      " -2.44032800e-01  2.56264925e-01 -5.77546299e-01 -1.79337002e-02\n",
      "  3.75045419e-01  3.90127748e-01  3.08235317e-01  4.32461239e-02\n",
      " -3.19815397e-01 -7.31928587e-01 -8.57445300e-01 -4.56196636e-01\n",
      "  1.91792414e-01  1.12777472e+00  1.14098623e-01 -3.93427730e-01\n",
      " -5.31382799e-01  3.73633385e-01  6.49822295e-01  6.61252618e-01\n",
      " -5.59039831e-01 -5.07438898e-01  1.88692242e-01  4.30638194e-01\n",
      " -4.43620563e-01  4.72782195e-01 -6.59415543e-01  6.56473190e-02\n",
      "  5.67307770e-01 -2.75379837e-01 -4.96693611e-01  7.40167964e-03\n",
      "  3.16577435e-01 -4.18295413e-01 -4.22885656e-01 -5.52101433e-01\n",
      "  1.22607075e-01  9.61042792e-02  3.23055595e-01 -8.41065586e-01\n",
      " -7.24421814e-02 -1.82764068e-01  6.32212877e-01  1.67815357e-01\n",
      "  9.77995574e-01 -5.84435463e-03  5.92191696e-01  3.41564238e-01\n",
      "  6.90299749e-01 -1.18450627e-01 -7.60542631e-01  3.70549768e-01\n",
      "  7.42236316e-01 -1.17789373e-01  1.43134284e+00 -5.23150861e-01\n",
      "  7.84933329e-01 -1.76844656e-01 -7.12502420e-01 -9.65571776e-02\n",
      " -3.18498445e+00  3.49936277e-01  1.60431802e-01 -4.65894155e-02\n",
      "  1.10750936e-01  5.58575392e-01  3.64335775e-01 -2.92843252e-01\n",
      " -2.93849111e-01 -2.34929547e-01  4.15985495e-01 -2.82904059e-01\n",
      " -2.30949983e-01  4.11721736e-01  1.61260486e-01  7.83595920e-01\n",
      "  9.95904356e-02 -6.22513071e-02  6.16447330e-01  1.61838517e-01\n",
      "  1.10445112e-01  9.79023241e-03  1.38397485e-01 -5.57122171e-01\n",
      " -3.70276511e-01  4.98058796e-02 -4.47070837e-01 -5.60444772e-01\n",
      " -3.11797470e-01 -1.47280797e-01  5.75177312e-01  6.68729022e-02\n",
      "  5.60362518e-01 -1.32139444e-01  8.84481013e-01 -8.28564614e-02\n",
      "  2.30413064e-01 -3.49385709e-01  4.17937599e-02 -9.30054039e-02\n",
      " -3.41964692e-01 -6.47252619e-01  1.84758946e-01 -8.61653328e-01\n",
      " -2.63148900e-02  2.52711296e-01 -6.25010908e-01  6.25658184e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"First element of X_train_name_last_hidden:\", X_train_name_last_hidden[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_name_last_hidden: (40032, 768)\n",
      "Data type of X_train_name_last_hidden: float32\n",
      "Shape of X_train_description_last_hidden: (40032, 768)\n",
      "Data type of X_train_description_last_hidden: float32\n",
      "Shape of X_test_name_last_hidden: (10009, 768)\n",
      "Data type of X_test_name_last_hidden: float32\n",
      "Shape of X_test_description_last_hidden: (10009, 768)\n",
      "Data type of X_test_description_last_hidden: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_name_last_hidden:\", X_train_name_last_hidden.shape)\n",
    "print(\"Data type of X_train_name_last_hidden:\", X_train_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_train_description_last_hidden:\", X_train_description_last_hidden.shape)\n",
    "print(\"Data type of X_train_description_last_hidden:\", X_train_description_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_name_last_hidden:\", X_test_name_last_hidden.shape)\n",
    "print(\"Data type of X_test_name_last_hidden:\", X_test_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_description_last_hidden:\", X_test_description_last_hidden.shape)\n",
    "print(\"Data type of X_test_description_last_hidden:\", X_test_description_last_hidden.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concatenated = np.concatenate((X_train_name_last_hidden, X_train_description_last_hidden), axis=1)\n",
    "X_test_concatenated = np.concatenate((X_test_name_last_hidden, X_test_description_last_hidden), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_concatenated: (40032, 1536)\n",
      "Data type of X_train_concatenated: float32\n",
      "Shape of X_test_concatenated: (10009, 1536)\n",
      "Data type of X_test_concatenated: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_concatenated:\", X_train_concatenated.shape)\n",
    "print(\"Data type of X_train_concatenated:\", X_train_concatenated.dtype)\n",
    "\n",
    "print(\"Shape of X_test_concatenated:\", X_test_concatenated.shape)\n",
    "print(\"Data type of X_test_concatenated:\", X_test_concatenated.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "X_train_combined = np.concatenate((X_train_processed_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_processed_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 2\n",
    "X_train_combined_1 = np.concatenate((X_train_processed_1_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_1 = np.concatenate((X_test_processed_1_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 3\n",
    "X_train_combined_2 = np.concatenate((X_train_processed_2_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_2 = np.concatenate((X_test_processed_2_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 4\n",
    "X_train_combined_3 = np.concatenate((X_train_processed_3_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_3 = np.concatenate((X_test_processed_3_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 5\n",
    "X_train_combined_4 = np.concatenate((X_train_processed_4_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_4 = np.concatenate((X_test_processed_4_loaded, X_test_concatenated), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Train combined shape: (40032, 3732)\n",
      "Stage 1 - Test combined shape: (10009, 3732)\n",
      "Stage 2 - Train combined shape: (40032, 3755)\n",
      "Stage 2 - Test combined shape: (10009, 3755)\n",
      "Stage 3 - Train combined shape: (40032, 3869)\n",
      "Stage 3 - Test combined shape: (10009, 3869)\n",
      "Stage 4 - Train combined shape: (40032, 4218)\n",
      "Stage 4 - Test combined shape: (10009, 4218)\n",
      "Stage 5 - Train combined shape: (40032, 4534)\n",
      "Stage 5 - Test combined shape: (10009, 4534)\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1 - Train combined shape:\", X_train_combined.shape)\n",
    "print(\"Stage 1 - Test combined shape:\", X_test_combined.shape)\n",
    "print(\"Stage 2 - Train combined shape:\", X_train_combined_1.shape)\n",
    "print(\"Stage 2 - Test combined shape:\", X_test_combined_1.shape)\n",
    "print(\"Stage 3 - Train combined shape:\", X_train_combined_2.shape)\n",
    "print(\"Stage 3 - Test combined shape:\", X_test_combined_2.shape)\n",
    "print(\"Stage 4 - Train combined shape:\", X_train_combined_3.shape)\n",
    "print(\"Stage 4 - Test combined shape:\", X_test_combined_3.shape)\n",
    "print(\"Stage 5 - Train combined shape:\", X_train_combined_4.shape)\n",
    "print(\"Stage 5 - Test combined shape:\", X_test_combined_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define num_clases\n",
    "\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "num_classes_1 = len(np.unique(y_1_encoded))\n",
    "num_classes_2 = len(np.unique(y_2_encoded))\n",
    "num_classes_3 = len(np.unique(y_3_encoded))\n",
    "num_classes_4 = len(np.unique(y_4_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "114\n",
      "349\n",
      "316\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)\n",
    "print(num_classes_1)\n",
    "print(num_classes_2)\n",
    "print(num_classes_3)\n",
    "print(num_classes_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 3ms/step - loss: 0.9774 - accuracy: 0.7387 - val_loss: 0.4695 - val_accuracy: 0.8662 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5638 - accuracy: 0.8444 - val_loss: 0.3384 - val_accuracy: 0.9073 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4709 - accuracy: 0.8730 - val_loss: 0.2795 - val_accuracy: 0.9231 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4045 - accuracy: 0.8926 - val_loss: 0.2647 - val_accuracy: 0.9279 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3643 - accuracy: 0.9034 - val_loss: 0.2645 - val_accuracy: 0.9302 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3365 - accuracy: 0.9116 - val_loss: 0.2228 - val_accuracy: 0.9412 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3134 - accuracy: 0.9168 - val_loss: 0.2283 - val_accuracy: 0.9423 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2927 - accuracy: 0.9246 - val_loss: 0.2067 - val_accuracy: 0.9455 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2713 - accuracy: 0.9297 - val_loss: 0.2067 - val_accuracy: 0.9457 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 3s 3ms/step - loss: 0.2523 - accuracy: 0.9326 - val_loss: 0.1965 - val_accuracy: 0.9496 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 3s 3ms/step - loss: 0.2347 - accuracy: 0.9391 - val_loss: 0.1920 - val_accuracy: 0.9514 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2250 - accuracy: 0.9419 - val_loss: 0.1903 - val_accuracy: 0.9519 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 3s 3ms/step - loss: 0.2181 - accuracy: 0.9434 - val_loss: 0.1840 - val_accuracy: 0.9537 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 3s 3ms/step - loss: 0.2102 - accuracy: 0.9458 - val_loss: 0.1856 - val_accuracy: 0.9535 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2014 - accuracy: 0.9472 - val_loss: 0.1842 - val_accuracy: 0.9541 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1956 - accuracy: 0.9491 - val_loss: 0.1828 - val_accuracy: 0.9542 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1945 - accuracy: 0.9484 - val_loss: 0.1818 - val_accuracy: 0.9555 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1901 - accuracy: 0.9510 - val_loss: 0.1825 - val_accuracy: 0.9556 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1890 - accuracy: 0.9522 - val_loss: 0.1820 - val_accuracy: 0.9555 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1833 - accuracy: 0.9523 - val_loss: 0.1812 - val_accuracy: 0.9560 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1831 - accuracy: 0.9516 - val_loss: 0.1814 - val_accuracy: 0.9554 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1824 - accuracy: 0.9516 - val_loss: 0.1815 - val_accuracy: 0.9554 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1858 - accuracy: 0.9511 - val_loss: 0.1812 - val_accuracy: 0.9552 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1760 - accuracy: 0.9535 - val_loss: 0.1812 - val_accuracy: 0.9555 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1821 - accuracy: 0.9523 - val_loss: 0.1812 - val_accuracy: 0.9555 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1817 - accuracy: 0.9528 - val_loss: 0.1819 - val_accuracy: 0.9556 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1813 - accuracy: 0.9525 - val_loss: 0.1814 - val_accuracy: 0.9558 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1792 - accuracy: 0.9538 - val_loss: 0.1814 - val_accuracy: 0.9555 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1853 - accuracy: 0.9511 - val_loss: 0.1812 - val_accuracy: 0.9556 - lr: 1.7824e-07\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1839 - accuracy: 0.9516 - val_loss: 0.1804 - val_accuracy: 0.9554 - lr: 1.0694e-07\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1807 - accuracy: 0.9518 - val_loss: 0.1814 - val_accuracy: 0.9555 - lr: 6.4167e-08\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1847 - accuracy: 0.9515 - val_loss: 0.1814 - val_accuracy: 0.9555 - lr: 3.8500e-08\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1774 - accuracy: 0.9538 - val_loss: 0.1811 - val_accuracy: 0.9557 - lr: 2.3100e-08\n",
      "Epoch 34/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1866 - accuracy: 0.9516 - val_loss: 0.1814 - val_accuracy: 0.9556 - lr: 1.3860e-08\n",
      "Epoch 35/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1847 - accuracy: 0.9521 - val_loss: 0.1809 - val_accuracy: 0.9558 - lr: 8.3160e-09\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9554\n",
      "Accuracy: 0.955440104007721\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1712\n",
      "           1       0.93      0.96      0.94       715\n",
      "           2       0.97      0.98      0.97        91\n",
      "           3       0.95      0.96      0.95       649\n",
      "           4       0.96      0.97      0.96       505\n",
      "           5       0.97      0.97      0.97      1392\n",
      "           6       0.94      0.95      0.94      1247\n",
      "           7       0.93      0.93      0.93       890\n",
      "           8       0.00      0.00      0.00         6\n",
      "           9       0.84      1.00      0.91        16\n",
      "          10       0.00      0.00      0.00         3\n",
      "          11       0.92      0.94      0.93       245\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00        15\n",
      "          14       0.83      0.56      0.67        27\n",
      "          15       1.00      0.70      0.82        10\n",
      "          16       0.95      0.96      0.96       624\n",
      "          17       0.85      0.47      0.61        36\n",
      "          18       0.91      0.92      0.91       417\n",
      "          19       0.94      0.91      0.93       132\n",
      "          20       0.99      0.99      0.99      1194\n",
      "          21       0.98      0.86      0.92        57\n",
      "          22       1.00      0.08      0.15        25\n",
      "\n",
      "    accuracy                           0.96     10009\n",
      "   macro avg       0.82      0.74      0.76     10009\n",
      "weighted avg       0.95      0.96      0.95     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage1\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Compile the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model fit\n",
    "run_1 = model.fit(X_train_combined, y_train, epochs=60, batch_size=32,\n",
    "                  validation_data=(X_test_combined, y_test),\n",
    "                  callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "save_model(model, 'model_1_preberttune.h5')\n",
    "\n",
    "y_pred_probabilities = model.predict(X_test_combined)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.8211 - accuracy: 0.5912 - val_loss: 0.6835 - val_accuracy: 0.8192 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8864 - accuracy: 0.7609 - val_loss: 0.4522 - val_accuracy: 0.8706 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7085 - accuracy: 0.7980 - val_loss: 0.3618 - val_accuracy: 0.8892 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6533 - accuracy: 0.8100 - val_loss: 0.3351 - val_accuracy: 0.8971 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6046 - accuracy: 0.8224 - val_loss: 0.3008 - val_accuracy: 0.9075 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5497 - accuracy: 0.8370 - val_loss: 0.2820 - val_accuracy: 0.9127 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5160 - accuracy: 0.8456 - val_loss: 0.2556 - val_accuracy: 0.9243 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4898 - accuracy: 0.8540 - val_loss: 0.2459 - val_accuracy: 0.9281 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4640 - accuracy: 0.8604 - val_loss: 0.2339 - val_accuracy: 0.9313 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4458 - accuracy: 0.8661 - val_loss: 0.2264 - val_accuracy: 0.9346 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4142 - accuracy: 0.8744 - val_loss: 0.2170 - val_accuracy: 0.9344 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8769 - val_loss: 0.2131 - val_accuracy: 0.9346 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3924 - accuracy: 0.8823 - val_loss: 0.2144 - val_accuracy: 0.9363 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3802 - accuracy: 0.8836 - val_loss: 0.2065 - val_accuracy: 0.9385 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3758 - accuracy: 0.8881 - val_loss: 0.2039 - val_accuracy: 0.9383 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3719 - accuracy: 0.8864 - val_loss: 0.2012 - val_accuracy: 0.9393 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3556 - accuracy: 0.8908 - val_loss: 0.1993 - val_accuracy: 0.9405 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3633 - accuracy: 0.8912 - val_loss: 0.1992 - val_accuracy: 0.9409 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3541 - accuracy: 0.8934 - val_loss: 0.1985 - val_accuracy: 0.9405 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3565 - accuracy: 0.8914 - val_loss: 0.1973 - val_accuracy: 0.9408 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 3s 3ms/step - loss: 0.3455 - accuracy: 0.8960 - val_loss: 0.1972 - val_accuracy: 0.9415 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3468 - accuracy: 0.8948 - val_loss: 0.1968 - val_accuracy: 0.9419 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3499 - accuracy: 0.8972 - val_loss: 0.1970 - val_accuracy: 0.9411 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3482 - accuracy: 0.8947 - val_loss: 0.1971 - val_accuracy: 0.9413 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3491 - accuracy: 0.8944 - val_loss: 0.1970 - val_accuracy: 0.9416 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3549 - accuracy: 0.8931 - val_loss: 0.1970 - val_accuracy: 0.9413 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3514 - accuracy: 0.8929 - val_loss: 0.1966 - val_accuracy: 0.9411 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3513 - accuracy: 0.8940 - val_loss: 0.1965 - val_accuracy: 0.9415 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3466 - accuracy: 0.8951 - val_loss: 0.1967 - val_accuracy: 0.9416 - lr: 1.7824e-07\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3429 - accuracy: 0.8973 - val_loss: 0.1965 - val_accuracy: 0.9416 - lr: 1.0694e-07\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3485 - accuracy: 0.8954 - val_loss: 0.1971 - val_accuracy: 0.9413 - lr: 6.4167e-08\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3490 - accuracy: 0.8957 - val_loss: 0.1970 - val_accuracy: 0.9418 - lr: 3.8500e-08\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3500 - accuracy: 0.8945 - val_loss: 0.1971 - val_accuracy: 0.9411 - lr: 2.3100e-08\n",
      "Epoch 34/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3504 - accuracy: 0.8937 - val_loss: 0.1968 - val_accuracy: 0.9412 - lr: 1.3860e-08\n",
      "Epoch 35/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3464 - accuracy: 0.8954 - val_loss: 0.1971 - val_accuracy: 0.9414 - lr: 8.3160e-09\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1965 - accuracy: 0.9416\n",
      "Accuracy for Stage 2 model: 0.9415525794029236\n",
      "313/313 [==============================] - 0s 945us/step\n",
      "Classification Report for Stage 2 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90        35\n",
      "           1       0.83      0.65      0.73        23\n",
      "           2       1.00      0.83      0.91         6\n",
      "           3       0.96      1.00      0.98        54\n",
      "           4       0.94      0.93      0.93       128\n",
      "           5       0.93      1.00      0.96        13\n",
      "           6       1.00      0.88      0.93         8\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       1.00      0.96      0.98        79\n",
      "           9       1.00      1.00      1.00         9\n",
      "          10       0.95      0.92      0.94       156\n",
      "          11       0.85      0.56      0.68        71\n",
      "          12       1.00      0.73      0.84        11\n",
      "          13       1.00      0.33      0.50         3\n",
      "          14       0.88      0.97      0.92       128\n",
      "          15       0.83      0.83      0.83         6\n",
      "          16       0.94      0.97      0.96       278\n",
      "          17       0.80      0.67      0.73        12\n",
      "          18       1.00      0.50      0.67        12\n",
      "          19       0.88      0.70      0.78        10\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.99      1.00      1.00      1242\n",
      "          22       0.50      0.25      0.33         4\n",
      "          23       0.96      0.99      0.97       464\n",
      "          24       1.00      0.88      0.94        74\n",
      "          25       0.92      0.99      0.95       125\n",
      "          26       0.84      0.54      0.66        39\n",
      "          27       1.00      0.99      0.99        83\n",
      "          28       0.90      0.98      0.94       372\n",
      "          29       0.95      0.92      0.94        88\n",
      "          30       1.00      0.96      0.98        57\n",
      "          31       0.00      0.00      0.00         8\n",
      "          32       0.81      0.75      0.78        28\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       1.00      1.00      1.00        14\n",
      "          35       1.00      1.00      1.00       245\n",
      "          36       1.00      0.50      0.67         6\n",
      "          37       0.97      0.81      0.89        43\n",
      "          38       0.89      0.91      0.90       105\n",
      "          39       0.94      0.91      0.92        32\n",
      "          40       1.00      0.75      0.86         4\n",
      "          41       1.00      1.00      1.00         6\n",
      "          42       1.00      1.00      1.00       132\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       0.88      0.96      0.92        24\n",
      "          45       0.99      1.00      0.99       207\n",
      "          46       0.95      0.98      0.97       200\n",
      "          47       0.90      0.97      0.93       275\n",
      "          48       0.93      0.79      0.86        34\n",
      "          49       0.95      0.86      0.90        43\n",
      "          50       0.95      0.96      0.96       134\n",
      "          51       0.67      1.00      0.80         2\n",
      "          52       1.00      0.74      0.85        19\n",
      "          53       1.00      0.71      0.83         7\n",
      "          54       1.00      0.64      0.78        25\n",
      "          55       0.99      0.99      0.99       113\n",
      "          56       1.00      1.00      1.00        27\n",
      "          57       0.82      0.67      0.73        27\n",
      "          58       1.00      1.00      1.00        27\n",
      "          59       0.70      0.87      0.78       100\n",
      "          60       1.00      1.00      1.00        58\n",
      "          61       1.00      1.00      1.00        52\n",
      "          62       1.00      1.00      1.00         6\n",
      "          63       1.00      1.00      1.00         4\n",
      "          64       0.60      0.80      0.68        99\n",
      "          65       0.98      0.97      0.97        58\n",
      "          66       0.78      0.60      0.68        30\n",
      "          67       0.88      0.91      0.89        32\n",
      "          68       0.91      0.62      0.74        16\n",
      "          69       0.97      0.97      0.97       217\n",
      "          70       0.82      0.87      0.84        53\n",
      "          71       0.93      0.87      0.90       108\n",
      "          72       1.00      0.66      0.79        32\n",
      "          73       0.00      0.00      0.00        10\n",
      "          74       0.00      0.00      0.00         2\n",
      "          75       0.94      0.73      0.82        22\n",
      "          76       0.00      0.00      0.00        13\n",
      "          77       0.29      0.14      0.19        37\n",
      "          78       0.48      0.87      0.62        68\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.99      1.00      1.00       722\n",
      "          81       0.97      0.97      0.97        31\n",
      "          82       1.00      1.00      1.00       236\n",
      "          83       0.68      0.53      0.59        57\n",
      "          84       0.95      0.97      0.96       197\n",
      "          85       1.00      1.00      1.00         9\n",
      "          86       0.00      0.00      0.00         8\n",
      "          87       0.88      0.78      0.82         9\n",
      "          88       0.98      0.99      0.99       181\n",
      "          89       0.98      0.99      0.98       614\n",
      "          90       1.00      0.99      0.99        84\n",
      "          91       1.00      0.25      0.40         8\n",
      "          92       0.93      0.96      0.94       117\n",
      "          93       0.71      1.00      0.83         5\n",
      "          94       0.96      0.98      0.97       191\n",
      "          95       0.98      0.97      0.98        61\n",
      "          96       0.96      1.00      0.98        68\n",
      "          97       0.97      1.00      0.99        72\n",
      "          98       0.89      1.00      0.94        55\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       1.00      1.00      1.00        64\n",
      "         101       0.95      0.97      0.96        62\n",
      "         102       0.80      0.76      0.78        21\n",
      "         103       1.00      0.98      0.99        96\n",
      "         104       0.71      0.81      0.76        21\n",
      "         105       0.64      0.47      0.54        15\n",
      "         106       0.72      0.66      0.69        65\n",
      "         107       0.83      0.96      0.89        54\n",
      "         108       0.96      0.97      0.96       243\n",
      "         109       0.95      1.00      0.97        18\n",
      "         110       0.95      0.83      0.88        23\n",
      "         111       1.00      1.00      1.00        12\n",
      "         112       0.98      0.91      0.94       154\n",
      "         113       0.88      0.33      0.48        21\n",
      "\n",
      "    accuracy                           0.94     10009\n",
      "   macro avg       0.83      0.78      0.79     10009\n",
      "weighted avg       0.94      0.94      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage2\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_2 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_1.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_2 = model_2.fit(X_train_combined_1, y_train_1, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_1, y_test_1),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_2, 'model_2_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_2, accuracy_2 = model_2.evaluate(X_test_combined_1, y_test_1)\n",
    "print(\"Accuracy for Stage 2 model:\", accuracy_2)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_2 = model_2.predict(X_test_combined_1)\n",
    "y_pred_2 = np.argmax(y_pred_probabilities_2, axis=1)\n",
    "report_2 = classification_report(y_test_1, y_pred_2)\n",
    "print(\"Classification Report for Stage 2 model:\")\n",
    "print(report_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 2.8616 - accuracy: 0.4597 - val_loss: 1.4320 - val_accuracy: 0.6822 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.5729 - accuracy: 0.6391 - val_loss: 1.0036 - val_accuracy: 0.7520 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.2517 - accuracy: 0.6884 - val_loss: 0.6970 - val_accuracy: 0.8118 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.0994 - accuracy: 0.7125 - val_loss: 0.6239 - val_accuracy: 0.8291 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.9939 - accuracy: 0.7369 - val_loss: 0.5500 - val_accuracy: 0.8410 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.9307 - accuracy: 0.7497 - val_loss: 0.5146 - val_accuracy: 0.8466 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8748 - accuracy: 0.7604 - val_loss: 0.4752 - val_accuracy: 0.8613 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8265 - accuracy: 0.7729 - val_loss: 0.4495 - val_accuracy: 0.8733 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7920 - accuracy: 0.7799 - val_loss: 0.4511 - val_accuracy: 0.8730 - lr: 4.3541e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7554 - accuracy: 0.7881 - val_loss: 0.4220 - val_accuracy: 0.8800 - lr: 3.7010e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7403 - accuracy: 0.7921 - val_loss: 0.4077 - val_accuracy: 0.8865 - lr: 3.1459e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7119 - accuracy: 0.8008 - val_loss: 0.4012 - val_accuracy: 0.8868 - lr: 2.6740e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6939 - accuracy: 0.8058 - val_loss: 0.3889 - val_accuracy: 0.8910 - lr: 2.2729e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6779 - accuracy: 0.8082 - val_loss: 0.3914 - val_accuracy: 0.8943 - lr: 1.9319e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6605 - accuracy: 0.8121 - val_loss: 0.3827 - val_accuracy: 0.8947 - lr: 1.6422e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6499 - accuracy: 0.8156 - val_loss: 0.3825 - val_accuracy: 0.8948 - lr: 1.3958e-04\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6338 - accuracy: 0.8207 - val_loss: 0.3768 - val_accuracy: 0.8940 - lr: 1.0469e-04\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6335 - accuracy: 0.8184 - val_loss: 0.3734 - val_accuracy: 0.8954 - lr: 7.8515e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6257 - accuracy: 0.8207 - val_loss: 0.3723 - val_accuracy: 0.8964 - lr: 5.8887e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6103 - accuracy: 0.8247 - val_loss: 0.3703 - val_accuracy: 0.8959 - lr: 4.1221e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6218 - accuracy: 0.8220 - val_loss: 0.3691 - val_accuracy: 0.8974 - lr: 2.8854e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6049 - accuracy: 0.8255 - val_loss: 0.3679 - val_accuracy: 0.8974 - lr: 2.0198e-05\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5993 - accuracy: 0.8279 - val_loss: 0.3672 - val_accuracy: 0.8969 - lr: 1.4139e-05\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5953 - accuracy: 0.8279 - val_loss: 0.3679 - val_accuracy: 0.8978 - lr: 9.8971e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6017 - accuracy: 0.8277 - val_loss: 0.3672 - val_accuracy: 0.8981 - lr: 6.9280e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5962 - accuracy: 0.8288 - val_loss: 0.3664 - val_accuracy: 0.8983 - lr: 4.8496e-06\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5994 - accuracy: 0.8289 - val_loss: 0.3664 - val_accuracy: 0.8986 - lr: 3.3947e-06\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6028 - accuracy: 0.8292 - val_loss: 0.3653 - val_accuracy: 0.8980 - lr: 2.3763e-06\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5921 - accuracy: 0.8300 - val_loss: 0.3656 - val_accuracy: 0.8979 - lr: 1.6634e-06\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6031 - accuracy: 0.8277 - val_loss: 0.3670 - val_accuracy: 0.8982 - lr: 1.1644e-06\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6031 - accuracy: 0.8259 - val_loss: 0.3660 - val_accuracy: 0.8980 - lr: 8.1507e-07\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6024 - accuracy: 0.8286 - val_loss: 0.3666 - val_accuracy: 0.8985 - lr: 5.7055e-07\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5925 - accuracy: 0.8323 - val_loss: 0.3667 - val_accuracy: 0.8981 - lr: 3.9938e-07\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3653 - accuracy: 0.8980\n",
      "Accuracy for Stage 3 model: 0.8979918360710144\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 3 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.78      1.00      0.88        70\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       1.00      0.86      0.92         7\n",
      "           5       0.67      1.00      0.80         6\n",
      "           6       0.88      0.88      0.88         8\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       0.83      0.97      0.89        39\n",
      "           9       1.00      0.33      0.50         6\n",
      "          10       0.75      1.00      0.86         3\n",
      "          11       0.80      0.91      0.85        35\n",
      "          12       0.86      0.95      0.91       101\n",
      "          13       0.88      0.92      0.90        25\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       1.00      0.95      0.98        21\n",
      "          16       1.00      1.00      1.00        83\n",
      "          17       0.63      1.00      0.77        34\n",
      "          18       1.00      0.24      0.38        21\n",
      "          19       1.00      1.00      1.00       113\n",
      "          20       0.93      1.00      0.96        25\n",
      "          21       0.95      1.00      0.97        55\n",
      "          22       0.00      0.00      0.00        18\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       0.69      1.00      0.82       135\n",
      "          25       0.91      0.98      0.95        53\n",
      "          26       0.98      1.00      0.99        63\n",
      "          27       1.00      0.70      0.82        10\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.89      1.00      0.94         8\n",
      "          31       1.00      1.00      1.00        43\n",
      "          32       1.00      0.67      0.80         3\n",
      "          33       1.00      0.25      0.40        12\n",
      "          34       1.00      0.67      0.80         6\n",
      "          35       0.60      1.00      0.75         6\n",
      "          36       1.00      0.90      0.95        10\n",
      "          37       1.00      1.00      1.00         9\n",
      "          38       0.00      0.00      0.00         2\n",
      "          39       0.00      0.00      0.00         6\n",
      "          40       1.00      1.00      1.00        13\n",
      "          41       0.87      0.97      0.91        61\n",
      "          42       1.00      1.00      1.00         4\n",
      "          43       0.67      0.22      0.33         9\n",
      "          44       0.00      0.00      0.00         4\n",
      "          45       1.00      1.00      1.00         4\n",
      "          46       1.00      1.00      1.00         1\n",
      "          47       0.00      0.00      0.00         4\n",
      "          48       1.00      0.67      0.80         3\n",
      "          49       0.88      1.00      0.93        50\n",
      "          50       0.62      0.45      0.53        11\n",
      "          51       0.96      0.85      0.90        27\n",
      "          52       1.00      1.00      1.00         7\n",
      "          53       1.00      0.33      0.50         9\n",
      "          54       0.80      0.95      0.87        41\n",
      "          55       1.00      1.00      1.00         3\n",
      "          56       0.00      0.00      0.00         4\n",
      "          57       0.95      1.00      0.97        18\n",
      "          58       0.00      0.00      0.00         5\n",
      "          59       0.87      0.94      0.91        36\n",
      "          60       0.92      0.96      0.94        76\n",
      "          61       0.95      0.98      0.96        84\n",
      "          63       0.60      0.27      0.38        11\n",
      "          64       1.00      1.00      1.00         3\n",
      "          65       1.00      0.92      0.96        25\n",
      "          66       0.93      1.00      0.96       243\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.75      1.00      0.86         6\n",
      "          69       0.96      0.92      0.94        26\n",
      "          70       0.00      0.00      0.00         8\n",
      "          71       0.88      1.00      0.94        29\n",
      "          72       0.91      0.95      0.93        42\n",
      "          73       1.00      1.00      1.00         3\n",
      "          74       0.00      0.00      0.00         3\n",
      "          75       0.85      1.00      0.92       184\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       0.00      0.00      0.00         4\n",
      "          78       0.97      0.97      0.97       159\n",
      "          79       0.96      0.93      0.95       395\n",
      "          80       0.80      0.80      0.80         5\n",
      "          81       0.67      0.40      0.50         5\n",
      "          82       1.00      0.38      0.55        16\n",
      "          83       1.00      1.00      1.00         2\n",
      "          84       0.00      0.00      0.00         4\n",
      "          85       1.00      0.50      0.67         2\n",
      "          86       1.00      1.00      1.00         6\n",
      "          87       0.95      0.69      0.80        29\n",
      "          88       0.80      0.73      0.76        11\n",
      "          89       0.00      0.00      0.00        11\n",
      "          90       1.00      1.00      1.00         4\n",
      "          91       1.00      0.75      0.86         8\n",
      "          92       0.60      0.60      0.60         5\n",
      "          93       1.00      1.00      1.00       236\n",
      "          94       0.90      0.95      0.92       131\n",
      "          95       1.00      1.00      1.00       200\n",
      "          96       0.43      0.50      0.46         6\n",
      "          97       0.85      0.97      0.90        29\n",
      "          98       0.00      0.00      0.00         3\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       1.00      1.00      1.00         5\n",
      "         101       0.75      0.60      0.67        10\n",
      "         102       0.72      1.00      0.84        21\n",
      "         103       1.00      0.30      0.46        10\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       1.00      0.17      0.29         6\n",
      "         106       1.00      0.50      0.67        10\n",
      "         107       0.00      0.00      0.00         3\n",
      "         108       1.00      1.00      1.00         3\n",
      "         109       0.75      0.96      0.84        28\n",
      "         110       1.00      0.14      0.25         7\n",
      "         111       0.85      0.85      0.85        13\n",
      "         112       1.00      1.00      1.00         5\n",
      "         113       0.00      0.00      0.00         6\n",
      "         114       0.72      0.97      0.82        29\n",
      "         115       1.00      0.71      0.83        14\n",
      "         116       0.98      1.00      0.99        58\n",
      "         117       1.00      0.33      0.50         3\n",
      "         118       0.80      0.81      0.81        69\n",
      "         119       1.00      0.89      0.94        19\n",
      "         120       0.89      1.00      0.94        17\n",
      "         121       0.49      0.95      0.64        19\n",
      "         122       0.50      0.25      0.33         4\n",
      "         123       0.50      1.00      0.67         7\n",
      "         124       1.00      0.44      0.62         9\n",
      "         125       0.90      0.75      0.82        12\n",
      "         126       1.00      0.50      0.67         2\n",
      "         127       1.00      0.25      0.40         4\n",
      "         128       1.00      1.00      1.00         9\n",
      "         129       0.83      0.83      0.83         6\n",
      "         130       0.94      0.95      0.94        63\n",
      "         131       1.00      0.73      0.84        11\n",
      "         132       1.00      1.00      1.00        10\n",
      "         133       0.91      0.91      0.91        32\n",
      "         134       0.78      0.86      0.82        21\n",
      "         135       1.00      0.33      0.50         3\n",
      "         136       0.92      0.65      0.76        17\n",
      "         137       0.00      0.00      0.00         7\n",
      "         138       0.62      0.83      0.71         6\n",
      "         139       0.00      0.00      0.00         5\n",
      "         140       1.00      0.79      0.88        19\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       1.00      1.00      1.00         7\n",
      "         143       0.92      0.92      0.92        26\n",
      "         144       1.00      0.40      0.57         5\n",
      "         145       1.00      1.00      1.00         2\n",
      "         146       0.82      1.00      0.90         9\n",
      "         147       0.86      1.00      0.92         6\n",
      "         148       1.00      0.50      0.67         6\n",
      "         149       0.74      1.00      0.85        35\n",
      "         150       0.74      0.94      0.83        18\n",
      "         151       1.00      0.25      0.40         8\n",
      "         152       1.00      0.97      0.99        71\n",
      "         153       0.00      0.00      0.00         5\n",
      "         154       0.62      0.71      0.67         7\n",
      "         155       1.00      0.95      0.97        37\n",
      "         156       0.86      1.00      0.92        12\n",
      "         157       0.00      0.00      0.00         2\n",
      "         158       0.80      0.22      0.35        18\n",
      "         159       0.92      0.85      0.88        13\n",
      "         160       0.00      0.00      0.00        10\n",
      "         161       0.76      0.73      0.75        26\n",
      "         162       1.00      1.00      1.00        32\n",
      "         163       1.00      1.00      1.00        19\n",
      "         164       0.67      0.50      0.57        28\n",
      "         165       0.00      0.00      0.00         3\n",
      "         166       1.00      1.00      1.00         7\n",
      "         167       1.00      0.50      0.67         4\n",
      "         168       1.00      0.75      0.86         4\n",
      "         169       0.00      0.00      0.00         1\n",
      "         170       0.85      0.88      0.86       112\n",
      "         171       0.00      0.00      0.00         4\n",
      "         172       0.54      1.00      0.70        28\n",
      "         173       1.00      0.75      0.86         4\n",
      "         174       0.93      0.96      0.94       146\n",
      "         175       0.83      0.96      0.89        26\n",
      "         176       0.60      0.60      0.60         5\n",
      "         177       0.85      0.73      0.79        15\n",
      "         178       0.75      0.75      0.75        12\n",
      "         179       0.00      0.00      0.00         8\n",
      "         180       0.68      0.76      0.72        17\n",
      "         181       0.83      0.92      0.87        26\n",
      "         182       0.89      1.00      0.94        33\n",
      "         183       1.00      1.00      1.00         7\n",
      "         185       1.00      1.00      1.00        23\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.50      0.86      0.63         7\n",
      "         188       0.00      0.00      0.00         1\n",
      "         189       1.00      0.50      0.67         2\n",
      "         190       0.85      1.00      0.92        17\n",
      "         191       0.85      1.00      0.92        60\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       0.79      1.00      0.88        59\n",
      "         194       0.85      0.92      0.88        12\n",
      "         195       0.88      1.00      0.93         7\n",
      "         196       0.96      0.92      0.94        26\n",
      "         197       0.94      0.89      0.91        35\n",
      "         198       1.00      0.50      0.67         2\n",
      "         199       0.87      0.62      0.72        21\n",
      "         200       1.00      0.68      0.81        25\n",
      "         201       1.00      1.00      1.00         5\n",
      "         202       1.00      1.00      1.00       191\n",
      "         203       0.00      0.00      0.00         4\n",
      "         204       0.53      0.95      0.68        21\n",
      "         205       1.00      1.00      1.00        16\n",
      "         206       0.00      0.00      0.00         4\n",
      "         207       0.80      1.00      0.89         4\n",
      "         208       1.00      1.00      1.00        25\n",
      "         209       0.97      1.00      0.98        31\n",
      "         210       0.96      0.98      0.97        52\n",
      "         211       0.00      0.00      0.00         2\n",
      "         212       1.00      0.33      0.50         3\n",
      "         213       1.00      1.00      1.00         6\n",
      "         214       0.94      1.00      0.97        88\n",
      "         215       1.00      0.88      0.93         8\n",
      "         216       0.66      0.97      0.78        80\n",
      "         217       1.00      0.33      0.50         6\n",
      "         218       0.75      1.00      0.86         3\n",
      "         219       1.00      0.95      0.98        21\n",
      "         220       1.00      0.50      0.67         4\n",
      "         221       0.50      0.45      0.48        11\n",
      "         222       0.00      0.00      0.00         3\n",
      "         223       1.00      1.00      1.00         8\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       0.67      1.00      0.80         2\n",
      "         226       0.97      1.00      0.99        33\n",
      "         227       0.93      1.00      0.96        13\n",
      "         228       1.00      1.00      1.00        54\n",
      "         229       0.78      1.00      0.88         7\n",
      "         230       0.92      0.98      0.95       102\n",
      "         231       1.00      0.62      0.76        13\n",
      "         232       0.82      1.00      0.90        14\n",
      "         233       0.97      0.91      0.94        35\n",
      "         235       0.94      1.00      0.97        47\n",
      "         236       0.00      0.00      0.00         7\n",
      "         237       1.00      1.00      1.00         4\n",
      "         238       0.55      0.92      0.69        13\n",
      "         239       0.67      1.00      0.80        10\n",
      "         240       1.00      0.50      0.67         8\n",
      "         241       0.86      1.00      0.92         6\n",
      "         242       0.62      0.45      0.53        11\n",
      "         243       0.67      1.00      0.80         4\n",
      "         244       1.00      0.86      0.92         7\n",
      "         245       1.00      1.00      1.00         3\n",
      "         246       1.00      1.00      1.00        15\n",
      "         247       1.00      1.00      1.00       213\n",
      "         248       0.95      1.00      0.97        19\n",
      "         249       1.00      1.00      1.00        31\n",
      "         250       0.75      1.00      0.86         3\n",
      "         251       1.00      1.00      1.00         2\n",
      "         252       1.00      1.00      1.00         8\n",
      "         253       1.00      0.12      0.22         8\n",
      "         254       0.71      0.83      0.77         6\n",
      "         255       0.77      0.97      0.86        31\n",
      "         256       0.96      0.96      0.96        25\n",
      "         257       0.79      1.00      0.88        15\n",
      "         258       0.67      1.00      0.80         2\n",
      "         259       0.90      0.56      0.69        16\n",
      "         260       0.86      0.80      0.83        15\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      0.78      0.88         9\n",
      "         263       1.00      1.00      1.00         9\n",
      "         264       0.46      0.81      0.59        21\n",
      "         265       1.00      1.00      1.00         1\n",
      "         266       1.00      0.83      0.91        12\n",
      "         267       1.00      0.62      0.77         8\n",
      "         268       1.00      0.40      0.57         5\n",
      "         269       0.88      0.74      0.81        39\n",
      "         270       0.72      1.00      0.83        53\n",
      "         271       0.83      0.94      0.88       134\n",
      "         272       0.86      0.92      0.89        13\n",
      "         273       0.00      0.00      0.00         7\n",
      "         274       0.60      0.75      0.67         8\n",
      "         275       1.00      0.67      0.80         3\n",
      "         276       0.00      0.00      0.00         6\n",
      "         277       0.90      0.75      0.82        12\n",
      "         278       0.76      0.85      0.80        26\n",
      "         279       0.90      1.00      0.95         9\n",
      "         280       0.82      1.00      0.90        18\n",
      "         281       1.00      0.67      0.80         3\n",
      "         282       1.00      1.00      1.00         2\n",
      "         283       0.46      1.00      0.63         6\n",
      "         284       0.98      0.96      0.97        67\n",
      "         285       0.62      1.00      0.77         5\n",
      "         286       1.00      1.00      1.00        34\n",
      "         287       0.93      0.96      0.95       170\n",
      "         288       0.81      0.91      0.86        65\n",
      "         289       0.89      1.00      0.94         8\n",
      "         290       1.00      0.57      0.73        14\n",
      "         291       1.00      1.00      1.00         2\n",
      "         292       1.00      0.33      0.50         3\n",
      "         293       1.00      0.25      0.40         8\n",
      "         294       0.86      0.75      0.80         8\n",
      "         295       0.85      0.82      0.84        28\n",
      "         296       1.00      1.00      1.00        19\n",
      "         297       0.50      0.40      0.44         5\n",
      "         298       1.00      0.30      0.46        10\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       1.00      0.73      0.85        15\n",
      "         301       0.96      0.96      0.96        27\n",
      "         302       0.76      0.88      0.81        25\n",
      "         303       1.00      1.00      1.00         4\n",
      "         304       0.93      0.82      0.88        17\n",
      "         305       0.95      0.83      0.89        24\n",
      "         306       1.00      1.00      1.00        79\n",
      "         307       0.93      0.64      0.76        22\n",
      "         308       0.89      0.97      0.93        33\n",
      "         309       0.75      0.75      0.75         4\n",
      "         310       0.00      0.00      0.00        12\n",
      "         311       0.50      0.67      0.57         3\n",
      "         312       1.00      1.00      1.00         3\n",
      "         313       0.71      0.92      0.80        26\n",
      "         314       1.00      0.50      0.67         8\n",
      "         315       1.00      0.95      0.97        55\n",
      "         316       1.00      0.40      0.57         5\n",
      "         317       0.44      1.00      0.62         8\n",
      "         318       0.69      0.69      0.69        13\n",
      "         319       0.90      1.00      0.95        38\n",
      "         320       0.00      0.00      0.00         1\n",
      "         321       0.83      1.00      0.91        10\n",
      "         322       1.00      0.60      0.75         5\n",
      "         323       0.00      0.00      0.00        10\n",
      "         324       1.00      1.00      1.00        28\n",
      "         325       1.00      0.50      0.67         2\n",
      "         326       1.00      1.00      1.00        25\n",
      "         327       1.00      1.00      1.00         2\n",
      "         328       1.00      0.67      0.80         3\n",
      "         329       0.95      1.00      0.97        18\n",
      "         330       1.00      1.00      1.00         3\n",
      "         331       1.00      1.00      1.00        10\n",
      "         332       0.00      0.00      0.00        11\n",
      "         333       0.86      0.38      0.52        16\n",
      "         334       0.00      0.00      0.00         9\n",
      "         335       1.00      0.67      0.80         3\n",
      "         336       0.71      1.00      0.83         5\n",
      "         337       1.00      0.83      0.91         6\n",
      "         338       0.98      1.00      0.99        55\n",
      "         339       0.89      1.00      0.94        16\n",
      "         340       0.00      0.00      0.00         1\n",
      "         341       1.00      0.97      0.99        37\n",
      "         342       0.64      0.88      0.74         8\n",
      "         343       0.00      0.00      0.00         9\n",
      "         344       0.92      0.97      0.94       403\n",
      "         345       1.00      1.00      1.00         7\n",
      "         346       1.00      1.00      1.00         5\n",
      "         347       0.96      0.94      0.95      1254\n",
      "         348       0.73      0.57      0.64       145\n",
      "\n",
      "    accuracy                           0.90     10009\n",
      "   macro avg       0.76      0.71      0.72     10009\n",
      "weighted avg       0.89      0.90      0.88     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 3\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.85\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.75\n",
    "    else:\n",
    "        return lr * 0.7\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "model_3 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_2.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_3 = model_3.fit(X_train_combined_2, y_train_2, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_2, y_test_2),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_3, 'model_3_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_3, accuracy_3 = model_3.evaluate(X_test_combined_2, y_test_2)\n",
    "print(\"Accuracy for Stage 3 model:\", accuracy_3)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_3 = model_3.predict(X_test_combined_2)\n",
    "y_pred_3 = np.argmax(y_pred_probabilities_3, axis=1)\n",
    "report_3 = classification_report(y_test_2, y_pred_3)\n",
    "print(\"Classification Report for Stage 3 model:\")\n",
    "print(report_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 2.5191 - accuracy: 0.5539 - val_loss: 1.2217 - val_accuracy: 0.7234 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.3264 - accuracy: 0.7036 - val_loss: 0.7747 - val_accuracy: 0.8044 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.0495 - accuracy: 0.7438 - val_loss: 0.5684 - val_accuracy: 0.8464 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.9018 - accuracy: 0.7701 - val_loss: 0.5101 - val_accuracy: 0.8475 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8145 - accuracy: 0.7869 - val_loss: 0.4323 - val_accuracy: 0.8776 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7491 - accuracy: 0.8002 - val_loss: 0.4041 - val_accuracy: 0.8803 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6874 - accuracy: 0.8124 - val_loss: 0.3744 - val_accuracy: 0.8804 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6499 - accuracy: 0.8207 - val_loss: 0.3618 - val_accuracy: 0.8941 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6130 - accuracy: 0.8281 - val_loss: 0.3446 - val_accuracy: 0.9001 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5802 - accuracy: 0.8366 - val_loss: 0.3406 - val_accuracy: 0.9030 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5565 - accuracy: 0.8433 - val_loss: 0.3277 - val_accuracy: 0.9041 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5334 - accuracy: 0.8502 - val_loss: 0.3304 - val_accuracy: 0.9075 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5163 - accuracy: 0.8536 - val_loss: 0.3101 - val_accuracy: 0.9124 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5068 - accuracy: 0.8548 - val_loss: 0.3113 - val_accuracy: 0.9129 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4955 - accuracy: 0.8588 - val_loss: 0.3102 - val_accuracy: 0.9133 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4804 - accuracy: 0.8621 - val_loss: 0.3075 - val_accuracy: 0.9146 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4804 - accuracy: 0.8632 - val_loss: 0.3034 - val_accuracy: 0.9150 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4780 - accuracy: 0.8638 - val_loss: 0.2995 - val_accuracy: 0.9159 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.4717 - accuracy: 0.8651 - val_loss: 0.3027 - val_accuracy: 0.9168 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4622 - accuracy: 0.8680 - val_loss: 0.3028 - val_accuracy: 0.9171 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4689 - accuracy: 0.8660 - val_loss: 0.3025 - val_accuracy: 0.9157 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4688 - accuracy: 0.8654 - val_loss: 0.3023 - val_accuracy: 0.9164 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4602 - accuracy: 0.8677 - val_loss: 0.3015 - val_accuracy: 0.9173 - lr: 3.8203e-06\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2995 - accuracy: 0.9159\n",
      "Accuracy for Stage 4 model: 0.9158757328987122\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 4 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      1.00      1.00        56\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.50      0.88      0.64         8\n",
      "           5       0.50      0.17      0.25         6\n",
      "           6       0.60      0.75      0.67         4\n",
      "           7       0.82      0.93      0.88        15\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       0.00      0.00      0.00         3\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.66      1.00      0.79        23\n",
      "          12       1.00      1.00      1.00        22\n",
      "          13       0.97      0.97      0.97        33\n",
      "          14       0.95      0.95      0.95        22\n",
      "          15       0.77      0.91      0.83        11\n",
      "          16       1.00      1.00      1.00         6\n",
      "          17       0.67      0.44      0.53         9\n",
      "          18       0.83      1.00      0.91         5\n",
      "          19       1.00      0.62      0.77         8\n",
      "          20       1.00      0.20      0.33         5\n",
      "          21       0.92      1.00      0.96        24\n",
      "          22       0.70      1.00      0.82        28\n",
      "          23       1.00      1.00      1.00         3\n",
      "          24       0.00      0.00      0.00         3\n",
      "          25       1.00      0.11      0.20         9\n",
      "          26       0.86      1.00      0.92        12\n",
      "          27       0.90      1.00      0.95        26\n",
      "          28       0.79      1.00      0.88        11\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       1.00      1.00      1.00         3\n",
      "          31       1.00      1.00      1.00         4\n",
      "          32       0.00      0.00      0.00         4\n",
      "          33       1.00      1.00      1.00        33\n",
      "          34       1.00      1.00      1.00       131\n",
      "          35       0.75      1.00      0.86         6\n",
      "          36       1.00      0.90      0.95        10\n",
      "          37       0.00      0.00      0.00         3\n",
      "          38       1.00      0.62      0.77         8\n",
      "          39       0.80      1.00      0.89        39\n",
      "          40       0.94      0.96      0.95        47\n",
      "          41       0.78      1.00      0.88         7\n",
      "          42       0.50      0.67      0.57         3\n",
      "          43       0.60      1.00      0.75         3\n",
      "          44       0.79      0.74      0.77        31\n",
      "          45       0.00      0.00      0.00         2\n",
      "          46       1.00      1.00      1.00         3\n",
      "          47       1.00      0.50      0.67         2\n",
      "          48       1.00      0.92      0.96        13\n",
      "          49       0.00      0.00      0.00         4\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.50      1.00      0.67         5\n",
      "          52       1.00      1.00      1.00         6\n",
      "          53       1.00      0.67      0.80         3\n",
      "          54       1.00      1.00      1.00       101\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       0.90      0.94      0.92        48\n",
      "          57       0.65      0.87      0.74        15\n",
      "          58       0.67      0.67      0.67         3\n",
      "          59       0.00      0.00      0.00         2\n",
      "          60       0.76      1.00      0.86        25\n",
      "          61       0.00      0.00      0.00         4\n",
      "          62       1.00      1.00      1.00        36\n",
      "          63       1.00      1.00      1.00         5\n",
      "          64       1.00      0.83      0.91         6\n",
      "          65       1.00      0.60      0.75         5\n",
      "          66       0.00      0.00      0.00         4\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       1.00      0.77      0.87        13\n",
      "          69       0.00      0.00      0.00         5\n",
      "          70       1.00      0.67      0.80         3\n",
      "          71       0.71      0.77      0.74        13\n",
      "          72       0.00      0.00      0.00         5\n",
      "          73       1.00      0.86      0.92         7\n",
      "          74       0.90      1.00      0.95        18\n",
      "          75       0.93      1.00      0.96        41\n",
      "          76       0.76      1.00      0.87        13\n",
      "          77       1.00      1.00      1.00         7\n",
      "          78       1.00      1.00      1.00       184\n",
      "          79       1.00      1.00      1.00        26\n",
      "          80       0.55      1.00      0.71         6\n",
      "          81       1.00      0.33      0.50         6\n",
      "          82       0.77      1.00      0.87        54\n",
      "          83       1.00      0.17      0.29         6\n",
      "          84       1.00      1.00      1.00        14\n",
      "          85       0.56      1.00      0.71         5\n",
      "          86       0.91      1.00      0.96       204\n",
      "          87       1.00      1.00      1.00         6\n",
      "          88       1.00      0.88      0.93         8\n",
      "          89       0.76      1.00      0.87        13\n",
      "          90       0.83      1.00      0.91        10\n",
      "          91       0.88      0.58      0.70        12\n",
      "          92       0.94      0.94      0.94        18\n",
      "          93       0.67      1.00      0.80         8\n",
      "          94       0.54      1.00      0.70        26\n",
      "          95       1.00      0.33      0.50         3\n",
      "          96       0.00      0.00      0.00         5\n",
      "          97       0.59      1.00      0.74        23\n",
      "          98       1.00      0.95      0.98        44\n",
      "          99       0.60      1.00      0.75         3\n",
      "         100       1.00      1.00      1.00        31\n",
      "         101       0.75      1.00      0.86         3\n",
      "         102       0.85      0.65      0.73        17\n",
      "         103       0.78      0.98      0.87        61\n",
      "         104       0.82      1.00      0.90        14\n",
      "         105       1.00      0.62      0.77         8\n",
      "         106       0.62      1.00      0.76        13\n",
      "         107       0.40      1.00      0.57         2\n",
      "         109       1.00      0.31      0.48        32\n",
      "         110       1.00      0.86      0.92         7\n",
      "         111       0.00      0.00      0.00         2\n",
      "         112       1.00      1.00      1.00        30\n",
      "         113       0.40      1.00      0.57         2\n",
      "         114       0.89      1.00      0.94         8\n",
      "         115       1.00      0.50      0.67         2\n",
      "         116       1.00      1.00      1.00        14\n",
      "         117       1.00      1.00      1.00         3\n",
      "         118       0.88      1.00      0.94        15\n",
      "         119       0.77      0.83      0.80        36\n",
      "         120       0.92      0.92      0.92        13\n",
      "         121       1.00      1.00      1.00         3\n",
      "         122       1.00      0.40      0.57         5\n",
      "         123       0.00      0.00      0.00         1\n",
      "         124       1.00      0.33      0.50         6\n",
      "         125       0.82      0.97      0.89        29\n",
      "         126       1.00      1.00      1.00         4\n",
      "         127       0.62      0.91      0.74        11\n",
      "         128       1.00      1.00      1.00        13\n",
      "         129       0.93      1.00      0.96        26\n",
      "         130       0.75      0.75      0.75         8\n",
      "         131       0.39      0.90      0.55        10\n",
      "         132       0.00      0.00      0.00         4\n",
      "         133       0.61      0.95      0.75        40\n",
      "         134       0.75      0.30      0.43        10\n",
      "         135       1.00      1.00      1.00        19\n",
      "         136       1.00      0.83      0.91         6\n",
      "         137       1.00      0.73      0.84        11\n",
      "         138       0.96      0.90      0.92        48\n",
      "         139       0.60      0.43      0.50        14\n",
      "         140       1.00      0.75      0.86         8\n",
      "         141       1.00      0.77      0.87        13\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.67      1.00      0.80         2\n",
      "         144       0.98      1.00      0.99        80\n",
      "         145       0.98      1.00      0.99        41\n",
      "         146       0.95      0.95      0.95        20\n",
      "         147       1.00      1.00      1.00         2\n",
      "         148       0.70      1.00      0.82         7\n",
      "         149       1.00      1.00      1.00        21\n",
      "         150       0.50      0.71      0.59         7\n",
      "         151       1.00      0.67      0.80         6\n",
      "         152       0.00      0.00      0.00         7\n",
      "         153       0.89      1.00      0.94         8\n",
      "         154       1.00      1.00      1.00         3\n",
      "         155       1.00      1.00      1.00         5\n",
      "         156       1.00      1.00      1.00         4\n",
      "         157       0.91      0.94      0.93        33\n",
      "         158       1.00      0.38      0.55        16\n",
      "         159       0.79      1.00      0.88        41\n",
      "         160       1.00      0.57      0.73         7\n",
      "         161       0.90      1.00      0.95         9\n",
      "         162       0.77      0.48      0.59        21\n",
      "         163       1.00      1.00      1.00         6\n",
      "         164       1.00      0.40      0.57         5\n",
      "         165       0.93      1.00      0.96        13\n",
      "         166       1.00      1.00      1.00         6\n",
      "         167       0.58      1.00      0.73        11\n",
      "         168       1.00      0.33      0.50         3\n",
      "         169       1.00      0.25      0.40         4\n",
      "         170       1.00      0.33      0.50         3\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       1.00      1.00      1.00         3\n",
      "         173       1.00      1.00      1.00        23\n",
      "         174       1.00      1.00      1.00         6\n",
      "         175       1.00      1.00      1.00        25\n",
      "         176       0.83      0.42      0.56        12\n",
      "         177       0.80      0.80      0.80         5\n",
      "         178       1.00      0.67      0.80         3\n",
      "         179       1.00      0.86      0.92         7\n",
      "         180       1.00      0.78      0.88         9\n",
      "         181       0.00      0.00      0.00         4\n",
      "         182       1.00      1.00      1.00         4\n",
      "         183       0.75      0.50      0.60        36\n",
      "         184       1.00      0.91      0.95        11\n",
      "         185       1.00      0.83      0.91         6\n",
      "         186       0.00      0.00      0.00         6\n",
      "         187       0.92      1.00      0.96        11\n",
      "         188       0.90      1.00      0.95         9\n",
      "         189       0.93      0.87      0.90        46\n",
      "         190       0.61      0.89      0.73        37\n",
      "         191       0.78      0.99      0.87        69\n",
      "         192       0.73      0.92      0.81        12\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       1.00      0.83      0.91         6\n",
      "         196       0.86      1.00      0.92         6\n",
      "         198       0.97      0.94      0.96        34\n",
      "         199       1.00      1.00      1.00        17\n",
      "         200       1.00      1.00      1.00         6\n",
      "         201       0.95      1.00      0.98        21\n",
      "         202       1.00      0.67      0.80         3\n",
      "         203       0.88      0.78      0.82         9\n",
      "         204       1.00      1.00      1.00         7\n",
      "         205       0.74      1.00      0.85        14\n",
      "         206       0.85      0.94      0.89        65\n",
      "         207       1.00      1.00      1.00         4\n",
      "         208       1.00      0.33      0.50         3\n",
      "         209       1.00      0.40      0.57         5\n",
      "         210       0.00      0.00      0.00         1\n",
      "         211       0.96      0.96      0.96        91\n",
      "         212       0.94      0.89      0.92        38\n",
      "         213       0.50      0.50      0.50         4\n",
      "         214       1.00      1.00      1.00         5\n",
      "         215       0.60      0.86      0.71         7\n",
      "         216       0.75      0.75      0.75         4\n",
      "         217       1.00      1.00      1.00         1\n",
      "         218       1.00      1.00      1.00        47\n",
      "         219       0.99      1.00      1.00       110\n",
      "         220       1.00      0.89      0.94        19\n",
      "         221       0.85      0.92      0.88        12\n",
      "         222       0.90      1.00      0.95        19\n",
      "         223       1.00      1.00      1.00        19\n",
      "         224       1.00      1.00      1.00         2\n",
      "         226       1.00      1.00      1.00        11\n",
      "         227       0.00      0.00      0.00         1\n",
      "         228       0.96      0.76      0.85        29\n",
      "         229       0.00      0.00      0.00         5\n",
      "         230       0.80      1.00      0.89         4\n",
      "         231       1.00      1.00      1.00        10\n",
      "         232       1.00      1.00      1.00        28\n",
      "         233       1.00      0.88      0.93         8\n",
      "         234       0.62      0.80      0.70        10\n",
      "         235       1.00      0.50      0.67         6\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       1.00      0.22      0.36         9\n",
      "         238       0.77      1.00      0.87        27\n",
      "         239       0.73      0.73      0.73        15\n",
      "         240       0.88      1.00      0.94        23\n",
      "         241       1.00      1.00      1.00         5\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.72      0.90      0.80        29\n",
      "         244       0.50      0.67      0.57         3\n",
      "         245       0.00      0.00      0.00         4\n",
      "         246       0.00      0.00      0.00         5\n",
      "         247       0.89      0.73      0.80        11\n",
      "         248       1.00      1.00      1.00         2\n",
      "         249       1.00      0.78      0.88         9\n",
      "         250       1.00      1.00      1.00         1\n",
      "         251       1.00      1.00      1.00         2\n",
      "         252       0.96      1.00      0.98        51\n",
      "         253       1.00      0.88      0.93        16\n",
      "         254       0.00      0.00      0.00         2\n",
      "         255       1.00      1.00      1.00        18\n",
      "         256       1.00      0.42      0.59        12\n",
      "         257       0.80      1.00      0.89        20\n",
      "         258       1.00      0.50      0.67        10\n",
      "         259       0.76      0.87      0.81        15\n",
      "         260       0.73      1.00      0.84        16\n",
      "         261       1.00      0.20      0.33         5\n",
      "         262       0.67      0.67      0.67         3\n",
      "         263       0.80      0.67      0.73         6\n",
      "         264       1.00      0.10      0.18        10\n",
      "         265       0.80      0.80      0.80         5\n",
      "         266       0.67      0.50      0.57         8\n",
      "         267       0.50      0.25      0.33         4\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.25      0.50      0.33         2\n",
      "         270       0.89      0.67      0.76        12\n",
      "         271       0.65      0.85      0.73        13\n",
      "         272       1.00      0.25      0.40         4\n",
      "         273       1.00      0.90      0.95        10\n",
      "         274       0.93      0.88      0.90        16\n",
      "         275       0.81      1.00      0.90        13\n",
      "         276       0.77      1.00      0.87        23\n",
      "         277       0.85      0.94      0.89        36\n",
      "         278       0.92      0.97      0.94        91\n",
      "         279       1.00      1.00      1.00        11\n",
      "         280       1.00      1.00      1.00        29\n",
      "         281       0.88      0.88      0.88        16\n",
      "         282       0.00      0.00      0.00         3\n",
      "         283       0.76      0.94      0.84        17\n",
      "         284       0.92      0.95      0.94        61\n",
      "         285       1.00      0.36      0.53        11\n",
      "         286       0.00      0.00      0.00         8\n",
      "         287       0.80      0.80      0.80         5\n",
      "         288       0.58      0.85      0.69        13\n",
      "         289       0.88      1.00      0.94        22\n",
      "         290       0.93      1.00      0.97        28\n",
      "         291       0.44      1.00      0.62         4\n",
      "         292       0.67      1.00      0.80        10\n",
      "         293       1.00      0.67      0.80         3\n",
      "         294       1.00      1.00      1.00         1\n",
      "         295       0.89      0.89      0.89         9\n",
      "         296       1.00      0.50      0.67         2\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       1.00      1.00      1.00        10\n",
      "         299       0.83      1.00      0.91         5\n",
      "         300       1.00      1.00      1.00        34\n",
      "         301       0.00      0.00      0.00         4\n",
      "         302       1.00      1.00      1.00        15\n",
      "         303       0.00      0.00      0.00         2\n",
      "         304       0.90      0.75      0.82        24\n",
      "         305       0.75      1.00      0.86         9\n",
      "         306       1.00      0.60      0.75         5\n",
      "         307       0.95      1.00      0.97        19\n",
      "         308       0.73      1.00      0.84         8\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       1.00      1.00      1.00         3\n",
      "         311       0.92      1.00      0.96       358\n",
      "         312       1.00      0.62      0.76        13\n",
      "         313       0.67      1.00      0.80         2\n",
      "         314       0.97      0.97      0.97      4512\n",
      "         315       0.69      0.50      0.58       300\n",
      "\n",
      "    accuracy                           0.92     10009\n",
      "   macro avg       0.76      0.73      0.72     10009\n",
      "weighted avg       0.91      0.92      0.91     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 4\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_4 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_3.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_4.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_4 = model_4.fit(X_train_combined_3, y_train_3, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_3, y_test_3),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_4, 'model_4_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_4, accuracy_4 = model_4.evaluate(X_test_combined_3, y_test_3)\n",
    "print(\"Accuracy for Stage 4 model:\", accuracy_4)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_4 = model_4.predict(X_test_combined_3)\n",
    "y_pred_4 = np.argmax(y_pred_probabilities_4, axis=1)\n",
    "report_4 = classification_report(y_test_3, y_pred_4)\n",
    "print(\"Classification Report for Stage 4 model:\")\n",
    "print(report_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 5 model...\n",
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.5423 - accuracy: 0.7581 - val_loss: 0.5663 - val_accuracy: 0.8785 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6838 - accuracy: 0.8587 - val_loss: 0.4186 - val_accuracy: 0.9025 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5449 - accuracy: 0.8736 - val_loss: 0.3294 - val_accuracy: 0.9112 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4665 - accuracy: 0.8844 - val_loss: 0.2741 - val_accuracy: 0.9200 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4180 - accuracy: 0.8925 - val_loss: 0.2524 - val_accuracy: 0.9310 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3787 - accuracy: 0.8972 - val_loss: 0.2349 - val_accuracy: 0.9314 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3499 - accuracy: 0.9032 - val_loss: 0.2184 - val_accuracy: 0.9327 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3216 - accuracy: 0.9100 - val_loss: 0.2072 - val_accuracy: 0.9366 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3027 - accuracy: 0.9130 - val_loss: 0.1970 - val_accuracy: 0.9389 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2952 - accuracy: 0.9163 - val_loss: 0.1985 - val_accuracy: 0.9409 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2804 - accuracy: 0.9199 - val_loss: 0.1881 - val_accuracy: 0.9441 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2756 - accuracy: 0.9207 - val_loss: 0.1838 - val_accuracy: 0.9446 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2664 - accuracy: 0.9232 - val_loss: 0.1817 - val_accuracy: 0.9451 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2551 - accuracy: 0.9253 - val_loss: 0.1772 - val_accuracy: 0.9457 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2535 - accuracy: 0.9240 - val_loss: 0.1776 - val_accuracy: 0.9447 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2487 - accuracy: 0.9270 - val_loss: 0.1791 - val_accuracy: 0.9454 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2439 - accuracy: 0.9273 - val_loss: 0.1764 - val_accuracy: 0.9450 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2421 - accuracy: 0.9295 - val_loss: 0.1751 - val_accuracy: 0.9459 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2388 - accuracy: 0.9305 - val_loss: 0.1751 - val_accuracy: 0.9461 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2340 - accuracy: 0.9301 - val_loss: 0.1736 - val_accuracy: 0.9466 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2307 - accuracy: 0.9297 - val_loss: 0.1753 - val_accuracy: 0.9463 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2364 - accuracy: 0.9285 - val_loss: 0.1742 - val_accuracy: 0.9458 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2363 - accuracy: 0.9293 - val_loss: 0.1747 - val_accuracy: 0.9466 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2383 - accuracy: 0.9293 - val_loss: 0.1736 - val_accuracy: 0.9463 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2346 - accuracy: 0.9292 - val_loss: 0.1739 - val_accuracy: 0.9466 - lr: 1.3753e-06\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1736 - accuracy: 0.9466\n",
      "Accuracy for Stage 5 model: 0.9466480016708374\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 5 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        13\n",
      "           1       0.85      0.94      0.89        18\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      0.80      0.89         5\n",
      "           5       0.78      1.00      0.88        71\n",
      "           6       1.00      0.60      0.75         5\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       1.00      1.00      1.00         1\n",
      "          10       0.93      0.84      0.89        32\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.67      0.67      0.67         6\n",
      "          13       0.55      0.67      0.60         9\n",
      "          14       0.00      0.00      0.00         4\n",
      "          15       0.59      1.00      0.74        10\n",
      "          16       0.50      0.59      0.54        17\n",
      "          17       0.76      0.91      0.83        65\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.50      1.00      0.67         6\n",
      "          20       0.22      0.40      0.29         5\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.92      0.85      0.88        13\n",
      "          23       1.00      0.50      0.67         2\n",
      "          24       0.75      0.86      0.80         7\n",
      "          25       0.50      0.93      0.65        14\n",
      "          26       0.93      0.98      0.96        88\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00         8\n",
      "          29       1.00      1.00      1.00         3\n",
      "          31       0.45      0.71      0.56         7\n",
      "          32       1.00      0.91      0.95        11\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      1.00      1.00        14\n",
      "          35       0.97      0.99      0.98       140\n",
      "          36       0.53      0.62      0.57        13\n",
      "          37       0.82      0.96      0.89        28\n",
      "          38       0.80      0.31      0.44        13\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.50      0.67         4\n",
      "          41       0.50      0.86      0.63         7\n",
      "          42       0.62      1.00      0.77         5\n",
      "          43       0.00      0.00      0.00         3\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00         9\n",
      "          46       0.80      1.00      0.89        16\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       1.00      1.00      1.00        54\n",
      "          49       0.00      0.00      0.00         6\n",
      "          50       0.75      1.00      0.86         6\n",
      "          51       0.00      0.00      0.00         5\n",
      "          52       0.90      1.00      0.95         9\n",
      "          53       0.87      1.00      0.93        13\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       1.00      0.60      0.75         5\n",
      "          57       0.50      0.50      0.50         2\n",
      "          58       0.53      1.00      0.69        19\n",
      "          59       0.67      1.00      0.80         4\n",
      "          60       0.60      0.60      0.60         5\n",
      "          61       0.54      0.88      0.67         8\n",
      "          62       0.96      1.00      0.98        54\n",
      "          63       0.00      0.00      0.00         3\n",
      "          64       1.00      0.67      0.80        12\n",
      "          65       1.00      0.30      0.46        10\n",
      "          66       0.65      1.00      0.79        11\n",
      "          67       0.25      0.17      0.20         6\n",
      "          68       1.00      0.75      0.86         8\n",
      "          69       0.29      1.00      0.44         2\n",
      "          70       0.00      0.00      0.00         4\n",
      "          71       0.00      0.00      0.00         6\n",
      "          72       0.75      1.00      0.86         6\n",
      "          73       0.71      0.77      0.74        22\n",
      "          74       0.62      0.62      0.62        16\n",
      "          75       0.00      0.00      0.00         3\n",
      "          76       0.82      0.60      0.69        15\n",
      "          77       0.56      1.00      0.71         5\n",
      "          78       0.00      0.00      0.00         7\n",
      "          80       0.00      0.00      0.00         4\n",
      "          81       1.00      0.78      0.88         9\n",
      "          82       1.00      0.94      0.97        17\n",
      "          83       1.00      0.90      0.95        10\n",
      "          84       1.00      1.00      1.00         6\n",
      "          85       0.33      0.60      0.43         5\n",
      "          86       0.50      0.11      0.18         9\n",
      "          87       0.91      1.00      0.95        30\n",
      "          88       0.00      0.00      0.00         3\n",
      "          89       0.82      1.00      0.90         9\n",
      "          90       0.47      0.82      0.60        11\n",
      "          91       1.00      1.00      1.00         1\n",
      "          92       0.00      0.00      0.00         1\n",
      "          93       0.64      0.75      0.69        12\n",
      "          94       0.00      0.00      0.00         5\n",
      "          95       0.60      0.43      0.50         7\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       1.00      1.00      1.00         2\n",
      "          98       0.00      0.00      0.00         3\n",
      "          99       1.00      1.00      1.00        13\n",
      "         100       0.91      1.00      0.95        10\n",
      "         101       0.69      1.00      0.82         9\n",
      "         102       0.67      0.50      0.57         4\n",
      "         103       0.00      0.00      0.00         2\n",
      "         104       1.00      1.00      1.00         2\n",
      "         105       0.86      1.00      0.92         6\n",
      "         106       0.94      1.00      0.97        15\n",
      "         107       1.00      0.67      0.80         3\n",
      "         108       0.91      1.00      0.95        10\n",
      "         109       0.40      1.00      0.57        20\n",
      "         110       0.88      0.88      0.88         8\n",
      "         111       0.97      1.00      0.98        29\n",
      "         112       0.00      0.00      0.00         2\n",
      "         113       0.57      0.67      0.62         6\n",
      "         114       0.76      1.00      0.87        13\n",
      "         115       1.00      0.07      0.13        14\n",
      "         116       0.54      1.00      0.70        15\n",
      "         117       1.00      0.17      0.29        18\n",
      "         118       1.00      0.60      0.75         5\n",
      "         119       0.82      1.00      0.90        42\n",
      "         120       0.00      0.00      0.00         2\n",
      "         121       0.00      0.00      0.00         4\n",
      "         122       0.00      0.00      0.00         3\n",
      "         123       1.00      0.33      0.50         3\n",
      "         124       0.00      0.00      0.00         1\n",
      "         125       0.80      1.00      0.89         4\n",
      "         126       1.00      1.00      1.00         5\n",
      "         127       1.00      1.00      1.00         6\n",
      "         128       0.00      0.00      0.00         6\n",
      "         129       0.75      0.75      0.75         4\n",
      "         130       0.00      0.00      0.00         2\n",
      "         131       0.00      0.00      0.00         4\n",
      "         132       0.67      0.25      0.36         8\n",
      "         133       0.00      0.00      0.00         5\n",
      "         134       0.44      1.00      0.61        14\n",
      "         135       0.50      0.33      0.40         3\n",
      "         136       1.00      1.00      1.00         3\n",
      "         137       1.00      1.00      1.00         3\n",
      "         138       0.00      0.00      0.00        10\n",
      "         139       0.00      0.00      0.00         1\n",
      "         140       0.64      0.78      0.70         9\n",
      "         141       0.00      0.00      0.00         4\n",
      "         142       0.44      1.00      0.61         7\n",
      "         143       0.74      0.74      0.74        19\n",
      "         144       0.85      1.00      0.92        40\n",
      "         145       1.00      0.20      0.33         5\n",
      "         146       0.93      0.93      0.93        14\n",
      "         147       1.00      0.33      0.50         3\n",
      "         148       1.00      0.86      0.92         7\n",
      "         149       0.75      0.50      0.60         6\n",
      "         150       0.50      0.75      0.60         8\n",
      "         151       0.33      0.12      0.18         8\n",
      "         152       1.00      0.50      0.67         6\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.46      1.00      0.63        13\n",
      "         155       0.00      0.00      0.00         2\n",
      "         156       0.00      0.00      0.00         3\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.94      0.72      0.82        40\n",
      "         159       0.94      0.65      0.77        26\n",
      "         160       0.99      0.99      0.99      8119\n",
      "         161       0.65      0.50      0.57       106\n",
      "\n",
      "    accuracy                           0.95     10009\n",
      "   macro avg       0.58      0.58      0.55     10009\n",
      "weighted avg       0.94      0.95      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 5\n",
    "print(\"Running Stage 5 model...\")\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "# Create model\n",
    "model_5 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_4.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_5 = model_5.fit(X_train_combined_4, y_train_4, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_4, y_test_4),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_5, 'model_5_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_5, accuracy_5 = model_5.evaluate(X_test_combined_4, y_test_4)\n",
    "print(\"Accuracy for Stage 5 model:\", accuracy_5)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_5 = model_5.predict(X_test_combined_4)\n",
    "y_pred_5 = np.argmax(y_pred_probabilities_5, axis=1)\n",
    "report_5 = classification_report(y_test_4, y_pred_5)\n",
    "print(\"Classification Report for Stage 5 model:\")\n",
    "print(report_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
