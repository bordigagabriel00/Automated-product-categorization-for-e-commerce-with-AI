{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjKToYfc6adY",
    "outputId": "e010d05c-aab8-4947-c72f-255032e61ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\2213573283.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lASrrK4Z6msI",
    "outputId": "a79cb80c-5c32-4ecc-82b2-1689a695c3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50041 entries, 0 to 50040\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   name             50040 non-null  object \n",
      " 1   type             50041 non-null  object \n",
      " 2   price            50041 non-null  float64\n",
      " 3   description      50041 non-null  object \n",
      " 4   manufacturer     49975 non-null  object \n",
      " 5   url              50040 non-null  object \n",
      " 6   parent_category  50041 non-null  object \n",
      " 7   sub_category_1   49294 non-null  object \n",
      " 8   sub_category_2   43948 non-null  object \n",
      " 9   sub_category_3   27955 non-null  object \n",
      " 10  sub_category_4   9788 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\assignment\\alpha2_dataset_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(pd.NA)\n",
    "\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OUzBSTQi6p8v"
   },
   "outputs": [],
   "source": [
    "# stemmer, lemmatizer and stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import Optional\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8martjTt6sdd",
    "outputId": "04595e57-b78b-4702-9291-3d7251bcc274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50041, 13)\n"
     ]
    }
   ],
   "source": [
    "normalization = ['name', 'description']\n",
    "for column in normalization:\n",
    "    df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                                      Duracell - AAA Batteries (4-Pack)\n",
      "type                                                               HardGood\n",
      "price                                                                  5.49\n",
      "description               Compatible with select electronic devices; AAA...\n",
      "manufacturer                                                       Duracell\n",
      "url                                           duracell aaa batteries 4 pack\n",
      "parent_category                                 Connected Home & Housewares\n",
      "sub_category_1                                                   Housewares\n",
      "sub_category_2                                          Household Batteries\n",
      "sub_category_3                                           Alkaline Batteries\n",
      "sub_category_4                                                         <NA>\n",
      "name_normalized                                        duracell aaa battery\n",
      "description_normalized    compatible select electronic device aaa size d...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HYTfY_RArpEX"
   },
   "outputs": [],
   "source": [
    "#df['sub_category_1'].fillna('0', inplace=True)\n",
    "#df['sub_category_2'].fillna('0', inplace=True)\n",
    "#df['sub_category_3'].fillna('0', inplace=True)\n",
    "#df['sub_category_4'].fillna('0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwLSa6uB6upR",
    "outputId": "3b01d80e-984e-4e8f-ffb0-73b48cd31694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape\n",
    "X = df.drop(columns=['parent_category'])\n",
    "y = df['parent_category']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "woBvshjI-gHS"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "X_1 = df.drop(columns=['sub_category_1'])\n",
    "y_1 = df['sub_category_1']\n",
    "y_1.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wYMN3-Aj-f-n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_2 = df.drop(columns=['sub_category_2'])\n",
    "y_2 = df['sub_category_2']\n",
    "y_2.fillna('missing', inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_2_encoded = label_encoder.fit_transform(y_2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sLrnJPS1-f3n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_3 = df.drop(columns=['sub_category_3'])\n",
    "y_3 = df['sub_category_3']\n",
    "y_3.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_3_encoded = label_encoder.fit_transform(y_3)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MmO58b37-foH"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_4 = df.drop(columns=['sub_category_4'])\n",
    "y_4 = df['sub_category_4']\n",
    "y_4.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_4_encoded = label_encoder.fit_transform(y_4)\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4, y_4_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxJvoJcBsGfp",
    "outputId": "8064db2d-ac1c-4896-9349-64285425f5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "print(X_4.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_ArCp4U6xB_",
    "outputId": "d5cd1931-8036-4eea-de8f-8f3daa08bcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40032, 12)\n",
      "(10009, 12)\n",
      "(40032,)\n",
      "(10009,)\n",
      "9233                            star fox preowned nintendo\n",
      "25631    pioneer networkready ultra hd passthrough av h...\n",
      "19030                     evolve ultimate edition xbox one\n",
      "12044    joby pro series ultraplate quickrelease plate ...\n",
      "18967    aluratek bump w home audio speaker system ipod...\n",
      "                               ...                        \n",
      "11284    samsung class diag lead curved smart ultra hd ...\n",
      "44732    hifonics brutus class mono mosfet subwoofer am...\n",
      "38158    mobile edge premium laptop backpack apple macb...\n",
      "860                                 presonus presonus gray\n",
      "15795      insignia portable bluetooth stereo speaker blue\n",
      "Name: name_normalized, Length: 40032, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train['name_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXktfLf6y1M",
    "outputId": "4b9e8a29-b99b-4ab7-f153-2346eeee9663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  1, 20, ...,  6, 16,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdo3ct0S6zga"
   },
   "source": [
    "One Hot Encoder y Scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMYODyT5cLA"
   },
   "source": [
    "Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHamdvAY64SL",
    "outputId": "c312cd27-8ae2-4f37-bdde-f90f6b6e38ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded: (40032, 2195)\n",
      "Data type of X_train_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded: float64\n",
      "Shape of X_test_encoded: (10009, 2195)\n",
      "Data type of X_test_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded: float64\n",
      "Shape of X_train_scaled: (40032, 1)\n",
      "Data type of elements in X_train_scaled: float64\n",
      "Shape of X_test_scaled: (10009, 1)\n",
      "Data type of elements in X_test_scaled: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "numerical_columns = ['price']\n",
    "text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns]) \n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Data type of elements in X_train_scaled:\", X_train_scaled.dtype)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2195)\n",
      "(1, 2196)\n",
      "WARNING:tensorflow:5 out of the last 317 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000208D41C25F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "                      \n",
    "user_input = {\n",
    "    'name': 'Sample Product',\n",
    "    'description': 'This is a sample product description. It is used for demonstration purposes.',\n",
    "    'price': 999.99,\n",
    "    'type': 'Sample Type',\n",
    "    'manufacturer': 'Sample Manufacturer',\n",
    "}\n",
    "\n",
    "\n",
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def extract_last_hidden_state(embeddings):\n",
    "    return embeddings[:, -1, :]\n",
    "    \n",
    "def tokenize_and_get_embeddings(column):\n",
    "    # Tokenize text\n",
    "    if isinstance(column, str):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    elif isinstance(column, list) and all(isinstance(item, str) for item in column):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input format.\")\n",
    "    \n",
    "    outputs = model(tokenized_text)\n",
    "    embeddings = outputs.last_hidden_state.numpy()\n",
    "    \n",
    "    return embeddings\n",
    "    \n",
    "def prepare_input(user_input, scaler, encoder, categorical_columns):\n",
    "\n",
    "    user_input['name'] = normalize_text(user_input['name'])\n",
    "    user_input['description'] = normalize_text(user_input['description'])\n",
    "\n",
    "    name_embeddings = tokenize_and_get_embeddings(user_input['name'])\n",
    "    description_embeddings = tokenize_and_get_embeddings(user_input['description'])\n",
    "    extracted_name_hidden = extract_last_hidden_state(name_embeddings)\n",
    "    extracted_description_hidden = extract_last_hidden_state(description_embeddings)\n",
    "    scaled_price = scaler.transform([[user_input['price']]])[0, 0]\n",
    "\n",
    "    encoded_user_input = [[user_input[column]] for column in categorical_columns]\n",
    "    encoded_user_input = np.array(encoded_user_input).reshape(1, -1)\n",
    "    encoded_categorical_features = encoder.transform(encoded_user_input)\n",
    "    print(encoded_categorical_features.shape)\n",
    "\n",
    "    combined_features = hstack([encoded_categorical_features, np.array([[scaled_price]])])\n",
    "    print(combined_features.shape)\n",
    "    num_cat_input_array = combined_features.toarray().astype(np.float32)\n",
    "    final_input_array = np.concatenate((num_cat_input_array,extracted_name_hidden,extracted_description_hidden), axis=1)\n",
    "\n",
    "    return name_embeddings, description_embeddings, final_input_array\n",
    "\n",
    "name_embeddings, description_embeddings, final_input_array = prepare_input(user_input, scaler, encoder, categorical_columns)\n",
    "\n",
    "\n",
    "categories_dict = {\n",
    "    'parent_category': {category: index for index, category in enumerate(df['parent_category'].unique())},\n",
    "    'sub_category_1': {category: index for index, category in enumerate(df['sub_category_1'].unique())},\n",
    "    'sub_category_2': {category: index for index, category in enumerate(df['sub_category_2'].unique())},\n",
    "    'sub_category_3': {category: index for index, category in enumerate(df['sub_category_3'].unique())},\n",
    "    'sub_category_4': {category: index for index, category in enumerate(df['sub_category_4'].unique())},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def predict_model_1(final_input_array, model_1):\n",
    "    predictions = model_1.predict(final_input_array)  # creo que este es el modelo correspondiente // acá cargo el h5 vector\n",
    "    subcategory_pred_labels = np.argmax(predictions, axis=1)  \n",
    "    return subcategory_pred_labels\n",
    "\n",
    "def compare_predictions(subcategory_pred_labels, categories_dict, category_type):\n",
    "    mapping = {v: k for k, v in categories_dict[category_type].items()}  \n",
    "    predicted_labels = [mapping[idx] for idx in subcategory_pred_labels] \n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "predicted_labels = predict_model_1(final_input_array, model_1)\n",
    "predicted_labels_2 = compare_predictions(predicted_labels, categories_dict, 'parent_category')\n",
    "\n",
    "#INCLUIR HASTA AQUI EN PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H/VG_X360/Games/B2G1_20130602']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14kj_745e3A"
   },
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rrKOHZ_4_AK",
    "outputId": "bd1b3f38-75f9-494e-dbf6-9bd87d1f6832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_1: (40032, 2218)\n",
      "Data type of X_train_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_1: float64\n",
      "Shape of X_test_encoded_1: (10009, 2218)\n",
      "Data type of X_test_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_1: float64\n",
      "Shape of X_train_scaled_1: (40032, 1)\n",
      "Data type of elements in X_train_scaled_1: float64\n",
      "Shape of X_test_scaled_1: (10009, 1)\n",
      "Data type of elements in X_test_scaled_1: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_1 = ['type', 'manufacturer', 'parent_category']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_1[categorical_columns_1] = X_train_1[categorical_columns_1].fillna('missing')\n",
    "X_test_1[categorical_columns_1] = X_test_1[categorical_columns_1].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_1 = encoder_1.fit_transform(X_train_1[categorical_columns_1])\n",
    "X_test_encoded_1 = encoder_1.transform(X_test_1[categorical_columns_1])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_1:\", X_train_encoded_1.shape)\n",
    "print(\"Data type of X_train_encoded_1:\", type(X_train_encoded_1))\n",
    "print(\"Data type of elements in X_train_encoded_1:\", X_train_encoded_1.dtype)\n",
    "print(\"Shape of X_test_encoded_1:\", X_test_encoded_1.shape)\n",
    "print(\"Data type of X_test_encoded_1:\", type(X_test_encoded_1))\n",
    "print(\"Data type of elements in X_test_encoded_1:\", X_test_encoded_1.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled_1 = scaler_1.fit_transform(X_train_1[numerical_columns])\n",
    "X_test_scaled_1 = scaler_1.transform(X_test_1[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_1:\", X_train_scaled_1.shape)\n",
    "print(\"Data type of elements in X_train_scaled_1:\", X_train_scaled_1.dtype)\n",
    "print(\"Shape of X_test_scaled_1:\", X_test_scaled_1.shape)\n",
    "print(\"Data type of elements in X_test_scaled_1:\", X_test_scaled_1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4uI4css5hqx"
   },
   "source": [
    "Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSlE-ZXJ4-5Q",
    "outputId": "cb5f926f-438b-42f9-ab8b-6c1f9f7ba5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_2: (40032, 2332)\n",
      "Data type of X_train_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_2: float64\n",
      "Shape of X_test_encoded_2: (10009, 2332)\n",
      "Data type of X_test_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_2: float64\n",
      "Shape of X_train_scaled_2: (40032, 1)\n",
      "Data type of elements in X_train_scaled_2: float64\n",
      "Shape of X_test_scaled_2: (10009, 1)\n",
      "Data type of elements in X_test_scaled_2: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_2 = ['type', 'manufacturer', 'parent_category', 'sub_category_1']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_2[categorical_columns_2] = X_train_2[categorical_columns_2].fillna('missing')\n",
    "X_test_2[categorical_columns_2] = X_test_2[categorical_columns_2].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_2 = encoder_2.fit_transform(X_train_2[categorical_columns_2])\n",
    "X_test_encoded_2 = encoder_2.transform(X_test_2[categorical_columns_2])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_2:\", X_train_encoded_2.shape)\n",
    "print(\"Data type of X_train_encoded_2:\", type(X_train_encoded_2))\n",
    "print(\"Data type of elements in X_train_encoded_2:\", X_train_encoded_2.dtype)\n",
    "print(\"Shape of X_test_encoded_2:\", X_test_encoded_2.shape)\n",
    "print(\"Data type of X_test_encoded_2:\", type(X_test_encoded_2))\n",
    "print(\"Data type of elements in X_test_encoded_2:\", X_test_encoded_2.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scaled_2 = scaler_2.fit_transform(X_train_2[numerical_columns])\n",
    "X_test_scaled_2 = scaler_2.transform(X_test_2[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_2:\", X_train_scaled_2.shape)\n",
    "print(\"Data type of elements in X_train_scaled_2:\", X_train_scaled_2.dtype)\n",
    "print(\"Shape of X_test_scaled_2:\", X_test_scaled_2.shape)\n",
    "print(\"Data type of elements in X_test_scaled_2:\", X_test_scaled_2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjEeJaua5jYm"
   },
   "source": [
    "Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UWrFjTJ4-y7",
    "outputId": "14c5078c-21d3-4614-8393-a3daa5372ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_3: (40032, 2681)\n",
      "Data type of X_train_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_3: float64\n",
      "Shape of X_test_encoded_3: (10009, 2681)\n",
      "Data type of X_test_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_3: float64\n",
      "Shape of X_train_scaled_3: (40032, 1)\n",
      "Data type of elements in X_train_scaled_3: float64\n",
      "Shape of X_test_scaled_3: (10009, 1)\n",
      "Data type of elements in X_test_scaled_3: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_3 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_3[categorical_columns_3] = X_train_3[categorical_columns_3].fillna('missing')\n",
    "X_test_3[categorical_columns_3] = X_test_3[categorical_columns_3].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_3 = encoder_3.fit_transform(X_train_3[categorical_columns_3])\n",
    "X_test_encoded_3 = encoder_3.transform(X_test_3[categorical_columns_3])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_3:\", X_train_encoded_3.shape)\n",
    "print(\"Data type of X_train_encoded_3:\", type(X_train_encoded_3))\n",
    "print(\"Data type of elements in X_train_encoded_3:\", X_train_encoded_3.dtype)\n",
    "print(\"Shape of X_test_encoded_3:\", X_test_encoded_3.shape)\n",
    "print(\"Data type of X_test_encoded_3:\", type(X_test_encoded_3))\n",
    "print(\"Data type of elements in X_test_encoded_3:\", X_test_encoded_3.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_3 = StandardScaler()\n",
    "X_train_scaled_3 = scaler_3.fit_transform(X_train_3[numerical_columns])\n",
    "X_test_scaled_3 = scaler_3.transform(X_test_3[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_3:\", X_train_scaled_3.shape)\n",
    "print(\"Data type of elements in X_train_scaled_3:\", X_train_scaled_3.dtype)\n",
    "print(\"Shape of X_test_scaled_3:\", X_test_scaled_3.shape)\n",
    "print(\"Data type of elements in X_test_scaled_3:\", X_test_scaled_3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX9LC3rA5k-T"
   },
   "source": [
    "Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E-BaTiE4-rh",
    "outputId": "0fc98aa5-895c-4d65-e0c7-d19d5a80f750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_4: (40032, 2997)\n",
      "Data type of X_train_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_4: float64\n",
      "Shape of X_test_encoded_4: (10009, 2997)\n",
      "Data type of X_test_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_4: float64\n",
      "Shape of X_train_scaled_4: (40032, 1)\n",
      "Data type of elements in X_train_scaled_4: float64\n",
      "Shape of X_test_scaled_4: (10009, 1)\n",
      "Data type of elements in X_test_scaled_4: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_4 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2', 'sub_category_3']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_4[categorical_columns_4] = X_train_4[categorical_columns_4].fillna('missing')\n",
    "X_test_4[categorical_columns_4] = X_test_4[categorical_columns_4].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_4 = encoder_4.fit_transform(X_train_4[categorical_columns_4])\n",
    "X_test_encoded_4 = encoder_4.transform(X_test_4[categorical_columns_4])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_4:\", X_train_encoded_4.shape)\n",
    "print(\"Data type of X_train_encoded_4:\", type(X_train_encoded_4))\n",
    "print(\"Data type of elements in X_train_encoded_4:\", X_train_encoded_4.dtype)\n",
    "print(\"Shape of X_test_encoded_4:\", X_test_encoded_4.shape)\n",
    "print(\"Data type of X_test_encoded_4:\", type(X_test_encoded_4))\n",
    "print(\"Data type of elements in X_test_encoded_4:\", X_test_encoded_4.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_4 = StandardScaler()\n",
    "X_train_scaled_4 = scaler_4.fit_transform(X_train_4[numerical_columns])\n",
    "X_test_scaled_4 = scaler_4.transform(X_test_4[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_4:\", X_train_scaled_4.shape)\n",
    "print(\"Data type of elements in X_train_scaled_4:\", X_train_scaled_4.dtype)\n",
    "print(\"Shape of X_test_scaled_4:\", X_test_scaled_4.shape)\n",
    "print(\"Data type of elements in X_test_scaled_4:\", X_test_scaled_4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CgYBDhCwHKQA"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Stage 1\n",
    "X_train_processed = hstack([X_train_encoded, X_train_scaled]).astype(np.float32).toarray()\n",
    "X_test_processed = hstack([X_test_encoded, X_test_scaled]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 2\n",
    "X_train_processed_1 = hstack([X_train_encoded_1, X_train_scaled_1]).astype(np.float32).toarray()\n",
    "X_test_processed_1 = hstack([X_test_encoded_1, X_test_scaled_1]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 3\n",
    "X_train_processed_2 = hstack([X_train_encoded_2, X_train_scaled_2]).astype(np.float32).toarray()\n",
    "X_test_processed_2 = hstack([X_test_encoded_2, X_test_scaled_2]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 4\n",
    "X_train_processed_3 = hstack([X_train_encoded_3, X_train_scaled_3]).astype(np.float32).toarray()\n",
    "X_test_processed_3 = hstack([X_test_encoded_3, X_test_scaled_3]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 5\n",
    "X_train_processed_4 = hstack([X_train_encoded_4, X_train_scaled_4]).astype(np.float32).toarray()\n",
    "X_test_processed_4 = hstack([X_test_encoded_4, X_test_scaled_4]).astype(np.float32).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvYoUCeHHjx_",
    "outputId": "61b6f3c1-d6bf-4b4a-fcd5-cc6f7e54ab90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed: float32\n",
      "Data type of X_test_processed: float32\n",
      "X_train_processed shape: (40032, 2196)\n",
      "X_test_processed shape: (10009, 2196)\n"
     ]
    }
   ],
   "source": [
    "# Dim 1\n",
    "print(\"Data type of X_train_processed:\", X_train_processed.dtype)\n",
    "print(\"Data type of X_test_processed:\", X_test_processed.dtype)\n",
    "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhfSkN9I1vK",
    "outputId": "180da8c5-683c-4361-98ce-f51e50cd01ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_1: float32\n",
      "Data type of X_test_processed_1: float32\n",
      "X_train_processed_1 shape: (40032, 2219)\n",
      "X_test_processed_1 shape: (10009, 2219)\n"
     ]
    }
   ],
   "source": [
    "# Dim 2\n",
    "print(\"Data type of X_train_processed_1:\", X_train_processed_1.dtype)\n",
    "print(\"Data type of X_test_processed_1:\", X_test_processed_1.dtype)\n",
    "print(\"X_train_processed_1 shape:\", X_train_processed_1.shape)\n",
    "print(\"X_test_processed_1 shape:\", X_test_processed_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Az3OeCXI1rI",
    "outputId": "3b70f69e-056d-4351-9485-bd9313c99b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_2: float32\n",
      "Data type of X_test_processed_2: float32\n",
      "X_train_processed_2 shape: (40032, 2333)\n",
      "X_test_processed_2 shape: (10009, 2333)\n"
     ]
    }
   ],
   "source": [
    "# Dim 3\n",
    "print(\"Data type of X_train_processed_2:\", X_train_processed_2.dtype)\n",
    "print(\"Data type of X_test_processed_2:\", X_test_processed_2.dtype)\n",
    "print(\"X_train_processed_2 shape:\", X_train_processed_2.shape)\n",
    "print(\"X_test_processed_2 shape:\", X_test_processed_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBSG8gbeI1ng",
    "outputId": "902e994e-05fa-4a26-997f-66ff75d4eef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_3: float32\n",
      "Data type of X_test_processed_3: float32\n",
      "X_train_processed_3 shape: (40032, 2682)\n",
      "X_test_processed_3 shape: (10009, 2682)\n"
     ]
    }
   ],
   "source": [
    "# Dim 4\n",
    "print(\"Data type of X_train_processed_3:\", X_train_processed_3.dtype)\n",
    "print(\"Data type of X_test_processed_3:\",X_test_processed_3.dtype)\n",
    "print(\"X_train_processed_3 shape:\", X_train_processed_3.shape)\n",
    "print(\"X_test_processed_3 shape:\", X_test_processed_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIkzAILmI5Iv",
    "outputId": "945746cf-2be2-42d4-f3d9-49007732e383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_4: float32\n",
      "Data type of X_test_processed_4: float32\n",
      "X_train_processed_4 shape: (40032, 2998)\n",
      "X_test_processed_4 shape: (10009, 2998)\n"
     ]
    }
   ],
   "source": [
    "# Dim 5\n",
    "print(\"Data type of X_train_processed_4:\", X_train_processed_4.dtype)\n",
    "print(\"Data type of X_test_processed_4:\", X_test_processed_4.dtype)\n",
    "print(\"X_train_processed_4 shape:\", X_train_processed_4.shape)\n",
    "print(\"X_test_processed_4 shape:\", X_test_processed_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define file paths to save the arrays\n",
    "file_paths = {\n",
    "    'X_train_processed.npy': X_train_processed,\n",
    "    'X_test_processed.npy': X_test_processed,\n",
    "    'X_train_processed_1.npy': X_train_processed_1,\n",
    "    'X_test_processed_1.npy': X_test_processed_1,\n",
    "    'X_train_processed_2.npy': X_train_processed_2,\n",
    "    'X_test_processed_2.npy': X_test_processed_2,\n",
    "    'X_train_processed_3.npy': X_train_processed_3,\n",
    "    'X_test_processed_3.npy': X_test_processed_3,\n",
    "    'X_train_processed_4.npy': X_train_processed_4,\n",
    "    'X_test_processed_4.npy': X_test_processed_4\n",
    "}\n",
    "\n",
    "# Save each array\n",
    "for file_name, array in file_paths.items():\n",
    "    np.save(file_name, array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EVfWlKtNKCOR"
   },
   "outputs": [],
   "source": [
    "X_train_name_embeddings_loaded = np.load('X_train_name_embeddings.npy', allow_pickle= True)\n",
    "X_train_description_embeddings_loaded = np.load('X_train_description_embeddings.npy',allow_pickle= True)\n",
    "X_test_name_embeddings_loaded = np.load('X_test_name_embeddings.npy', allow_pickle= True)\n",
    "X_test_description_embeddings_loaded = np.load('X_test_description_embeddings.npy', allow_pickle= True)\n",
    "\n",
    "# Load the saved NumPy arrays\n",
    "X_train_processed_loaded = np.load('X_train_processed.npy', allow_pickle=True)\n",
    "X_test_processed_loaded = np.load('X_test_processed.npy', allow_pickle=True)\n",
    "X_train_processed_1_loaded = np.load('X_train_processed_1.npy', allow_pickle=True)\n",
    "X_test_processed_1_loaded = np.load('X_test_processed_1.npy', allow_pickle=True)\n",
    "X_train_processed_2_loaded = np.load('X_train_processed_2.npy', allow_pickle=True)\n",
    "X_test_processed_2_loaded = np.load('X_test_processed_2.npy', allow_pickle=True)\n",
    "X_train_processed_3_loaded = np.load('X_train_processed_3.npy', allow_pickle=True)\n",
    "X_test_processed_3_loaded = np.load('X_test_processed_3.npy', allow_pickle=True)\n",
    "X_train_processed_4_loaded = np.load('X_train_processed_4.npy', allow_pickle=True)\n",
    "X_test_processed_4_loaded = np.load('X_test_processed_4.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name_last_hidden = np.array([x[0][-1] for x in X_train_name_embeddings_loaded])\n",
    "X_train_description_last_hidden = np.array([x[0][-1] for x in X_train_description_embeddings_loaded])\n",
    "X_test_name_last_hidden = np.array([x[0][-1] for x in X_test_name_embeddings_loaded])\n",
    "X_test_description_last_hidden = np.array([x[0][-1] for x in X_test_description_embeddings_loaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of X_train_name_last_hidden: [ 6.20537519e-01 -1.57028392e-01 -4.39105332e-01  5.87245941e-01\n",
      " -4.92567480e-01 -6.75922573e-01  3.66373003e-01 -7.91122556e-01\n",
      "  6.63730741e-01  4.76211309e-02  4.97472771e-02 -3.77824754e-01\n",
      " -5.29451743e-02 -3.52970883e-03 -7.33473897e-01 -2.74465412e-01\n",
      " -8.19092095e-02 -1.52533606e-01  7.26244077e-02 -4.86062840e-02\n",
      "  3.07591200e-01 -1.32735014e-01  8.47112298e-01  2.39423364e-01\n",
      "  1.34860486e-01  3.51377934e-01 -4.78440404e-01 -1.80202276e-02\n",
      " -2.26639926e-01 -3.52599382e-01 -6.21600211e-01 -2.67526209e-01\n",
      " -1.92636084e-02  3.99675339e-01  1.55551210e-01 -8.61436129e-02\n",
      "  3.39647025e-01 -4.60110493e-02 -5.71235001e-01 -4.14561182e-01\n",
      " -3.34018916e-01 -1.61173679e-02 -1.93653673e-01  6.32311583e-01\n",
      "  5.47741950e-02 -6.00057304e-01  6.22461319e-01  3.42181236e-01\n",
      " -2.25496083e-01  4.89310294e-01  2.93806463e-01  1.60992384e-01\n",
      " -7.31535181e-02  8.67539346e-02  5.76151237e-02  1.21880889e-01\n",
      "  2.94119507e-01 -4.34998453e-01  1.54902250e-01  2.93051004e-01\n",
      " -5.07322550e-02  6.77793384e-01 -2.17716694e-01 -1.94208890e-01\n",
      "  6.51268899e-01 -1.67188928e-01  2.17404403e-02 -3.18291545e-01\n",
      " -6.07384741e-01 -3.05744082e-01 -2.39384830e-01 -1.02852356e+00\n",
      "  6.55281067e-01  3.35646331e-01  3.62706840e-01  2.41505504e-01\n",
      " -3.31891775e-01  7.45274603e-01  1.18391871e-01  3.59835505e-01\n",
      "  3.76000077e-01 -3.06956947e-01 -1.38037503e-01  7.29502961e-02\n",
      "  5.02546012e-01  1.56718288e-02 -2.42989823e-01  4.20575216e-02\n",
      " -4.59762454e-01 -1.26091674e-01  3.46461594e-01  2.71966606e-01\n",
      "  3.90934289e-01 -3.59424323e-01 -1.19082555e-01  3.55039179e-01\n",
      " -6.93194717e-02 -9.02826637e-02 -2.01892227e-01 -1.14928253e-01\n",
      " -3.77589911e-01 -1.72685519e-01 -5.20075373e-02  8.87612224e-01\n",
      " -1.62285715e-01 -2.95471847e-01  4.46627766e-01  5.57538629e-01\n",
      "  2.56792873e-01  1.03553367e+00  5.43226182e-01  2.22238839e-01\n",
      "  1.73543707e-01 -6.79688826e-02 -4.89283442e-01 -5.52646935e-01\n",
      "  2.85861969e-01  2.83273697e-01  3.80976677e-01  2.35821024e-01\n",
      " -6.67967856e-01 -4.66061831e-01  5.99375069e-01  1.08154857e+00\n",
      " -1.25421882e-01 -3.18098664e-02  2.50165947e-02 -7.18430936e-01\n",
      "  9.47480053e-02 -7.66403854e-01 -3.98136169e-01  6.48566902e-01\n",
      "  2.57072091e-01  8.71164203e-01 -1.05473615e-01  2.04653174e-01\n",
      " -3.08098674e-01  5.59956357e-02 -6.77700162e-01  1.01367123e-01\n",
      " -3.72281134e-01  9.31154370e-01  6.59990549e-01 -1.15582418e+00\n",
      "  5.36541790e-02  6.25010610e-01  5.59506238e-01 -3.16040702e-02\n",
      "  4.84125316e-02 -3.06834847e-01  7.54427016e-01 -1.69089168e-01\n",
      " -2.54145682e-01 -2.30913237e-02 -5.35159945e-01  2.63796747e-01\n",
      " -1.52081802e-01  1.89791620e-02  4.29030150e-01  7.86521673e-01\n",
      "  2.92653799e-01  3.41052055e-01  3.65956217e-01  5.37699759e-01\n",
      " -5.17349124e-01  2.24996656e-01 -1.17838478e+00 -1.76467597e-01\n",
      "  1.01287454e-01  4.32819426e-01 -4.23639059e-01 -4.54228640e-01\n",
      "  7.73141310e-02  3.79181325e-01 -3.45093966e-01  3.52966428e-01\n",
      " -1.62603110e-01  8.23184326e-02 -2.84529865e-01 -6.81628287e-01\n",
      " -8.46690559e+00 -2.35476375e-01 -9.76541191e-02  2.22685993e-01\n",
      " -3.64512168e-02 -3.91305238e-01 -8.25014532e-01 -1.73412472e-01\n",
      "  3.74747545e-01 -8.75730038e-01 -1.22479826e-01 -1.77380100e-01\n",
      " -9.63426471e-01  8.22258413e-01  3.41257453e-01 -2.38749608e-01\n",
      "  2.57653482e-02  2.47886866e-01 -3.30268562e-01  3.26861292e-01\n",
      " -1.97851792e-01 -3.42524916e-01  9.84942913e-02  6.40032470e-01\n",
      " -2.42812037e-01 -1.43791389e+00  2.46120483e-01 -2.29564041e-01\n",
      "  5.11401772e-01  5.85837178e-02 -1.31213963e+00 -1.12423360e-01\n",
      " -3.62519175e-03 -7.86731243e-01  1.56666100e-01 -5.34660220e-01\n",
      "  4.07156013e-02 -5.64395070e-01 -9.06348109e-01 -2.95303822e-01\n",
      " -5.59946835e-01 -3.20167810e-01 -1.84398927e-02  2.15938147e-02\n",
      "  5.29996753e-01 -1.46843731e+00  3.76851022e-01  7.21755743e-01\n",
      "  6.36393428e-01  2.52907544e-01 -8.59751627e-02 -1.71806812e-01\n",
      "  7.80056894e-01  1.51630104e-01  2.96293616e-01  1.60506576e-01\n",
      " -6.33846670e-02 -5.69900796e-02 -5.88919036e-02 -6.84572160e-01\n",
      " -7.04938546e-04  2.46744156e-01  2.25677311e-01 -8.23454112e-02\n",
      " -5.27979791e-01 -5.08032322e-01 -1.84435844e-01  9.23074484e-01\n",
      "  7.35809445e-01 -4.10479635e-01 -2.21967623e-01 -5.50180316e-01\n",
      "  3.03729922e-01 -4.96013671e-01  5.50130188e-01  7.58742869e-01\n",
      "  5.94226606e-02 -4.39812601e-01  1.16967094e+00 -5.56134701e-01\n",
      "  4.20879245e-01  7.08146334e-01  3.12095106e-01  1.29798785e-01\n",
      " -6.85978770e-01  6.66718185e-01  4.30409670e-01  2.85361290e-01\n",
      " -3.43734175e-01  3.40557098e-01  4.58756328e-01 -1.30768120e-01\n",
      " -4.67268795e-01  5.87800920e-01 -1.30464152e-01 -1.07199299e+00\n",
      "  6.74340010e-01 -8.66399184e-02  3.83873284e-01 -1.83765680e-01\n",
      " -3.93801302e-01  2.69577444e-01 -5.39098233e-02  1.67211309e-01\n",
      "  2.02100113e-01 -4.57387537e-01 -8.52275074e-01 -1.42785877e-01\n",
      "  3.35738182e-01 -6.06438577e-01 -1.52843416e-01 -3.03091884e-01\n",
      " -8.47115964e-02  6.67190492e-01 -6.68784678e-02  6.33220226e-02\n",
      "  5.23253739e-01 -1.75359398e-02 -4.24083918e-01 -2.04071775e-01\n",
      "  2.14214072e-01 -7.02822745e-01  1.12920888e-01 -3.13348323e-02\n",
      " -2.11466819e-01 -1.50750086e-01  4.50119257e-01  1.24140903e-01\n",
      "  7.55189002e-01 -1.10170841e-01  2.85554498e-01 -6.85934961e-01\n",
      "  2.90729553e-01 -1.57349512e-01 -2.48254389e-02  7.73136169e-02\n",
      " -1.95251510e-01 -1.05724163e-01  1.81615889e-01 -4.38701361e-01\n",
      "  2.71253109e-01  2.69789636e-01  1.97617477e-03  4.88260210e-01\n",
      " -6.08754009e-02 -6.35233000e-02 -8.48654211e-01 -4.67859924e-01\n",
      " -4.04063538e-02  3.92924100e-01  4.71455455e-01 -1.16113812e-01\n",
      "  6.34941638e-01 -2.62468249e-01 -4.95653987e-01 -3.08744982e-02\n",
      "  3.34599704e-01 -8.20729434e-02 -3.69854458e-02 -4.42685336e-01\n",
      " -5.49810231e-01  7.30598047e-02  2.70330966e-01 -1.09698489e-01\n",
      " -2.68085241e-01  5.26564658e-01  2.56138742e-01 -1.54329166e-01\n",
      "  2.68264174e-01  6.50385201e-01  1.24321252e-01 -8.18036258e-01\n",
      " -7.96757817e-01 -3.77051294e-01 -2.34010071e-01 -4.08915311e-01\n",
      " -4.76937480e-02  3.31464559e-01  7.78878927e-01  2.71527432e-02\n",
      " -3.22321892e-01  6.98770434e-02 -4.85345066e-01 -7.42784023e-01\n",
      " -4.95838150e-02  2.48317659e-01 -6.83598459e-01 -2.63965130e-01\n",
      " -3.53393927e-02 -4.28863764e-01  3.14770162e-01  1.37096956e-01\n",
      " -3.14997852e-01  8.23536664e-02  2.95320004e-02  5.10599613e-01\n",
      " -3.71191561e-01 -7.13964030e-02  2.93738581e-02 -3.19731176e-01\n",
      " -5.60358286e-01 -6.47917867e-01 -4.73039091e-01  1.92232206e-01\n",
      " -4.33800578e-01  1.69615537e-01  2.53361315e-01  1.28410399e-01\n",
      "  2.39444315e-01 -5.11149056e-02  2.00972959e-01  4.52268481e-01\n",
      " -9.15191919e-02  7.91754797e-02  6.82296529e-02 -2.93338329e-01\n",
      "  4.06531304e-01 -4.03168797e-01  3.09270024e-01  6.50663003e-02\n",
      "  1.05046615e-01 -8.64279717e-02 -1.53797373e-01  4.02077079e-01\n",
      "  5.03367066e-01  7.63851777e-02  2.09626377e-01  1.53182715e-01\n",
      " -5.65243840e-01 -1.32839739e-01  6.27765238e-01 -3.82222414e-01\n",
      " -3.88237000e-01  4.48667586e-01 -4.94298071e-01 -4.10451323e-01\n",
      " -4.89565134e-01 -4.12086457e-01  5.01163781e-01 -6.34864271e-01\n",
      "  4.93994743e-01  3.40991318e-02 -7.70612210e-02 -5.23390770e-01\n",
      " -3.90536785e-01  9.76225793e-01 -2.31955290e-01  4.92922813e-01\n",
      "  2.07122847e-01  3.43242586e-01  6.75997019e-01  3.43689382e-01\n",
      " -7.34328479e-03 -7.62901306e-02 -1.36262581e-01 -3.49554360e-01\n",
      "  2.33103514e-01 -2.97866374e-01  1.42095387e-01  2.46145166e-02\n",
      "  6.91457465e-02 -7.26145744e-01 -2.68725991e-01  4.27473038e-01\n",
      "  8.14948857e-01  1.12877059e+00  5.72656631e-01  6.83157027e-01\n",
      "  2.91611552e-01  4.32616830e-01 -8.02971900e-01 -2.75246471e-01\n",
      "  1.94451630e-01  6.13069296e-01 -5.72932422e-01  2.11954072e-01\n",
      "  1.04959095e+00 -7.37529248e-02  1.31617635e-01  5.35595417e-01\n",
      "  5.16681373e-01 -9.72960353e-01  2.33713865e-01 -6.80972874e-01\n",
      " -4.43347394e-02  6.06121868e-02 -8.95191729e-01  6.56643152e-01\n",
      " -4.67941612e-01  1.72935054e-01  3.06211114e-01 -2.09628448e-01\n",
      " -4.73560929e-01 -7.77069032e-01  5.25709808e-01 -5.47120214e-01\n",
      " -4.76924509e-01  3.44251424e-01  5.33317804e-01 -1.14928357e-01\n",
      "  8.92056108e-01 -1.09298661e-01 -1.78937271e-01  4.77421999e-01\n",
      "  1.20972060e-02 -6.01830721e-01  1.15526125e-01  1.84282243e-01\n",
      "  1.20737918e-01 -8.41489851e-01  2.74324179e-01  6.53214380e-02\n",
      "  3.10614675e-01  1.87517211e-01 -3.63761693e-01 -4.86926019e-01\n",
      "  2.67216444e-01  7.08758384e-02  4.93382275e-01 -6.62216306e-01\n",
      " -8.03277671e-01  5.06674275e-02 -4.52651903e-02  8.58727932e-01\n",
      "  3.86713773e-01 -2.26233333e-01  2.94005901e-01  8.16828609e-02\n",
      "  4.67826068e-01  9.88701731e-02 -5.52253127e-01 -6.25416636e-02\n",
      " -4.36638117e-01 -1.23846389e-01  3.10184360e-01  1.10362805e-01\n",
      " -3.76352072e-01 -6.95727170e-01 -2.82260239e-01 -2.98604459e-01\n",
      " -5.82896113e-01 -7.70874918e-02  6.86300695e-01  4.38593328e-01\n",
      "  2.09501594e-01  1.93385899e-01  1.10943151e+00 -6.18437052e-01\n",
      " -5.46705842e-01 -3.53303879e-01 -4.70864028e-03  2.02219620e-01\n",
      "  1.96173102e-01 -8.64403367e-01 -6.46913230e-01  1.45509869e-01\n",
      " -6.57370865e-01  2.44000092e-01  6.11473203e-01  5.38556337e-01\n",
      " -6.71112657e-01 -4.79236394e-02  1.10620749e+00  2.27204785e-01\n",
      " -2.44950041e-01 -4.12875004e-02 -4.92067188e-02 -3.21430787e-02\n",
      " -3.99663895e-01 -2.86540627e-01 -4.12346452e-01  3.79928648e-02\n",
      "  3.89240086e-01 -1.03023134e-01  3.54121447e-01  1.34739459e+00\n",
      " -1.63724944e-02 -6.84470415e-01 -2.10255329e-02 -1.12153515e-01\n",
      "  4.70662117e-01  5.75038865e-02 -6.25386477e-01 -3.15227062e-01\n",
      " -4.69694585e-01 -4.81898487e-02 -1.43374532e-01 -3.73101383e-01\n",
      "  1.92334324e-01 -1.23024061e-01  1.10399842e+00 -8.03215504e-01\n",
      "  3.67378086e-01  8.49291235e-02 -2.87107050e-01 -3.32370818e-01\n",
      " -2.46271253e-01  4.15957689e-01  1.03018686e-01  4.71279770e-01\n",
      " -1.91447139e-01  3.02230835e-01 -6.40176773e-01  4.34333146e-01\n",
      "  4.13303167e-01 -2.25464195e-01 -2.43023872e-01 -2.54389524e-01\n",
      "  1.05514765e-01  4.62910652e-01 -4.52698588e-01  4.98102456e-02\n",
      " -2.12586150e-01 -5.03674865e-01  1.03526384e-01  2.60310173e-01\n",
      " -1.91697702e-01 -4.90235597e-01  9.73624110e-01  1.67644136e-02\n",
      " -3.09523135e-01 -5.45606792e-01 -3.38727415e-01  1.95590585e-01\n",
      " -3.95997971e-01  4.34039265e-01 -4.79470164e-01 -1.79774463e-01\n",
      " -1.33216277e-01  2.92425573e-01  1.17567241e-01 -2.01351941e-04\n",
      " -2.28282869e-01 -2.98369490e-03  1.14583872e-01  1.87827110e-01\n",
      " -6.16419092e-02 -2.99429864e-01 -5.49328566e-01  1.95448071e-01\n",
      "  3.30738798e-02  4.45143133e-01 -1.45918047e+00  2.76990116e-01\n",
      "  1.06867522e-01 -6.36705995e-01  1.49721503e-02  6.21623933e-01\n",
      "  4.35345471e-01  3.67273450e-01  2.21252292e-01 -1.68745726e-01\n",
      "  6.34661615e-01  3.05576414e-01 -6.60773218e-01 -3.48220825e-01\n",
      " -6.01494908e-01  1.62416726e-01  8.51277590e-01  5.64038873e-01\n",
      " -1.32888541e-01  5.78130960e-01  4.37329769e-01 -2.50419378e-01\n",
      "  1.27389640e-01  3.04841608e-01  1.69228345e-01 -6.80671453e-01\n",
      " -7.91647136e-02 -3.90768163e-02 -5.89853674e-02  3.26032192e-01\n",
      "  2.79810995e-01  2.01112419e-01  2.79959261e-01 -3.94225419e-01\n",
      " -5.61534055e-02 -2.80866593e-01  4.21973437e-01 -1.25217527e-01\n",
      " -2.44032800e-01  2.56264925e-01 -5.77546299e-01 -1.79337002e-02\n",
      "  3.75045419e-01  3.90127748e-01  3.08235317e-01  4.32461239e-02\n",
      " -3.19815397e-01 -7.31928587e-01 -8.57445300e-01 -4.56196636e-01\n",
      "  1.91792414e-01  1.12777472e+00  1.14098623e-01 -3.93427730e-01\n",
      " -5.31382799e-01  3.73633385e-01  6.49822295e-01  6.61252618e-01\n",
      " -5.59039831e-01 -5.07438898e-01  1.88692242e-01  4.30638194e-01\n",
      " -4.43620563e-01  4.72782195e-01 -6.59415543e-01  6.56473190e-02\n",
      "  5.67307770e-01 -2.75379837e-01 -4.96693611e-01  7.40167964e-03\n",
      "  3.16577435e-01 -4.18295413e-01 -4.22885656e-01 -5.52101433e-01\n",
      "  1.22607075e-01  9.61042792e-02  3.23055595e-01 -8.41065586e-01\n",
      " -7.24421814e-02 -1.82764068e-01  6.32212877e-01  1.67815357e-01\n",
      "  9.77995574e-01 -5.84435463e-03  5.92191696e-01  3.41564238e-01\n",
      "  6.90299749e-01 -1.18450627e-01 -7.60542631e-01  3.70549768e-01\n",
      "  7.42236316e-01 -1.17789373e-01  1.43134284e+00 -5.23150861e-01\n",
      "  7.84933329e-01 -1.76844656e-01 -7.12502420e-01 -9.65571776e-02\n",
      " -3.18498445e+00  3.49936277e-01  1.60431802e-01 -4.65894155e-02\n",
      "  1.10750936e-01  5.58575392e-01  3.64335775e-01 -2.92843252e-01\n",
      " -2.93849111e-01 -2.34929547e-01  4.15985495e-01 -2.82904059e-01\n",
      " -2.30949983e-01  4.11721736e-01  1.61260486e-01  7.83595920e-01\n",
      "  9.95904356e-02 -6.22513071e-02  6.16447330e-01  1.61838517e-01\n",
      "  1.10445112e-01  9.79023241e-03  1.38397485e-01 -5.57122171e-01\n",
      " -3.70276511e-01  4.98058796e-02 -4.47070837e-01 -5.60444772e-01\n",
      " -3.11797470e-01 -1.47280797e-01  5.75177312e-01  6.68729022e-02\n",
      "  5.60362518e-01 -1.32139444e-01  8.84481013e-01 -8.28564614e-02\n",
      "  2.30413064e-01 -3.49385709e-01  4.17937599e-02 -9.30054039e-02\n",
      " -3.41964692e-01 -6.47252619e-01  1.84758946e-01 -8.61653328e-01\n",
      " -2.63148900e-02  2.52711296e-01 -6.25010908e-01  6.25658184e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"First element of X_train_name_last_hidden:\", X_train_name_last_hidden[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_name_last_hidden: (40032, 768)\n",
      "Data type of X_train_name_last_hidden: float32\n",
      "Shape of X_train_description_last_hidden: (40032, 768)\n",
      "Data type of X_train_description_last_hidden: float32\n",
      "Shape of X_test_name_last_hidden: (10009, 768)\n",
      "Data type of X_test_name_last_hidden: float32\n",
      "Shape of X_test_description_last_hidden: (10009, 768)\n",
      "Data type of X_test_description_last_hidden: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_name_last_hidden:\", X_train_name_last_hidden.shape)\n",
    "print(\"Data type of X_train_name_last_hidden:\", X_train_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_train_description_last_hidden:\", X_train_description_last_hidden.shape)\n",
    "print(\"Data type of X_train_description_last_hidden:\", X_train_description_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_name_last_hidden:\", X_test_name_last_hidden.shape)\n",
    "print(\"Data type of X_test_name_last_hidden:\", X_test_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_description_last_hidden:\", X_test_description_last_hidden.shape)\n",
    "print(\"Data type of X_test_description_last_hidden:\", X_test_description_last_hidden.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concatenated = np.concatenate((X_train_name_last_hidden, X_train_description_last_hidden), axis=1)\n",
    "X_test_concatenated = np.concatenate((X_test_name_last_hidden, X_test_description_last_hidden), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_concatenated: (40032, 1536)\n",
      "Data type of X_train_concatenated: float32\n",
      "Shape of X_test_concatenated: (10009, 1536)\n",
      "Data type of X_test_concatenated: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_concatenated:\", X_train_concatenated.shape)\n",
    "print(\"Data type of X_train_concatenated:\", X_train_concatenated.dtype)\n",
    "\n",
    "print(\"Shape of X_test_concatenated:\", X_test_concatenated.shape)\n",
    "print(\"Data type of X_test_concatenated:\", X_test_concatenated.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "X_train_combined = np.concatenate((X_train_processed_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_processed_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 2\n",
    "X_train_combined_1 = np.concatenate((X_train_processed_1_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_1 = np.concatenate((X_test_processed_1_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 3\n",
    "X_train_combined_2 = np.concatenate((X_train_processed_2_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_2 = np.concatenate((X_test_processed_2_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 4\n",
    "X_train_combined_3 = np.concatenate((X_train_processed_3_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_3 = np.concatenate((X_test_processed_3_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 5\n",
    "X_train_combined_4 = np.concatenate((X_train_processed_4_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_4 = np.concatenate((X_test_processed_4_loaded, X_test_concatenated), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define num_clases\n",
    "\n",
    "num_classes = len(df['parent_category'].unique())\n",
    "num_classes_1 = len(df['sub_category_1'].unique())\n",
    "num_classes_2 = len(df['sub_category_2'].unique())\n",
    "num_classes_3 = len(df['sub_category_3'].unique())\n",
    "num_classes_4 = len(df['sub_category_4'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Connected Home & Housewares': 0, 'other': 1, 'Car Electronics & GPS': 2, 'In-Store Only': 3, 'Musical Instruments': 4, 'Toys': 5, 'Video Games': 6, 'Cameras & Camcorders': 7, 'Computers & Tablets': 8, 'Appliances': 9, 'Audio': 10, 'TV & Home Theater': 11, 'Health': 12, 'Name Brands': 13, 'Cell Phones': 14, 'Movies & Music': 15, 'Magnolia Home Theater': 16, 'Geek Squad': 17, 'Best Buy Gift Cards': 18, 'H/VG_X360/Games/B2G1_20130602': 19, 'MP Exclusives': 20, 'Wearable Technology': 21, 'Custom Parts': 22}\n",
      "{' Housewares': 0, 'missing': 1, ' Car Installation Parts & Accessories': 2, ' Telephones & Communication': 3, ' Car Audio': 4, ' Recording Equipment': 5, ' Games & Drones': 6, ' Musical Instrument Accessories': 7, ' Microphones & Live Sound': 8, ' Keyboards': 9, ' DJ & Lighting Equipment': 10, ' Sheet Music & DVDs': 11, ' Wii': 12, ' Digital Cameras': 13, ' Xbox 360': 14, ' Camcorder Accessories': 15, ' Karaoke': 16, ' Computer Accessories & Peripherals': 17, ' Pre-Owned Games': 18, ' Office Electronics': 19, ' Marine & Powersports': 20, ' Ranges': 21, ' Headphones': 22, ' Small Kitchen Appliances': 23, ' Freezers & Ice Makers': 24, ' TV Stands': 25, ' Computer Cards & Components': 26, ' Fitness & Beauty': 27, ' Apple': 28, ' Nintendo DS': 29, ' PlayStation 3': 30, ' TV & Home Theater Accessories': 31, ' Digital Camera Accessories': 32, ' Heating': 33, ' Cell Phone Accessories': 34, ' Irons': 35, ' Scanners': 36, ' Office & School Supplies': 37, ' TVs': 38, ' iPad & Tablet Accessories': 39, 'other': 40, ' Incase': 41, ' Office Furniture & Storage': 42, ' Home Audio': 43, ' Memory Cards': 44, ' Refrigerators': 45, ' Car Safety & Convenience': 46, ' iPod & MP3 Player Accessories': 47, ' Microwaves': 48, ' Washers & Dryers': 49, ' Drums & Percussion': 50, ' Bluetooth & Wireless Speakers': 51, ' Monitors': 52, ' Vacuum Cleaners & Floor Care': 53, ' Household Essentials': 54, ' Furniture & Decor': 55, ' PC Gaming': 56, ' Software': 57, ' PlayStation 2': 58, ' Movie & Music Cards': 59, ' Tablets': 60, ' Unlocked Cell Phones': 61, ' Home Audio Accessories': 62, ' No-Contract Phones': 63, ' Camcorders': 64, ' Amps & Effects': 65, ' Dishwashers': 66, ' Nintendo 3DS': 67, ' Connected Home': 68, ' iPod and MP3 Players': 69, ' Projectors & Screens': 70, ' Video Game Accessories': 71, ' Networking & Wireless': 72, ' Appliance Parts & Accessories': 73, ' Guitars': 74, ' PlayStation 4': 75, ' Xbox One': 76, ' Magnolia Accessories': 77, ' GPS Navigation & Accessories': 78, ' PSP': 79, ' TV & Home Theater Services': 80, ' Toys to Life': 81, ' All Cell Phones with Plans': 82, ' Car Electronics Professional Installation': 83, ' Computer & Tablet Services': 84, ' Binoculars': 85, ' Pre-Owned & Trade-In Games': 86, ' Laptops': 87, ' Streaming Media Players': 88, ' Wii U': 89, ' Car Lights & Lighting Accessories': 90, ' E-Readers & Accessories': 91, ' Car Security & Remote Starters': 92, ' Desktop & All-in-One Computers': 93, ' GoPro': 94, ' Docks': 95, ' Samsung Galaxy': 96, ' Entertainment Gift Cards': 97, ' Folk Instruments': 98, ' Movies & TV Shows': 99, ' Game Downloads': 100, ' Paper': 101, ' PS Vita': 102, ' Blu-ray & DVD Players': 103, ' Car & GPS Accessories': 104, ' Batteries & Power': 105, ' Ballistic': 106, ' Car Video': 107, ' Trail Cameras & Rangefinders': 108, ' Apple Watch': 109, ' Band & Orchestra': 110, ' Game Guides': 111, ' iPhone': 112, ' Refurbished & Pre-Owned Phones': 113}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Connected Home & Housewares' 'other' 'Car Electronics & GPS'\n",
      " 'In-Store Only' 'Musical Instruments' 'Toys' 'Video Games'\n",
      " 'Cameras & Camcorders' 'Computers & Tablets' 'Appliances' 'Audio'\n",
      " 'TV & Home Theater' 'Health' 'Name Brands' 'Cell Phones' 'Movies & Music'\n",
      " 'Magnolia Home Theater' 'Geek Squad' 'Best Buy Gift Cards'\n",
      " 'H/VG_X360/Games/B2G1_20130602' 'MP Exclusives' 'Wearable Technology'\n",
      " 'Custom Parts']\n"
     ]
    }
   ],
   "source": [
    "print(df['parent_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Housewares' 'missing' ' Car Installation Parts & Accessories'\n",
      " ' Telephones & Communication' ' Car Audio' ' Recording Equipment'\n",
      " ' Games & Drones' ' Musical Instrument Accessories'\n",
      " ' Microphones & Live Sound' ' Keyboards' ' DJ & Lighting Equipment'\n",
      " ' Sheet Music & DVDs' ' Wii' ' Digital Cameras' ' Xbox 360'\n",
      " ' Camcorder Accessories' ' Karaoke' ' Computer Accessories & Peripherals'\n",
      " ' Pre-Owned Games' ' Office Electronics' ' Marine & Powersports'\n",
      " ' Ranges' ' Headphones' ' Small Kitchen Appliances'\n",
      " ' Freezers & Ice Makers' ' TV Stands' ' Computer Cards & Components'\n",
      " ' Fitness & Beauty' ' Apple' ' Nintendo DS' ' PlayStation 3'\n",
      " ' TV & Home Theater Accessories' ' Digital Camera Accessories' ' Heating'\n",
      " ' Cell Phone Accessories' ' Irons' ' Scanners'\n",
      " ' Office & School Supplies' ' TVs' ' iPad & Tablet Accessories' 'other'\n",
      " ' Incase' ' Office Furniture & Storage' ' Home Audio' ' Memory Cards'\n",
      " ' Refrigerators' ' Car Safety & Convenience'\n",
      " ' iPod & MP3 Player Accessories' ' Microwaves' ' Washers & Dryers'\n",
      " ' Drums & Percussion' ' Bluetooth & Wireless Speakers' ' Monitors'\n",
      " ' Vacuum Cleaners & Floor Care' ' Household Essentials'\n",
      " ' Furniture & Decor' ' PC Gaming' ' Software' ' PlayStation 2'\n",
      " ' Movie & Music Cards' ' Tablets' ' Unlocked Cell Phones'\n",
      " ' Home Audio Accessories' ' No-Contract Phones' ' Camcorders'\n",
      " ' Amps & Effects' ' Dishwashers' ' Nintendo 3DS' ' Connected Home'\n",
      " ' iPod and MP3 Players' ' Projectors & Screens' ' Video Game Accessories'\n",
      " ' Networking & Wireless' ' Appliance Parts & Accessories' ' Guitars'\n",
      " ' PlayStation 4' ' Xbox One' ' Magnolia Accessories'\n",
      " ' GPS Navigation & Accessories' ' PSP' ' TV & Home Theater Services'\n",
      " ' Toys to Life' ' All Cell Phones with Plans'\n",
      " ' Car Electronics Professional Installation'\n",
      " ' Computer & Tablet Services' ' Binoculars' ' Pre-Owned & Trade-In Games'\n",
      " ' Laptops' ' Streaming Media Players' ' Wii U'\n",
      " ' Car Lights & Lighting Accessories' ' E-Readers & Accessories'\n",
      " ' Car Security & Remote Starters' ' Desktop & All-in-One Computers'\n",
      " ' GoPro' ' Docks' ' Samsung Galaxy' ' Entertainment Gift Cards'\n",
      " ' Folk Instruments' ' Movies & TV Shows' ' Game Downloads' ' Paper'\n",
      " ' PS Vita' ' Blu-ray & DVD Players' ' Car & GPS Accessories'\n",
      " ' Batteries & Power' ' Ballistic' ' Car Video'\n",
      " ' Trail Cameras & Rangefinders' ' Apple Watch' ' Band & Orchestra'\n",
      " ' Game Guides' ' iPhone' ' Refurbished & Pre-Owned Phones']\n"
     ]
    }
   ],
   "source": [
    "print(df['sub_category_1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Household Batteries' 'missing' ' Car Audio Installation Parts'\n",
      " ' Telephone Accessories' ' Car Subwoofers & Enclosures'\n",
      " ' Sound Recording Software' ' TV' ' Keyboard Accessories'\n",
      " ' Live Sound Speakers' ' Audio Interfaces' 'other'\n",
      " ' Midi Keyboards & Controllers' ' Microphones & Accessories'\n",
      " ' DJ Equipment Accessories' ' Recording Furniture & Stands' ' Games'\n",
      " ' Sheet Music' ' Wii Games' ' Digital SLR Cameras' ' Xbox 360 Games'\n",
      " ' Action Camcorder Accessories' ' DJ Lighting & Effects'\n",
      " ' Karaoke Machines' ' Surge Protectors & Power' ' Car Amplifiers'\n",
      " ' Powersports Audio' ' Monitor & Screen Accessories' ' Cooktops & Ovens'\n",
      " ' All Headphones' ' Blenders & Juicers' ' Upright Freezers' ' DJ Mixers'\n",
      " ' Mounts & Furniture' ' Power Supplies' ' Personal Care & Beauty'\n",
      " ' Over-Ear & On-Ear Headphones' ' Accessories' ' Nintendo DS Games'\n",
      " ' GPUs/Video Graphics Cards' ' Cordless Telephones' ' PS3 Games'\n",
      " ' Remote Controls' ' Camera Lenses' ' Laptop Accessories'\n",
      " ' Cooling & Air Quality' ' Cell Phone Cases & Clips'\n",
      " ' iPhone Accessories' ' Cell Phone Batteries & Power'\n",
      " ' Filters & Accessories' ' Steamers & Sewing Machines' ' Faxes & Copiers'\n",
      " ' Printer Ink & Toner' ' All Flat-Panel TVs' ' Computer Speakers'\n",
      " ' Cases' ' Electric Griddles & Hotplates' ' Office Tables'\n",
      " ' Corded Telephones' ' Wireless & Multiroom Audio' ' All Memory Cards'\n",
      " ' All Refrigerators' ' Back-Up & Dash Cameras' ' FM Transmitters'\n",
      " ' A/V Cables & Connectors' ' Marine GPS & Navigation'\n",
      " ' Instrument Instructional Books' ' More Car Accessories' ' Coffee'\n",
      " ' All Microwaves' ' Dryers' ' Toaster & Pizza Ovens' ' Cymbals'\n",
      " ' Health Monitoring & Testing' ' Remote Control Toys'\n",
      " ' Photography Accessories' ' All Monitors' ' LCD Monitors'\n",
      " ' Handheld & Stick Vacuums' ' Camera Batteries & Power'\n",
      " ' Screen Protectors' ' Commercial & Garage Vacuums' ' Cleaning Products'\n",
      " ' Ice Makers' ' Water Dispensers & Purifiers' ' Kitchen Gadgets'\n",
      " ' PC Games' ' Toasters' ' Slow Cookers' ' Deep Fryers' ' Steamers'\n",
      " ' Waffle Makers' ' Outdoor Living' ' Cookware'\n",
      " ' Pet Supplies & Technology' ' A/V Surge Protectors & Power' ' Music'\n",
      " ' Washing Machines' ' Adapters' ' Blank Discs & Labels'\n",
      " ' Mice & Keyboards' ' Top-Freezer Refrigerators' ' PS2 Games' ' Racks'\n",
      " ' All Tablets' ' Car Stereo Receivers' ' All Unlocked Cell Phones'\n",
      " ' Speaker Accessories' ' All No-Contract Phones' ' Lens Accessories'\n",
      " ' Flashes' ' Health & Fitness Accessories' ' Action Camcorders'\n",
      " ' Receivers & Amplifiers' ' Effects' ' Built-In Dishwashers'\n",
      " ' Nintendo 3DS Accessories' ' Earbud & In-Ear Headphones'\n",
      " ' Wireless Headphones' ' Conferencing Telephones'\n",
      " ' Instrument Instructional DVDs' ' Security Cameras & Surveillance'\n",
      " ' Hard Drives & Storage' ' Buffet Servers & Chafing Dishes'\n",
      " ' Satellite Radios' ' PA Mixers' ' Power Amps' ' iPods'\n",
      " ' Flashlights & Portable Lights' ' Stylus Pens' ' Fitness & GPS Watches'\n",
      " ' Speakers' ' Xbox 360 Accessories' ' Projectors'\n",
      " ' Food Processors & Baby Food Makers' ' Mixers & Mixer Accessories'\n",
      " ' Smartwatches & Accessories' ' Gaming Headsets'\n",
      " ' Ethernet Hubs & Switches' ' Air Purifier Filters & Parts'\n",
      " ' Tripods & Monopods' ' Electric Guitars' ' Bass Guitars'\n",
      " ' Home Theater Systems' ' Smart Lighting' ' Point & Shoot Cameras'\n",
      " ' Mirrorless Cameras' ' PS4 Games' ' Cases & Armbands' ' Routers'\n",
      " ' Car & Travel Accessories' ' Refrigerator Parts & Accessories'\n",
      " ' Vacuum & Floor Care Accessories' ' Steam Mops' ' Xbox One Games'\n",
      " ' PS3 Accessories' ' Learning & Education' ' Labelers & Accessories'\n",
      " ' PC Range Extenders' ' Cell Phone Add-Ons' ' PC Gaming Controllers'\n",
      " ' Assistive Technology' ' Car Stereo Accessories'\n",
      " ' TV & Home Theater Accessories' ' Home Theater Networking'\n",
      " ' GPS Accessories' ' Acoustic Guitars' ' Shredders'\n",
      " ' Microwave Accessories' ' Chest Freezers' ' Side-by-Side Refrigerators'\n",
      " ' Clocks' ' GPS Maps' ' Nintendo 3DS Games' ' PlayStation Network'\n",
      " ' Education' ' Cables & Chargers' ' Specialty Appliances'\n",
      " ' Bar & Accessories' ' Activity Trackers & Pedometers' ' Calculators'\n",
      " ' USB Flash Drives' ' Baby Monitors' ' Breathalyzers' ' Carpet Cleaners'\n",
      " ' Prepaid Game Cards' ' Cables & Connectors' ' Optical Drives'\n",
      " ' Musical Instrument Cables' ' Camcorder Batteries & Power'\n",
      " ' Portable GPS' ' Memory (RAM)' ' Car Speakers' ' Telescopes & Optics'\n",
      " ' Safes' ' Sports & Outdoor Recreation' ' Stands & Mounts'\n",
      " ' Guitar Accessories' ' TV Antennas' ' LED Monitors' ' Virtual Reality'\n",
      " ' Camera Bags' ' Xbox One Accessories' ' PS4 Accessories' ' Desk Lamps'\n",
      " ' Printers' ' Radar Detectors' ' All Laptops'\n",
      " ' Drum & Percussion Accessories' ' Skillets & Woks' ' Wii Accessories'\n",
      " ' Microphones' ' Gaming Eyewear' ' Stereo Shelf Systems'\n",
      " ' Cell Phone Speakers' ' Drones & Accessories' ' Mac' ' Canister Vacuums'\n",
      " ' Handheld GPS' ' Cell Phone Headsets' ' Wii U Games'\n",
      " ' Studio Headphones' ' Business & Office' ' Tools' ' Desks'\n",
      " ' Prepaid Minutes' ' All Desktops' ' Game Room & Bar Furniture'\n",
      " ' 3D Glasses' ' Robot Vacuums' ' Car Security & Remote Start Accessories'\n",
      " ' Upright Vacuums' ' GoPro Camera Accessories'\n",
      " ' Smartphone & iPod Car Connectors' ' French Door Refrigerators'\n",
      " ' CD Players & Turntables' ' Graphics Tablets & Digital Pens'\n",
      " ' Radios & Boomboxes' ' Acoustical Treatments' ' Wall Art' ' SIM Cards'\n",
      " ' Camera Accessory Kits' ' Samsung Galaxy Accessories' ' Antivirus'\n",
      " ' MP3 Players' ' Specialty Gift Cards' ' Viewfinders & Accessories'\n",
      " ' Design & Illustration' ' Office Chairs' ' Headsets & Microphones'\n",
      " ' App-Enabled Toys' ' iPad Accessories' ' Camcorder Bags & Cases'\n",
      " ' Pedestals & Organization' ' iPad' ' Headphone Accessories'\n",
      " ' Studio Monitors' ' Stacking Kits' ' Portable Keyboards'\n",
      " ' Living Room Furniture' ' Home Theater Furniture' ' CPUs / Processors'\n",
      " ' PC Game Downloads' ' Hobbies & Leisure' ' Computer Cases'\n",
      " ' Compact Refrigerators' ' Wine Refrigerators & Coolers'\n",
      " ' PC USB Wireless Adapters' ' Multipurpose Paper'\n",
      " ' Marine Accessories & Lighting' ' Range & Oven Accessories'\n",
      " ' Networking Accessories' ' PS Vita Games'\n",
      " ' Portable & Countertop Dishwashers' ' Photo-Quality Paper'\n",
      " ' Portable DVD Players & Accessories' ' Finance' ' Fans'\n",
      " ' Bluetooth & Wireless Speaker Accessories' ' Keyboards'\n",
      " ' File Cabinets & Storage' ' World Percussion' ' Smart Sports Equipment'\n",
      " ' Operating Systems' ' Rideables & Outdoor Play' ' Cleaning Equipment'\n",
      " ' Blu-ray Players' ' Traditional Camcorders' ' Meat Slicers & Grinders'\n",
      " ' Signal Boosters' ' Dining Gift Cards' ' Building Sets & Blocks'\n",
      " ' 4K Ultra HD TVs' ' All Camera Accessories'\n",
      " ' Car Security & Radar Installation Parts' ' Live Sound Accessories'\n",
      " ' Two-Way Radios' ' Magnolia TV Stands' ' Luggage' ' Smart Door Locks'\n",
      " ' Car Accessories' ' Care & Cleaning' ' Remote Transmitters' ' Ukuleles'\n",
      " ' Mobile Hotspots' ' Electric Guitar Amps' ' DJ Turntables'\n",
      " ' DSL & Cable Modems and Gateways' ' Counter-Depth Refrigerators'\n",
      " ' Sink Garbage Disposals' ' Curved TVs' ' Range Hood Parts & Accessories'\n",
      " ' Xbox One Consoles' ' Marine Audio' ' Wii U Accessories'\n",
      " ' Cleaning Equipment & Kits' ' Disney Infinity' ' Washer Accessories'\n",
      " ' Home Alarms & Sensors' ' Appliance & Outlet Control' ' Bookcases'\n",
      " ' Stacked Units' ' Apple Watch Accessories' ' Smart Tracker Tags'\n",
      " ' Acoustic Drums & Sets' ' Fireplace Accessories' \" Kids' Tablets\"\n",
      " ' Action Figures' ' Apple Watch Sport' ' Apple Watch'\n",
      " ' All Bluetooth & Wireless Speakers' ' Digital Pianos' ' Skylanders'\n",
      " ' Live Sound Equipment Cases' ' Amiibo' ' Refurbished Phones'\n",
      " ' Voice Recognition Software' ' Pre-Owned Phones' ' Trash Compactors'\n",
      " ' Lego Dimensions' ' Powersports GPS & Fishfinders' ' Motherboards'\n",
      " ' Electronic Drums' ' Brass Instruments' ' Woodwinds'\n",
      " ' Outdoor TVs & Accessories' ' Game Room' ' E-Reader Accessories'\n",
      " ' iPhone SE' ' Game Guides - Video Games']\n"
     ]
    }
   ],
   "source": [
    "print(df['sub_category_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Alkaline Batteries' 'missing' ' Specialty Batteries'\n",
      " ' Deck Installation Parts' ' Cordless Phone Batteries' ' Car Subwoofers'\n",
      " ' Movie & Character Toys' ' Speaker Monitors' ' Microphones'\n",
      " ' Recording & Music Stands' ' Puzzles & Cards' ' DSLR Body & Lens'\n",
      " ' Action Camcorder Mounts' ' Battery Backup (UPS)' ' 2-Channel Amps'\n",
      " ' Monitor & Video Cables' ' Cooktops' ' Blenders'\n",
      " ' Projector Mounts & Screens' 'other' ' Hair Care' ' Over-Ear Headphones'\n",
      " ' Shavers & Trimmers' ' PCI Express Graphics Cards' ' Expansion Handsets'\n",
      " ' Remote Control Accessories' ' TV Mounts' ' DSLR Lenses'\n",
      " ' Laptop Chargers & Adapters' ' Air Conditioners' ' iPhone Cases & Clips'\n",
      " ' Portable Chargers/Power Packs' ' Polarizing Filters' ' Irons'\n",
      " ' Sheetfed Scanners' ' Printer Ink' ' Covers & Keyboard Folios'\n",
      " ' Wireless & Multiroom Components' ' Back-Up Cameras'\n",
      " ' Composite Video Cables' ' Amp Installation Parts'\n",
      " ' Universal Remote Controls' ' Tea & Espresso' ' Electric Dryers'\n",
      " ' Gas Dryers' ' Toaster Ovens' ' Blood Pressure Monitors'\n",
      " ' Cell Phone Camera Lenses' ' Camera Batteries' ' 3D Printer Filament'\n",
      " ' Heaters' ' Humidifiers' ' Food Preparation Utensils'\n",
      " ' Crock Pots & Roaster Ovens' ' Rice Cookers & Pressure Cookers'\n",
      " ' Grills' ' Bakeware & Cutlery' ' TV Stands' ' Photo & Video Editing'\n",
      " ' Front-Loading Washers' ' Cables & Chargers' ' Microphone Accessories'\n",
      " ' DVD-R/RW' ' Wall Ovens' ' Mice' ' Scales' ' Computer Keyboards'\n",
      " ' Music & Dance' ' Mounts & Accessories' ' In-Dash with DVD'\n",
      " ' Speaker Wall Mounts' ' Lens Caps' ' Lighting & Studio'\n",
      " ' Activity Tracker Accessories' ' All Action Camcorders' ' Receivers'\n",
      " ' Pedals' ' Ranges' ' Toner' ' Wine Chillers & Accessories'\n",
      " ' Nintendo 3DS Cases & Skins' ' Systems' ' Internal Hard Drives'\n",
      " ' iPod touch' ' Floor Speakers' ' Center-Channel Speakers'\n",
      " ' Security Camera & System Accessories' ' Food Processors'\n",
      " ' Stand Mixers' ' Air Purifiers' ' Mouse & Wrist Pads'\n",
      " ' All Car Stereo Receivers' ' Smartwatches' ' Speaker Stands & Mounts'\n",
      " ' Monopods' ' Sound Bars' ' Smart Light Bulbs'\n",
      " ' All Point & Shoot Cameras' ' All Mirrorless Cameras' ' Hand Mixers'\n",
      " ' All Cases & Armbands' ' Range Hoods' ' Rechargeable Batteries'\n",
      " ' Thermostats' ' Smart & WiFi Light Switches' ' Childrens Cameras'\n",
      " ' Refrigerator Water Filters' ' Laptop Batteries' ' Mirrorless Lenses'\n",
      " ' Monitor Mounts & Stands' ' Juicers' ' Vacuum & Floor Tools'\n",
      " ' Remote Control Cars & Trucks' ' Labelers' ' Fireplaces'\n",
      " ' Speaker & Subwoofer Installation Parts' ' Internal Batteries'\n",
      " ' Action & Adventure' ' Fans' ' Keyboard & Mouse Combos' ' Hearing'\n",
      " ' Speaker Selector Switches' ' Outdoor Speakers' ' Speaker Cables & Wire'\n",
      " ' In-Wall & In-Ceiling Speakers' ' In-Wall Speaker Accessories'\n",
      " ' Cable Management & Concealment' ' Component Video Cables'\n",
      " ' HDMI Cables' ' GPS Mounts & Cradles' ' Bookshelf Speakers'\n",
      " ' Microwave Trim Kits' ' Subwoofer Speakers' ' PSN Cards & Codes'\n",
      " ' Reference & Language' ' Instant Print Cameras'\n",
      " ' Camera Chargers & Adapters' ' Popcorn Makers' ' CD & DVD Media Storage'\n",
      " ' Skin Care' ' Cleaning Solutions' ' USB Cables & Hubs'\n",
      " ' Security Camera Systems' ' Laptop Bags & Cases'\n",
      " ' Point & Shoot Camera Accessories' ' 6.5\" Car Speakers'\n",
      " ' Wireless Routers' ' Binoculars' ' Monoculars' ' Spotting Scopes'\n",
      " ' Amplifiers' ' Action Camcorder Batteries & Power' ' On-Ear Headphones'\n",
      " ' Surge Protectors' ' Guitar Cases & Bags' ' Cases & Straps'\n",
      " ' Security Cameras' ' CD/DVD Duplicators' ' Xbox One Controllers'\n",
      " ' PS4 Controllers' ' Science & Discovery' ' Tripods' ' Speaker Cables'\n",
      " ' In-Dash with GPS' ' Ink & Toner' ' Chromebooks' ' Flatbed Scanners'\n",
      " ' Oral Care' ' Drumset Hardware & Stands' ' Skillets'\n",
      " ' Compact System Lenses' ' DSLR Body Only' ' Soda Makers & Mixes'\n",
      " ' Battery Grips' ' Top-Loading Washers' ' All Home Theater Systems'\n",
      " ' MP3 Player Cables' ' Vision' ' A/V Switchers' ' Portable Speakers'\n",
      " ' All Drones' ' Vacuum Sealers' ' Xbox One Headsets' ' Vacuum Filters'\n",
      " ' Ice Cream & Yogurt Makers' ' Bluetooth Headsets' ' Sports & Outdoors'\n",
      " ' File Conversion Software' ' Office Software Suites' ' PS4 Headsets'\n",
      " ' Hand Tools' ' Home Office Desks' ' External Hard Drives' ' Splitters'\n",
      " ' Water Bottles' ' Wireless Multiroom Audio Systems' ' Vacuum Bags'\n",
      " ' Refurbished Laptops' ' Telescopes' ' Desktop Memory'\n",
      " ' Solid State Drives' ' Trash Cans' ' Dehydrators'\n",
      " ' Other GoPro Accessories' ' Auxiliary Input Cables'\n",
      " ' CD Players & Recorders' ' Speaker Systems' ' Boomboxes'\n",
      " ' Loaded Enclosures' ' Subwoofer Enclosures' ' Security & Utilities'\n",
      " ' More Guitar Accessories' ' Guitar Tuners & Metronomes' ' Can Openers'\n",
      " ' Camera Mounts' ' Graphic Design Software' ' Computer Headsets'\n",
      " ' PCI Graphics Cards' ' iPad Cases' ' Remote Control Aircraft'\n",
      " ' Guitar Stands' ' CD-R/RW' ' Radios' ' Turntables & Accessories'\n",
      " ' Subwoofer Cables' ' 5.25\" Car Speakers' ' 6\" x 9\" Car Speakers'\n",
      " ' Portable Scanners' ' Bluetooth & Wireless Speakers'\n",
      " ' Specialty Electric Bakers' ' Living Room Tables'\n",
      " ' Accents & Accessories' ' Sewing Machines' ' Mixer Accessories'\n",
      " ' Smartphone & iPod Car Stereo Kits' ' Dehumidifiers' ' PC Laptops'\n",
      " ' NAS/Personal Cloud Storage' ' Portable DVD Players' ' Tax & Legal'\n",
      " ' Heatsinks & Cooling' ' Refrigerator Trim Kits'\n",
      " ' Action Camcorder Housings' ' Car Tweeters' ' A/V Component Racks'\n",
      " ' Cell Phone Mounts' ' Scooters & Casterboards'\n",
      " ' Cordless Phone Headsets' ' All Traditional Camcorders'\n",
      " ' iPhone Screen Protectors' ' Business Plan Software' ' Mirrors'\n",
      " ' Food Scales & Measuring' ' Snow Cone Machines'\n",
      " ' Guitar Straps & Strap Locks' ' Computer Desks'\n",
      " ' Smartwatch Accessories' ' Guitar Strings' ' Digital Audio Cables'\n",
      " ' Mounts & Furniture' ' Bags & Travel' ' Home Theater Seating'\n",
      " ' Aromatherapy Diffusers & Oils' ' Interactive Learning Systems'\n",
      " ' Towers Only' ' Outdoor GPS' ' Heart Rate Monitors' ' Warming Drawers'\n",
      " ' Patio Furniture & Decor' ' Live Sound Speaker Systems' ' Selfie-Sticks'\n",
      " ' Smart Lighting Kits' ' Hard Drive Cables & Accessories'\n",
      " ' Lens Adapters & Converters' ' Camcorder Batteries'\n",
      " ' Multi-Channel Amps' ' Range Hood Blowers' ' Range Hood Covers'\n",
      " ' PS4 Console Add-Ons' ' Microscopes' ' Massagers & Spa'\n",
      " ' Door & Window Alarms' ' Motion Sensors' ' Mono Subwoofer Amps'\n",
      " ' Viewfinders & Eyepieces' ' Power & Extension Cords'\n",
      " ' All-in-One Computers' ' CD Players & Digital Media Receivers'\n",
      " ' Apple Watch Bands & Straps' ' Apple Watch Cases' ' Marine Speakers'\n",
      " ' Virtual Reality-Ready Computers' ' Drone Accessories'\n",
      " ' Apple Watch Stands & Docks' ' Wii U Controllers'\n",
      " ' Battery Charger Cases' ' Network Cables' ' Dash Cameras' ' Pet Collars'\n",
      " ' Outdoor Lighting' ' Bar Stools' ' Pub Tables' ' Vacuum Belts'\n",
      " ' Darts & Dartboards' ' Golf' ' Outdoor Heating' ' Skylight & UV Filters'\n",
      " ' MacBooks' ' Pool Tables & Accessories' ' Refurbished Desktops'\n",
      " ' Neutral Density Filters' ' Evaporative Coolers' ' Bug Zappers'\n",
      " ' Garage Storage' ' Shakers' ' Refrigerator Storage']\n"
     ]
    }
   ],
   "source": [
    "print(df['sub_category_3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['missing' ' Dash Installation Kits' ' Deck Harnesses'\n",
      " ' Antennas & Adapters' ' Music Memorabilia'\n",
      " ' More Pop Culture Merchandise' ' Dynamic' 'other'\n",
      " ' Handlebar/Seatpost Mounts' ' Single-Serve Blenders'\n",
      " ' Projector Screens' ' Hair Dryers' \" Women's Shavers\" ' Condenser'\n",
      " ' Prime Lenses' ' Hair & Nail Cutting Kits' ' Window Air Conditioners'\n",
      " ' Hair Straighteners' ' iPhone 6 Cases' ' Fuses & Capacitors'\n",
      " ' Interfaces & Converters' ' Amp Installation Kits' ' Tripod Mounts'\n",
      " ' Coffee Pod Racks & Storage' ' Replacement Parts & Accessories'\n",
      " ' Rice Cookers' ' Gas Grills' ' Cookware' ' Grill Accessories'\n",
      " ' Trimmers' ' Curling Irons' ' All TV Stands'\n",
      " ' Music & Audio Editing Software' ' Wall Oven & Microwave Combos'\n",
      " ' Double Ovens' ' Wired & USB Mice' ' Wired & USB Keyboards' ' Softboxes'\n",
      " ' Activity Tracker Bands' ' Gas Ranges' ' Wine Chillers'\n",
      " ' Lighting Accessories' ' Wall Chargers & Power Adapters'\n",
      " ' Electric Ranges' ' Hand Blenders' ' Electric Grills'\n",
      " ' Wireless & Bluetooth Mice' ' LED Lighting' ' Speaker Stands' ' Cutlery'\n",
      " ' Wall Mount Range Hoods' ' Long-Range Zoom Lenses'\n",
      " ' Centrifugal Juicers' ' Electric Fireplaces' ' Speaker Brackets'\n",
      " \" Men's Shavers\" ' Electric Shaver Replacement Heads' ' Hewlett-Packard'\n",
      " ' Table & Portable Fans' ' Cases' ' Wireless & Bluetooth Keyboards'\n",
      " ' Ergonomic Mice' ' Projector Mounts' ' Portable Air Conditioners'\n",
      " ' In-Wall Speakers' ' In-Ceiling Speakers' ' Clamp Mounts'\n",
      " ' Gaming Keyboards' ' Language Learning Software' ' Car Chargers'\n",
      " ' Media Cabinets' ' Tilt TV Mounts' ' Tower & Pedestal Fans'\n",
      " ' Floor & Box Fans' ' Coffee Grinders & Filters' ' Covers'\n",
      " ' Keyboard Folios' ' Point & Shoot Camera Batteries'\n",
      " ' DSLR Camera Batteries' ' Charcoal Grills'\n",
      " ' Instrument Instruction Software' ' Suction Cup Mounts'\n",
      " ' Short-Range Zoom Lenses' ' Camera Backpacks' ' All Printers'\n",
      " ' 3D Printers & Filament' ' Pressure Cookers' ' Electric Toothbrushes'\n",
      " ' Full-Size Blenders' ' Helmet/Head Mounts' ' Drones with Camera'\n",
      " ' Bakeware' ' Radio Replacement Modules' ' Ice Cream Makers'\n",
      " ' TV Mount Accessories' ' Masticating Juicers' ' Wine Accessory Sets'\n",
      " ' Couplers & Adapters' ' Dual-Fuel Ranges' ' Internet Security'\n",
      " ' Specialty Lenses' ' Camera Straps' ' Antivirus Software'\n",
      " ' Covers & Keyboard Folios' ' Single Ovens' ' Speaker Harnesses'\n",
      " ' iPhone 6 Plus Cases' ' TV Stands with Fireplace' ' Ottomans'\n",
      " ' Data Cables' ' Video Editing Software' ' Small Business Finance'\n",
      " ' Smokers' ' Adapters' ' Other' ' Island Range Hoods'\n",
      " ' Air Conditioner Accessories' ' Curlers' ' Coffee & Travel Mugs'\n",
      " ' Camera Belts & Hip Packs' ' Wireless' ' Multi Effects'\n",
      " ' Smartwatch Bands' ' Steamers' ' Milk Frothers' ' Fiber Optic Cables'\n",
      " ' Wine Racks' ' Luggage' ' Aromatherapy Diffusers' ' Jewel Cases'\n",
      " ' Portable CD & DVD Storage' ' Sound Damping' ' Brother'\n",
      " ' Smartwatch Screen Protectors' ' Fire Pits' ' Outdoor Fireplaces'\n",
      " ' Photo Editing Software' ' Toothbrush Replacement Heads'\n",
      " ' Fixed TV Mounts' ' Learning System Games' ' Patio Umbrellas & Bases'\n",
      " ' Network Attached Storage (NAS)' ' Camera Bags & Cases'\n",
      " ' Drones without Camera' ' Touch-Screen All-in-One Computers'\n",
      " ' Gel Fuel Fireplaces' ' Drink & Soda Mixes' ' Full-Motion TV Mounts'\n",
      " ' Star Wars' ' Drone Bags & Cases' ' Drone Parts'\n",
      " ' Smartwatch Charging Docks' ' Leashes & Apparel'\n",
      " ' Fountains & Bird Baths' ' Dart Board Cabinets' ' iPhone 6s Plus Cases'\n",
      " ' iPhone 6s Cases' ' Ceiling Fans' ' More Microphones'\n",
      " ' Pool Balls & Racks' ' Travel Accessories' ' Outdoor Furniture Sets']\n"
     ]
    }
   ],
   "source": [
    "print(df['sub_category_4'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "114\n",
      "349\n",
      "316\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)\n",
    "print(num_classes_1)\n",
    "print(num_classes_2)\n",
    "print(num_classes_3)\n",
    "print(num_classes_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1251/1251 [==============================] - 5s 3ms/step - loss: 0.9662 - accuracy: 0.7384 - val_loss: 0.4768 - val_accuracy: 0.8675 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5709 - accuracy: 0.8462 - val_loss: 0.4098 - val_accuracy: 0.8879 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4617 - accuracy: 0.8756 - val_loss: 0.3154 - val_accuracy: 0.9131 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4038 - accuracy: 0.8922 - val_loss: 0.2644 - val_accuracy: 0.9324 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3738 - accuracy: 0.9013 - val_loss: 0.2563 - val_accuracy: 0.9340 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3317 - accuracy: 0.9112 - val_loss: 0.2372 - val_accuracy: 0.9394 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3043 - accuracy: 0.9203 - val_loss: 0.2472 - val_accuracy: 0.9367 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2884 - accuracy: 0.9226 - val_loss: 0.2127 - val_accuracy: 0.9466 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2689 - accuracy: 0.9299 - val_loss: 0.2138 - val_accuracy: 0.9451 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2508 - accuracy: 0.9333 - val_loss: 0.1982 - val_accuracy: 0.9511 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2331 - accuracy: 0.9389 - val_loss: 0.1891 - val_accuracy: 0.9520 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2263 - accuracy: 0.9399 - val_loss: 0.1879 - val_accuracy: 0.9536 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2191 - accuracy: 0.9433 - val_loss: 0.1835 - val_accuracy: 0.9545 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2131 - accuracy: 0.9447 - val_loss: 0.1851 - val_accuracy: 0.9536 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2037 - accuracy: 0.9463 - val_loss: 0.1826 - val_accuracy: 0.9542 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1978 - accuracy: 0.9485 - val_loss: 0.1837 - val_accuracy: 0.9556 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1980 - accuracy: 0.9478 - val_loss: 0.1832 - val_accuracy: 0.9555 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1923 - accuracy: 0.9492 - val_loss: 0.1818 - val_accuracy: 0.9558 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1930 - accuracy: 0.9492 - val_loss: 0.1820 - val_accuracy: 0.9555 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1862 - accuracy: 0.9518 - val_loss: 0.1809 - val_accuracy: 0.9559 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1850 - accuracy: 0.9521 - val_loss: 0.1815 - val_accuracy: 0.9558 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1855 - accuracy: 0.9508 - val_loss: 0.1815 - val_accuracy: 0.9560 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1833 - accuracy: 0.9517 - val_loss: 0.1814 - val_accuracy: 0.9557 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1869 - accuracy: 0.9511 - val_loss: 0.1814 - val_accuracy: 0.9558 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1894 - accuracy: 0.9508 - val_loss: 0.1814 - val_accuracy: 0.9560 - lr: 1.3753e-06\n",
      "  1/313 [..............................] - ETA: 27s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\2724651961.py:46: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, 'model_1_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9559\n",
      "Accuracy: 0.9559396505355835\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1712\n",
      "           1       0.92      0.96      0.94       715\n",
      "           2       0.98      0.98      0.98        91\n",
      "           3       0.95      0.97      0.96       649\n",
      "           4       0.96      0.97      0.96       505\n",
      "           5       0.97      0.97      0.97      1392\n",
      "           6       0.94      0.95      0.94      1247\n",
      "           7       0.93      0.93      0.93       890\n",
      "           8       0.00      0.00      0.00         6\n",
      "           9       0.84      1.00      0.91        16\n",
      "          10       0.00      0.00      0.00         3\n",
      "          11       0.94      0.93      0.94       245\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00        15\n",
      "          14       0.93      0.48      0.63        27\n",
      "          15       1.00      0.80      0.89        10\n",
      "          16       0.95      0.96      0.96       624\n",
      "          17       0.90      0.53      0.67        36\n",
      "          18       0.91      0.92      0.91       417\n",
      "          19       0.96      0.92      0.94       132\n",
      "          20       0.99      0.98      0.99      1194\n",
      "          21       0.98      0.82      0.90        57\n",
      "          22       0.67      0.08      0.14        25\n",
      "\n",
      "    accuracy                           0.96     10009\n",
      "   macro avg       0.81      0.75      0.76     10009\n",
      "weighted avg       0.95      0.96      0.95     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage1\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Compile the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model fit\n",
    "run_1 = model.fit(X_train_combined, y_train, epochs=60, batch_size=32,\n",
    "                  validation_data=(X_test_combined, y_test),\n",
    "                  callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "save_model(model, 'model_1_preberttune.h5')\n",
    "\n",
    "y_pred_probabilities = model.predict(X_test_combined)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.8290 - accuracy: 0.5915 - val_loss: 0.6525 - val_accuracy: 0.8276 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8776 - accuracy: 0.7640 - val_loss: 0.4223 - val_accuracy: 0.8740 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6973 - accuracy: 0.8022 - val_loss: 0.3494 - val_accuracy: 0.8952 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6213 - accuracy: 0.8186 - val_loss: 0.3055 - val_accuracy: 0.9046 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5692 - accuracy: 0.8301 - val_loss: 0.2887 - val_accuracy: 0.9150 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5363 - accuracy: 0.8391 - val_loss: 0.2746 - val_accuracy: 0.9179 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5063 - accuracy: 0.8485 - val_loss: 0.2660 - val_accuracy: 0.9210 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4751 - accuracy: 0.8563 - val_loss: 0.2596 - val_accuracy: 0.9196 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4509 - accuracy: 0.8631 - val_loss: 0.2432 - val_accuracy: 0.9311 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4233 - accuracy: 0.8715 - val_loss: 0.2172 - val_accuracy: 0.9367 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4052 - accuracy: 0.8778 - val_loss: 0.2109 - val_accuracy: 0.9384 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3865 - accuracy: 0.8817 - val_loss: 0.2092 - val_accuracy: 0.9407 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3767 - accuracy: 0.8867 - val_loss: 0.2058 - val_accuracy: 0.9419 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3721 - accuracy: 0.8877 - val_loss: 0.2003 - val_accuracy: 0.9446 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3608 - accuracy: 0.8915 - val_loss: 0.1982 - val_accuracy: 0.9445 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3568 - accuracy: 0.8933 - val_loss: 0.1968 - val_accuracy: 0.9445 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3432 - accuracy: 0.8950 - val_loss: 0.1964 - val_accuracy: 0.9450 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3413 - accuracy: 0.8959 - val_loss: 0.1959 - val_accuracy: 0.9454 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3418 - accuracy: 0.8968 - val_loss: 0.1944 - val_accuracy: 0.9453 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3451 - accuracy: 0.8966 - val_loss: 0.1942 - val_accuracy: 0.9452 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3334 - accuracy: 0.9001 - val_loss: 0.1941 - val_accuracy: 0.9455 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3317 - accuracy: 0.8993 - val_loss: 0.1939 - val_accuracy: 0.9451 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3342 - accuracy: 0.9000 - val_loss: 0.1937 - val_accuracy: 0.9459 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3335 - accuracy: 0.8985 - val_loss: 0.1933 - val_accuracy: 0.9453 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.3362 - accuracy: 0.8982 - val_loss: 0.1940 - val_accuracy: 0.9456 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3317 - accuracy: 0.8993 - val_loss: 0.1939 - val_accuracy: 0.9455 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3299 - accuracy: 0.9005 - val_loss: 0.1934 - val_accuracy: 0.9458 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3299 - accuracy: 0.8984 - val_loss: 0.1939 - val_accuracy: 0.9456 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3341 - accuracy: 0.8991 - val_loss: 0.1939 - val_accuracy: 0.9455 - lr: 1.7824e-07\n",
      " 36/313 [==>...........................] - ETA: 0s - loss: 0.1783 - accuracy: 0.9488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\4019872855.py:41: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_2, 'model_2_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9453\n",
      "Accuracy for Stage 2 model: 0.9453491568565369\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 2 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90        35\n",
      "           1       0.89      0.74      0.81        23\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       0.96      1.00      0.98        54\n",
      "           4       0.94      0.92      0.93       128\n",
      "           5       0.87      1.00      0.93        13\n",
      "           6       1.00      0.88      0.93         8\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       1.00      0.96      0.98        79\n",
      "           9       1.00      1.00      1.00         9\n",
      "          10       0.96      0.92      0.94       156\n",
      "          11       0.86      0.52      0.65        71\n",
      "          12       1.00      0.55      0.71        11\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       0.89      0.98      0.93       128\n",
      "          15       0.83      0.83      0.83         6\n",
      "          16       0.95      0.97      0.96       278\n",
      "          17       0.89      0.67      0.76        12\n",
      "          18       1.00      0.58      0.74        12\n",
      "          19       0.86      0.60      0.71        10\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.99      1.00      1.00      1242\n",
      "          22       0.60      0.75      0.67         4\n",
      "          23       0.96      0.99      0.97       464\n",
      "          24       1.00      0.91      0.95        74\n",
      "          25       0.92      0.98      0.95       125\n",
      "          26       0.96      0.56      0.71        39\n",
      "          27       1.00      0.98      0.99        83\n",
      "          28       0.89      0.98      0.93       372\n",
      "          29       0.92      0.92      0.92        88\n",
      "          30       0.98      0.96      0.97        57\n",
      "          31       1.00      0.25      0.40         8\n",
      "          32       0.88      0.75      0.81        28\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       1.00      1.00      1.00        14\n",
      "          35       1.00      1.00      1.00       245\n",
      "          36       1.00      0.33      0.50         6\n",
      "          37       0.97      0.88      0.93        43\n",
      "          38       0.90      0.93      0.92       105\n",
      "          39       0.97      0.94      0.95        32\n",
      "          40       1.00      0.75      0.86         4\n",
      "          41       1.00      1.00      1.00         6\n",
      "          42       1.00      1.00      1.00       132\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       0.85      0.96      0.90        24\n",
      "          45       0.99      1.00      0.99       207\n",
      "          46       0.97      0.98      0.98       200\n",
      "          47       0.92      0.97      0.95       275\n",
      "          48       0.91      0.85      0.88        34\n",
      "          49       1.00      0.86      0.93        43\n",
      "          50       0.94      0.98      0.96       134\n",
      "          51       1.00      1.00      1.00         2\n",
      "          52       0.94      0.84      0.89        19\n",
      "          53       1.00      0.57      0.73         7\n",
      "          54       0.94      0.68      0.79        25\n",
      "          55       0.98      0.99      0.99       113\n",
      "          56       1.00      1.00      1.00        27\n",
      "          57       0.78      0.67      0.72        27\n",
      "          58       1.00      1.00      1.00        27\n",
      "          59       0.70      0.89      0.78       100\n",
      "          60       1.00      1.00      1.00        58\n",
      "          61       1.00      1.00      1.00        52\n",
      "          62       1.00      1.00      1.00         6\n",
      "          63       1.00      1.00      1.00         4\n",
      "          64       0.63      0.85      0.72        99\n",
      "          65       0.98      0.95      0.96        58\n",
      "          66       0.82      0.60      0.69        30\n",
      "          67       0.86      0.94      0.90        32\n",
      "          68       0.92      0.75      0.83        16\n",
      "          69       0.96      0.98      0.97       217\n",
      "          70       0.92      0.87      0.89        53\n",
      "          71       0.95      0.91      0.93       108\n",
      "          72       0.95      0.66      0.78        32\n",
      "          73       0.00      0.00      0.00        10\n",
      "          74       0.00      0.00      0.00         2\n",
      "          75       1.00      0.73      0.84        22\n",
      "          76       0.00      0.00      0.00        13\n",
      "          77       0.33      0.08      0.13        37\n",
      "          78       0.49      0.93      0.64        68\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.99      1.00      0.99       722\n",
      "          81       1.00      1.00      1.00        31\n",
      "          82       0.99      0.99      0.99       236\n",
      "          83       0.68      0.49      0.57        57\n",
      "          84       0.96      0.97      0.97       197\n",
      "          85       1.00      0.89      0.94         9\n",
      "          86       0.00      0.00      0.00         8\n",
      "          87       0.88      0.78      0.82         9\n",
      "          88       0.98      0.98      0.98       181\n",
      "          89       0.98      0.99      0.99       614\n",
      "          90       1.00      0.99      0.99        84\n",
      "          91       1.00      0.38      0.55         8\n",
      "          92       0.94      0.97      0.96       117\n",
      "          93       1.00      0.60      0.75         5\n",
      "          94       0.97      0.99      0.98       191\n",
      "          95       1.00      0.97      0.98        61\n",
      "          96       0.93      1.00      0.96        68\n",
      "          97       0.99      1.00      0.99        72\n",
      "          98       0.90      1.00      0.95        55\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       1.00      1.00      1.00        64\n",
      "         101       0.97      0.97      0.97        62\n",
      "         102       0.77      0.81      0.79        21\n",
      "         103       0.99      0.99      0.99        96\n",
      "         104       0.82      0.67      0.74        21\n",
      "         105       0.75      0.80      0.77        15\n",
      "         106       0.78      0.72      0.75        65\n",
      "         107       0.83      0.96      0.89        54\n",
      "         108       0.97      0.97      0.97       243\n",
      "         109       0.95      1.00      0.97        18\n",
      "         110       0.95      0.78      0.86        23\n",
      "         111       1.00      1.00      1.00        12\n",
      "         112       0.96      0.90      0.93       154\n",
      "         113       0.88      0.33      0.48        21\n",
      "\n",
      "    accuracy                           0.95     10009\n",
      "   macro avg       0.85      0.79      0.81     10009\n",
      "weighted avg       0.94      0.95      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage2\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_2 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_1.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_2 = model_2.fit(X_train_combined_1, y_train_1, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_1, y_test_1),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_2, 'model_2_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_2, accuracy_2 = model_2.evaluate(X_test_combined_1, y_test_1)\n",
    "print(\"Accuracy for Stage 2 model:\", accuracy_2)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_2 = model_2.predict(X_test_combined_1)\n",
    "y_pred_2 = np.argmax(y_pred_probabilities_2, axis=1)\n",
    "report_2 = classification_report(y_test_1, y_pred_2)\n",
    "print(\"Classification Report for Stage 2 model:\")\n",
    "print(report_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 2.8146 - accuracy: 0.4691 - val_loss: 1.3406 - val_accuracy: 0.7048 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.4910 - accuracy: 0.6539 - val_loss: 0.8169 - val_accuracy: 0.7912 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.1543 - accuracy: 0.7088 - val_loss: 0.6357 - val_accuracy: 0.8221 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 1.0036 - accuracy: 0.7356 - val_loss: 0.5503 - val_accuracy: 0.8440 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.9057 - accuracy: 0.7569 - val_loss: 0.4833 - val_accuracy: 0.8627 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8386 - accuracy: 0.7698 - val_loss: 0.4864 - val_accuracy: 0.8653 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7829 - accuracy: 0.7838 - val_loss: 0.4482 - val_accuracy: 0.8748 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7398 - accuracy: 0.7951 - val_loss: 0.4282 - val_accuracy: 0.8808 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6940 - accuracy: 0.8056 - val_loss: 0.4199 - val_accuracy: 0.8878 - lr: 4.3541e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6682 - accuracy: 0.8093 - val_loss: 0.3939 - val_accuracy: 0.8906 - lr: 3.7010e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6460 - accuracy: 0.8182 - val_loss: 0.3847 - val_accuracy: 0.8917 - lr: 3.1459e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6231 - accuracy: 0.8228 - val_loss: 0.3730 - val_accuracy: 0.8960 - lr: 2.6740e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6017 - accuracy: 0.8278 - val_loss: 0.3778 - val_accuracy: 0.8965 - lr: 2.2729e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5857 - accuracy: 0.8346 - val_loss: 0.3608 - val_accuracy: 0.8994 - lr: 1.9319e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5707 - accuracy: 0.8360 - val_loss: 0.3533 - val_accuracy: 0.8998 - lr: 1.6422e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5650 - accuracy: 0.8375 - val_loss: 0.3521 - val_accuracy: 0.9029 - lr: 1.3958e-04\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5485 - accuracy: 0.8438 - val_loss: 0.3519 - val_accuracy: 0.9042 - lr: 1.0469e-04\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5368 - accuracy: 0.8459 - val_loss: 0.3494 - val_accuracy: 0.9044 - lr: 7.8515e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5361 - accuracy: 0.8460 - val_loss: 0.3454 - val_accuracy: 0.9035 - lr: 5.8887e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5300 - accuracy: 0.8469 - val_loss: 0.3450 - val_accuracy: 0.9052 - lr: 4.1221e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5213 - accuracy: 0.8500 - val_loss: 0.3446 - val_accuracy: 0.9059 - lr: 2.8854e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5307 - accuracy: 0.8457 - val_loss: 0.3434 - val_accuracy: 0.9060 - lr: 2.0198e-05\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5182 - accuracy: 0.8512 - val_loss: 0.3440 - val_accuracy: 0.9053 - lr: 1.4139e-05\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5156 - accuracy: 0.8534 - val_loss: 0.3431 - val_accuracy: 0.9049 - lr: 9.8971e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5184 - accuracy: 0.8534 - val_loss: 0.3426 - val_accuracy: 0.9051 - lr: 6.9280e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5105 - accuracy: 0.8520 - val_loss: 0.3429 - val_accuracy: 0.9059 - lr: 4.8496e-06\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5138 - accuracy: 0.8524 - val_loss: 0.3426 - val_accuracy: 0.9056 - lr: 3.3947e-06\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5201 - accuracy: 0.8519 - val_loss: 0.3420 - val_accuracy: 0.9059 - lr: 2.3763e-06\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5118 - accuracy: 0.8537 - val_loss: 0.3431 - val_accuracy: 0.9056 - lr: 1.6634e-06\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5158 - accuracy: 0.8507 - val_loss: 0.3426 - val_accuracy: 0.9061 - lr: 1.1644e-06\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5113 - accuracy: 0.8525 - val_loss: 0.3422 - val_accuracy: 0.9064 - lr: 8.1507e-07\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5156 - accuracy: 0.8517 - val_loss: 0.3415 - val_accuracy: 0.9061 - lr: 5.7055e-07\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5180 - accuracy: 0.8515 - val_loss: 0.3423 - val_accuracy: 0.9061 - lr: 3.9938e-07\n",
      "Epoch 34/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5189 - accuracy: 0.8518 - val_loss: 0.3419 - val_accuracy: 0.9062 - lr: 2.7957e-07\n",
      "Epoch 35/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5160 - accuracy: 0.8511 - val_loss: 0.3426 - val_accuracy: 0.9057 - lr: 1.9570e-07\n",
      "Epoch 36/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5121 - accuracy: 0.8516 - val_loss: 0.3422 - val_accuracy: 0.9058 - lr: 1.3699e-07\n",
      "Epoch 37/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5128 - accuracy: 0.8514 - val_loss: 0.3418 - val_accuracy: 0.9056 - lr: 9.5892e-08\n",
      " 30/313 [=>............................] - ETA: 0s - loss: 0.3568 - accuracy: 0.9073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\3792186953.py:41: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_3, 'model_3_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.9061\n",
      "Accuracy for Stage 3 model: 0.9060845375061035\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 3 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.79      1.00      0.88        70\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       1.00      0.86      0.92         7\n",
      "           5       0.67      1.00      0.80         6\n",
      "           6       1.00      1.00      1.00         8\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       0.84      0.97      0.90        39\n",
      "           9       1.00      0.50      0.67         6\n",
      "          10       0.60      1.00      0.75         3\n",
      "          11       0.91      0.91      0.91        35\n",
      "          12       0.88      0.94      0.91       101\n",
      "          13       0.92      0.96      0.94        25\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.91      0.95      0.93        21\n",
      "          16       1.00      1.00      1.00        83\n",
      "          17       0.62      0.97      0.76        34\n",
      "          18       0.71      0.24      0.36        21\n",
      "          19       1.00      1.00      1.00       113\n",
      "          20       0.93      1.00      0.96        25\n",
      "          21       0.95      1.00      0.97        55\n",
      "          22       0.00      0.00      0.00        18\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       0.70      1.00      0.82       135\n",
      "          25       0.91      0.98      0.95        53\n",
      "          26       0.98      1.00      0.99        63\n",
      "          27       1.00      0.70      0.82        10\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.86      0.75      0.80         8\n",
      "          31       1.00      1.00      1.00        43\n",
      "          32       0.50      0.67      0.57         3\n",
      "          33       0.83      0.42      0.56        12\n",
      "          34       1.00      0.67      0.80         6\n",
      "          35       0.45      0.83      0.59         6\n",
      "          36       1.00      0.90      0.95        10\n",
      "          37       1.00      1.00      1.00         9\n",
      "          38       0.00      0.00      0.00         2\n",
      "          39       0.00      0.00      0.00         6\n",
      "          40       1.00      1.00      1.00        13\n",
      "          41       0.88      0.98      0.93        61\n",
      "          42       1.00      1.00      1.00         4\n",
      "          43       0.80      0.44      0.57         9\n",
      "          44       1.00      0.75      0.86         4\n",
      "          45       1.00      1.00      1.00         4\n",
      "          46       1.00      1.00      1.00         1\n",
      "          47       1.00      0.25      0.40         4\n",
      "          48       1.00      1.00      1.00         3\n",
      "          49       0.93      1.00      0.96        50\n",
      "          50       0.55      0.55      0.55        11\n",
      "          51       0.96      0.89      0.92        27\n",
      "          52       1.00      1.00      1.00         7\n",
      "          53       1.00      0.44      0.62         9\n",
      "          54       0.80      0.95      0.87        41\n",
      "          55       1.00      1.00      1.00         3\n",
      "          56       0.00      0.00      0.00         4\n",
      "          57       0.95      1.00      0.97        18\n",
      "          58       0.50      0.20      0.29         5\n",
      "          59       0.91      0.89      0.90        36\n",
      "          60       0.95      1.00      0.97        76\n",
      "          61       0.96      0.98      0.97        84\n",
      "          63       0.67      0.18      0.29        11\n",
      "          64       1.00      1.00      1.00         3\n",
      "          65       0.96      0.92      0.94        25\n",
      "          66       0.93      1.00      0.96       243\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.75      1.00      0.86         6\n",
      "          69       0.92      0.88      0.90        26\n",
      "          70       0.00      0.00      0.00         8\n",
      "          71       0.97      1.00      0.98        29\n",
      "          72       0.89      0.95      0.92        42\n",
      "          73       1.00      1.00      1.00         3\n",
      "          74       1.00      0.33      0.50         3\n",
      "          75       0.85      1.00      0.92       184\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       0.00      0.00      0.00         4\n",
      "          78       0.96      0.97      0.96       159\n",
      "          79       0.94      0.94      0.94       395\n",
      "          80       0.80      0.80      0.80         5\n",
      "          81       0.67      0.40      0.50         5\n",
      "          82       1.00      0.94      0.97        16\n",
      "          83       1.00      1.00      1.00         2\n",
      "          84       0.00      0.00      0.00         4\n",
      "          85       0.50      0.50      0.50         2\n",
      "          86       1.00      1.00      1.00         6\n",
      "          87       0.95      0.69      0.80        29\n",
      "          88       0.78      0.64      0.70        11\n",
      "          89       1.00      0.09      0.17        11\n",
      "          90       1.00      1.00      1.00         4\n",
      "          91       1.00      0.62      0.77         8\n",
      "          92       0.80      0.80      0.80         5\n",
      "          93       1.00      1.00      1.00       236\n",
      "          94       0.91      0.96      0.93       131\n",
      "          95       1.00      1.00      1.00       200\n",
      "          96       0.50      0.50      0.50         6\n",
      "          97       0.90      0.90      0.90        29\n",
      "          98       0.00      0.00      0.00         3\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       1.00      1.00      1.00         5\n",
      "         101       0.57      0.40      0.47        10\n",
      "         102       0.84      1.00      0.91        21\n",
      "         103       1.00      0.40      0.57        10\n",
      "         104       1.00      0.50      0.67         4\n",
      "         105       1.00      0.17      0.29         6\n",
      "         106       0.80      0.40      0.53        10\n",
      "         107       0.00      0.00      0.00         3\n",
      "         108       1.00      1.00      1.00         3\n",
      "         109       0.85      1.00      0.92        28\n",
      "         110       0.00      0.00      0.00         7\n",
      "         111       0.87      1.00      0.93        13\n",
      "         112       1.00      0.80      0.89         5\n",
      "         113       0.50      0.17      0.25         6\n",
      "         114       0.80      0.97      0.88        29\n",
      "         115       0.91      0.71      0.80        14\n",
      "         116       1.00      1.00      1.00        58\n",
      "         117       1.00      0.67      0.80         3\n",
      "         118       0.78      0.86      0.81        69\n",
      "         119       0.95      1.00      0.97        19\n",
      "         120       0.89      1.00      0.94        17\n",
      "         121       0.58      0.79      0.67        19\n",
      "         122       1.00      0.50      0.67         4\n",
      "         123       0.54      1.00      0.70         7\n",
      "         124       1.00      0.33      0.50         9\n",
      "         125       0.91      0.83      0.87        12\n",
      "         126       0.00      0.00      0.00         2\n",
      "         127       1.00      0.25      0.40         4\n",
      "         128       1.00      1.00      1.00         9\n",
      "         129       0.80      0.67      0.73         6\n",
      "         130       0.94      0.94      0.94        63\n",
      "         131       1.00      0.91      0.95        11\n",
      "         132       0.91      1.00      0.95        10\n",
      "         133       0.91      0.94      0.92        32\n",
      "         134       0.81      0.81      0.81        21\n",
      "         135       1.00      0.33      0.50         3\n",
      "         136       0.92      0.65      0.76        17\n",
      "         137       0.00      0.00      0.00         7\n",
      "         138       0.60      1.00      0.75         6\n",
      "         139       0.00      0.00      0.00         5\n",
      "         140       1.00      0.89      0.94        19\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       1.00      1.00      1.00         7\n",
      "         143       1.00      0.92      0.96        26\n",
      "         144       1.00      0.40      0.57         5\n",
      "         145       1.00      1.00      1.00         2\n",
      "         146       0.82      1.00      0.90         9\n",
      "         147       0.86      1.00      0.92         6\n",
      "         148       1.00      0.67      0.80         6\n",
      "         149       0.83      1.00      0.91        35\n",
      "         150       0.75      1.00      0.86        18\n",
      "         151       1.00      0.25      0.40         8\n",
      "         152       0.99      0.97      0.98        71\n",
      "         153       0.00      0.00      0.00         5\n",
      "         154       0.71      0.71      0.71         7\n",
      "         155       0.97      1.00      0.99        37\n",
      "         156       1.00      1.00      1.00        12\n",
      "         157       0.00      0.00      0.00         2\n",
      "         158       0.73      0.61      0.67        18\n",
      "         159       0.85      0.85      0.85        13\n",
      "         160       0.00      0.00      0.00        10\n",
      "         161       0.75      0.69      0.72        26\n",
      "         162       1.00      1.00      1.00        32\n",
      "         163       1.00      1.00      1.00        19\n",
      "         164       0.70      0.50      0.58        28\n",
      "         165       1.00      0.33      0.50         3\n",
      "         166       1.00      1.00      1.00         7\n",
      "         167       0.50      0.50      0.50         4\n",
      "         168       1.00      0.25      0.40         4\n",
      "         169       0.00      0.00      0.00         1\n",
      "         170       0.86      0.91      0.88       112\n",
      "         171       0.00      0.00      0.00         4\n",
      "         172       0.54      1.00      0.70        28\n",
      "         173       1.00      1.00      1.00         4\n",
      "         174       0.94      0.97      0.95       146\n",
      "         175       0.84      1.00      0.91        26\n",
      "         176       0.60      0.60      0.60         5\n",
      "         177       0.79      0.73      0.76        15\n",
      "         178       0.50      0.58      0.54        12\n",
      "         179       0.00      0.00      0.00         8\n",
      "         180       0.76      0.76      0.76        17\n",
      "         181       0.83      0.96      0.89        26\n",
      "         182       0.89      1.00      0.94        33\n",
      "         183       1.00      1.00      1.00         7\n",
      "         185       1.00      1.00      1.00        23\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.50      0.86      0.63         7\n",
      "         188       0.00      0.00      0.00         1\n",
      "         189       0.00      0.00      0.00         2\n",
      "         190       0.94      1.00      0.97        17\n",
      "         191       0.86      1.00      0.92        60\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       0.77      0.98      0.87        59\n",
      "         194       0.79      0.92      0.85        12\n",
      "         195       0.88      1.00      0.93         7\n",
      "         196       0.96      0.92      0.94        26\n",
      "         197       0.97      0.89      0.93        35\n",
      "         198       1.00      0.50      0.67         2\n",
      "         199       0.89      0.76      0.82        21\n",
      "         200       1.00      0.68      0.81        25\n",
      "         201       0.83      1.00      0.91         5\n",
      "         202       1.00      1.00      1.00       191\n",
      "         203       0.00      0.00      0.00         4\n",
      "         204       0.70      1.00      0.82        21\n",
      "         205       1.00      1.00      1.00        16\n",
      "         206       0.00      0.00      0.00         4\n",
      "         207       0.80      1.00      0.89         4\n",
      "         208       1.00      1.00      1.00        25\n",
      "         209       0.97      1.00      0.98        31\n",
      "         210       0.98      0.98      0.98        52\n",
      "         211       0.00      0.00      0.00         2\n",
      "         212       1.00      0.33      0.50         3\n",
      "         213       1.00      1.00      1.00         6\n",
      "         214       0.95      1.00      0.97        88\n",
      "         215       1.00      0.88      0.93         8\n",
      "         216       0.69      0.94      0.80        80\n",
      "         217       1.00      0.50      0.67         6\n",
      "         218       0.75      1.00      0.86         3\n",
      "         219       0.95      0.95      0.95        21\n",
      "         220       1.00      0.50      0.67         4\n",
      "         221       0.60      0.55      0.57        11\n",
      "         222       0.00      0.00      0.00         3\n",
      "         223       1.00      1.00      1.00         8\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       0.67      1.00      0.80         2\n",
      "         226       0.97      1.00      0.99        33\n",
      "         227       0.87      1.00      0.93        13\n",
      "         228       1.00      1.00      1.00        54\n",
      "         229       0.78      1.00      0.88         7\n",
      "         230       0.93      0.99      0.96       102\n",
      "         231       1.00      0.69      0.82        13\n",
      "         232       0.82      1.00      0.90        14\n",
      "         233       1.00      0.89      0.94        35\n",
      "         235       0.98      1.00      0.99        47\n",
      "         236       1.00      0.43      0.60         7\n",
      "         237       1.00      1.00      1.00         4\n",
      "         238       0.60      0.92      0.73        13\n",
      "         239       0.62      1.00      0.77        10\n",
      "         240       0.60      0.38      0.46         8\n",
      "         241       0.86      1.00      0.92         6\n",
      "         242       0.75      0.55      0.63        11\n",
      "         243       0.67      1.00      0.80         4\n",
      "         244       1.00      0.86      0.92         7\n",
      "         245       1.00      1.00      1.00         3\n",
      "         246       1.00      1.00      1.00        15\n",
      "         247       1.00      1.00      1.00       213\n",
      "         248       0.90      1.00      0.95        19\n",
      "         249       1.00      1.00      1.00        31\n",
      "         250       1.00      1.00      1.00         3\n",
      "         251       1.00      1.00      1.00         2\n",
      "         252       1.00      1.00      1.00         8\n",
      "         253       1.00      0.12      0.22         8\n",
      "         254       0.75      1.00      0.86         6\n",
      "         255       0.81      0.97      0.88        31\n",
      "         256       0.92      0.96      0.94        25\n",
      "         257       0.94      1.00      0.97        15\n",
      "         258       0.67      1.00      0.80         2\n",
      "         259       0.86      0.75      0.80        16\n",
      "         260       0.93      0.93      0.93        15\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      0.78      0.88         9\n",
      "         263       0.90      1.00      0.95         9\n",
      "         264       0.56      0.95      0.70        21\n",
      "         265       0.50      1.00      0.67         1\n",
      "         266       1.00      1.00      1.00        12\n",
      "         267       1.00      0.62      0.77         8\n",
      "         268       1.00      0.60      0.75         5\n",
      "         269       0.89      0.64      0.75        39\n",
      "         270       0.83      0.98      0.90        53\n",
      "         271       0.83      0.96      0.89       134\n",
      "         272       1.00      0.92      0.96        13\n",
      "         273       0.00      0.00      0.00         7\n",
      "         274       0.64      0.88      0.74         8\n",
      "         275       1.00      1.00      1.00         3\n",
      "         276       0.00      0.00      0.00         6\n",
      "         277       0.89      0.67      0.76        12\n",
      "         278       0.72      0.88      0.79        26\n",
      "         279       0.82      1.00      0.90         9\n",
      "         280       0.95      1.00      0.97        18\n",
      "         281       1.00      0.67      0.80         3\n",
      "         282       1.00      1.00      1.00         2\n",
      "         283       0.46      1.00      0.63         6\n",
      "         284       1.00      0.96      0.98        67\n",
      "         285       0.80      0.80      0.80         5\n",
      "         286       1.00      1.00      1.00        34\n",
      "         287       0.93      0.97      0.95       170\n",
      "         288       0.77      0.94      0.85        65\n",
      "         289       0.80      1.00      0.89         8\n",
      "         290       0.90      0.64      0.75        14\n",
      "         291       1.00      1.00      1.00         2\n",
      "         292       1.00      0.33      0.50         3\n",
      "         293       1.00      0.25      0.40         8\n",
      "         294       1.00      0.62      0.77         8\n",
      "         295       0.92      0.82      0.87        28\n",
      "         296       1.00      1.00      1.00        19\n",
      "         297       0.40      0.40      0.40         5\n",
      "         298       1.00      0.50      0.67        10\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       1.00      0.73      0.85        15\n",
      "         301       1.00      0.93      0.96        27\n",
      "         302       0.82      0.92      0.87        25\n",
      "         303       1.00      1.00      1.00         4\n",
      "         304       0.88      0.88      0.88        17\n",
      "         305       0.92      0.96      0.94        24\n",
      "         306       1.00      1.00      1.00        79\n",
      "         307       1.00      0.73      0.84        22\n",
      "         308       0.91      0.97      0.94        33\n",
      "         309       1.00      0.75      0.86         4\n",
      "         310       0.00      0.00      0.00        12\n",
      "         311       0.60      1.00      0.75         3\n",
      "         312       1.00      1.00      1.00         3\n",
      "         313       0.74      0.96      0.83        26\n",
      "         314       1.00      0.62      0.77         8\n",
      "         315       1.00      0.95      0.97        55\n",
      "         316       1.00      0.60      0.75         5\n",
      "         317       0.89      1.00      0.94         8\n",
      "         318       0.86      0.92      0.89        13\n",
      "         319       0.90      1.00      0.95        38\n",
      "         320       0.00      0.00      0.00         1\n",
      "         321       1.00      1.00      1.00        10\n",
      "         322       1.00      0.60      0.75         5\n",
      "         323       1.00      0.10      0.18        10\n",
      "         324       1.00      1.00      1.00        28\n",
      "         325       1.00      0.50      0.67         2\n",
      "         326       0.96      1.00      0.98        25\n",
      "         327       1.00      1.00      1.00         2\n",
      "         328       1.00      1.00      1.00         3\n",
      "         329       1.00      1.00      1.00        18\n",
      "         330       0.75      1.00      0.86         3\n",
      "         331       1.00      0.90      0.95        10\n",
      "         332       0.00      0.00      0.00        11\n",
      "         333       0.86      0.38      0.52        16\n",
      "         334       0.00      0.00      0.00         9\n",
      "         335       0.67      0.67      0.67         3\n",
      "         336       0.62      1.00      0.77         5\n",
      "         337       1.00      0.83      0.91         6\n",
      "         338       0.98      1.00      0.99        55\n",
      "         339       0.89      1.00      0.94        16\n",
      "         340       0.00      0.00      0.00         1\n",
      "         341       1.00      0.97      0.99        37\n",
      "         342       0.64      0.88      0.74         8\n",
      "         343       0.00      0.00      0.00         9\n",
      "         344       0.93      0.97      0.95       403\n",
      "         345       1.00      1.00      1.00         7\n",
      "         346       1.00      1.00      1.00         5\n",
      "         347       0.97      0.93      0.95      1254\n",
      "         348       0.73      0.58      0.65       145\n",
      "\n",
      "    accuracy                           0.91     10009\n",
      "   macro avg       0.78      0.74      0.74     10009\n",
      "weighted avg       0.90      0.91      0.89     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 3\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.85\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.75\n",
    "    else:\n",
    "        return lr * 0.7\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "model_3 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_2.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_3 = model_3.fit(X_train_combined_2, y_train_2, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_2, y_test_2),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_3, 'model_3_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_3, accuracy_3 = model_3.evaluate(X_test_combined_2, y_test_2)\n",
    "print(\"Accuracy for Stage 3 model:\", accuracy_3)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_3 = model_3.predict(X_test_combined_2)\n",
    "y_pred_3 = np.argmax(y_pred_probabilities_3, axis=1)\n",
    "report_3 = classification_report(y_test_2, y_pred_3)\n",
    "print(\"Classification Report for Stage 3 model:\")\n",
    "print(report_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 2.5734 - accuracy: 0.5470 - val_loss: 1.3015 - val_accuracy: 0.7166 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.3942 - accuracy: 0.6906 - val_loss: 0.8040 - val_accuracy: 0.7996 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.0790 - accuracy: 0.7383 - val_loss: 0.6155 - val_accuracy: 0.8409 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.9129 - accuracy: 0.7668 - val_loss: 0.4903 - val_accuracy: 0.8507 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.8244 - accuracy: 0.7842 - val_loss: 0.4852 - val_accuracy: 0.8624 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7471 - accuracy: 0.7982 - val_loss: 0.4259 - val_accuracy: 0.8775 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6952 - accuracy: 0.8100 - val_loss: 0.3915 - val_accuracy: 0.8858 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6461 - accuracy: 0.8192 - val_loss: 0.3876 - val_accuracy: 0.8910 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6163 - accuracy: 0.8271 - val_loss: 0.3411 - val_accuracy: 0.8981 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5793 - accuracy: 0.8367 - val_loss: 0.3285 - val_accuracy: 0.9001 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5521 - accuracy: 0.8435 - val_loss: 0.3282 - val_accuracy: 0.9041 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5382 - accuracy: 0.8452 - val_loss: 0.3253 - val_accuracy: 0.9074 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5138 - accuracy: 0.8528 - val_loss: 0.3135 - val_accuracy: 0.9108 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5019 - accuracy: 0.8559 - val_loss: 0.3085 - val_accuracy: 0.9124 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4995 - accuracy: 0.8570 - val_loss: 0.3101 - val_accuracy: 0.9114 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4848 - accuracy: 0.8604 - val_loss: 0.3052 - val_accuracy: 0.9133 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4761 - accuracy: 0.8623 - val_loss: 0.3007 - val_accuracy: 0.9125 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4701 - accuracy: 0.8639 - val_loss: 0.3016 - val_accuracy: 0.9149 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.4623 - accuracy: 0.8666 - val_loss: 0.3036 - val_accuracy: 0.9145 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4669 - accuracy: 0.8658 - val_loss: 0.3004 - val_accuracy: 0.9155 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4665 - accuracy: 0.8651 - val_loss: 0.3004 - val_accuracy: 0.9147 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4633 - accuracy: 0.8656 - val_loss: 0.3007 - val_accuracy: 0.9158 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4655 - accuracy: 0.8655 - val_loss: 0.3003 - val_accuracy: 0.9160 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4666 - accuracy: 0.8648 - val_loss: 0.3005 - val_accuracy: 0.9153 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4610 - accuracy: 0.8661 - val_loss: 0.2989 - val_accuracy: 0.9148 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4588 - accuracy: 0.8668 - val_loss: 0.2969 - val_accuracy: 0.9151 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4647 - accuracy: 0.8659 - val_loss: 0.2986 - val_accuracy: 0.9145 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4587 - accuracy: 0.8661 - val_loss: 0.2988 - val_accuracy: 0.9150 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4591 - accuracy: 0.8675 - val_loss: 0.2997 - val_accuracy: 0.9152 - lr: 1.7824e-07\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4598 - accuracy: 0.8677 - val_loss: 0.3002 - val_accuracy: 0.9153 - lr: 1.0694e-07\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4615 - accuracy: 0.8656 - val_loss: 0.2990 - val_accuracy: 0.9149 - lr: 6.4167e-08\n",
      " 37/313 [==>...........................] - ETA: 0s - loss: 0.3154 - accuracy: 0.9181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\1281415934.py:40: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_4, 'model_4_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2969 - accuracy: 0.9151\n",
      "Accuracy for Stage 4 model: 0.9150764346122742\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 4 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      1.00      1.00        56\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.42      0.62      0.50         8\n",
      "           5       0.75      0.50      0.60         6\n",
      "           6       0.50      0.50      0.50         4\n",
      "           7       0.87      0.87      0.87        15\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       1.00      0.33      0.50         3\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.66      1.00      0.79        23\n",
      "          12       1.00      1.00      1.00        22\n",
      "          13       1.00      0.97      0.98        33\n",
      "          14       0.95      0.95      0.95        22\n",
      "          15       0.64      0.64      0.64        11\n",
      "          16       1.00      1.00      1.00         6\n",
      "          17       0.75      0.33      0.46         9\n",
      "          18       0.83      1.00      0.91         5\n",
      "          19       1.00      0.38      0.55         8\n",
      "          20       1.00      0.20      0.33         5\n",
      "          21       0.92      1.00      0.96        24\n",
      "          22       0.72      1.00      0.84        28\n",
      "          23       1.00      1.00      1.00         3\n",
      "          24       0.00      0.00      0.00         3\n",
      "          25       1.00      0.44      0.62         9\n",
      "          26       0.86      1.00      0.92        12\n",
      "          27       0.87      1.00      0.93        26\n",
      "          28       0.85      1.00      0.92        11\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       1.00      0.67      0.80         3\n",
      "          31       1.00      1.00      1.00         4\n",
      "          32       0.00      0.00      0.00         4\n",
      "          33       1.00      1.00      1.00        33\n",
      "          34       1.00      1.00      1.00       131\n",
      "          35       0.75      1.00      0.86         6\n",
      "          36       1.00      0.90      0.95        10\n",
      "          37       0.00      0.00      0.00         3\n",
      "          38       1.00      0.62      0.77         8\n",
      "          39       0.80      1.00      0.89        39\n",
      "          40       0.96      0.94      0.95        47\n",
      "          41       0.78      1.00      0.88         7\n",
      "          42       0.40      0.67      0.50         3\n",
      "          43       0.60      1.00      0.75         3\n",
      "          44       0.83      0.77      0.80        31\n",
      "          45       0.00      0.00      0.00         2\n",
      "          46       1.00      0.67      0.80         3\n",
      "          47       1.00      0.50      0.67         2\n",
      "          48       0.86      0.92      0.89        13\n",
      "          49       0.00      0.00      0.00         4\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.50      0.80      0.62         5\n",
      "          52       1.00      1.00      1.00         6\n",
      "          53       0.75      1.00      0.86         3\n",
      "          54       1.00      1.00      1.00       101\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       0.88      0.96      0.92        48\n",
      "          57       0.68      0.87      0.76        15\n",
      "          58       0.33      0.67      0.44         3\n",
      "          59       0.00      0.00      0.00         2\n",
      "          60       0.76      1.00      0.86        25\n",
      "          61       0.00      0.00      0.00         4\n",
      "          62       1.00      1.00      1.00        36\n",
      "          63       1.00      0.80      0.89         5\n",
      "          64       0.86      1.00      0.92         6\n",
      "          65       0.80      0.80      0.80         5\n",
      "          66       0.00      0.00      0.00         4\n",
      "          67       1.00      0.11      0.20         9\n",
      "          68       0.92      0.85      0.88        13\n",
      "          69       0.00      0.00      0.00         5\n",
      "          70       1.00      0.33      0.50         3\n",
      "          71       0.69      0.85      0.76        13\n",
      "          72       0.00      0.00      0.00         5\n",
      "          73       1.00      1.00      1.00         7\n",
      "          74       0.86      1.00      0.92        18\n",
      "          75       0.95      1.00      0.98        41\n",
      "          76       0.81      1.00      0.90        13\n",
      "          77       0.88      1.00      0.93         7\n",
      "          78       1.00      1.00      1.00       184\n",
      "          79       1.00      1.00      1.00        26\n",
      "          80       0.46      1.00      0.63         6\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.77      1.00      0.87        54\n",
      "          83       0.00      0.00      0.00         6\n",
      "          84       1.00      1.00      1.00        14\n",
      "          85       0.56      1.00      0.71         5\n",
      "          86       0.93      1.00      0.96       204\n",
      "          87       1.00      1.00      1.00         6\n",
      "          88       1.00      1.00      1.00         8\n",
      "          89       0.76      1.00      0.87        13\n",
      "          90       0.89      0.80      0.84        10\n",
      "          91       0.73      0.67      0.70        12\n",
      "          92       0.85      0.94      0.89        18\n",
      "          93       0.67      1.00      0.80         8\n",
      "          94       0.55      1.00      0.71        26\n",
      "          95       1.00      1.00      1.00         3\n",
      "          96       0.00      0.00      0.00         5\n",
      "          97       0.59      1.00      0.74        23\n",
      "          98       1.00      0.98      0.99        44\n",
      "          99       0.50      0.67      0.57         3\n",
      "         100       1.00      0.97      0.98        31\n",
      "         101       0.50      1.00      0.67         3\n",
      "         102       0.83      0.88      0.86        17\n",
      "         103       0.76      0.98      0.86        61\n",
      "         104       0.88      1.00      0.93        14\n",
      "         105       1.00      0.62      0.77         8\n",
      "         106       0.57      1.00      0.72        13\n",
      "         107       0.50      1.00      0.67         2\n",
      "         109       1.00      0.34      0.51        32\n",
      "         110       0.86      0.86      0.86         7\n",
      "         111       0.00      0.00      0.00         2\n",
      "         112       0.94      0.97      0.95        30\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       0.80      1.00      0.89         8\n",
      "         115       0.33      0.50      0.40         2\n",
      "         116       1.00      1.00      1.00        14\n",
      "         117       1.00      1.00      1.00         3\n",
      "         118       0.83      1.00      0.91        15\n",
      "         119       0.86      0.83      0.85        36\n",
      "         120       0.92      0.92      0.92        13\n",
      "         121       1.00      0.67      0.80         3\n",
      "         122       1.00      0.40      0.57         5\n",
      "         123       0.00      0.00      0.00         1\n",
      "         124       1.00      0.33      0.50         6\n",
      "         125       0.85      0.97      0.90        29\n",
      "         126       1.00      1.00      1.00         4\n",
      "         127       0.56      0.91      0.69        11\n",
      "         128       0.92      0.92      0.92        13\n",
      "         129       0.90      1.00      0.95        26\n",
      "         130       0.88      0.88      0.88         8\n",
      "         131       0.40      1.00      0.57        10\n",
      "         132       0.00      0.00      0.00         4\n",
      "         133       0.62      0.95      0.75        40\n",
      "         134       0.75      0.30      0.43        10\n",
      "         135       1.00      1.00      1.00        19\n",
      "         136       1.00      1.00      1.00         6\n",
      "         137       1.00      0.91      0.95        11\n",
      "         138       0.94      0.92      0.93        48\n",
      "         139       0.64      0.64      0.64        14\n",
      "         140       0.86      0.75      0.80         8\n",
      "         141       0.92      0.85      0.88        13\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.67      1.00      0.80         2\n",
      "         144       0.95      1.00      0.98        80\n",
      "         145       1.00      1.00      1.00        41\n",
      "         146       0.95      0.95      0.95        20\n",
      "         147       1.00      1.00      1.00         2\n",
      "         148       0.70      1.00      0.82         7\n",
      "         149       1.00      1.00      1.00        21\n",
      "         150       0.60      0.86      0.71         7\n",
      "         151       1.00      0.67      0.80         6\n",
      "         152       0.00      0.00      0.00         7\n",
      "         153       0.89      1.00      0.94         8\n",
      "         154       1.00      1.00      1.00         3\n",
      "         155       1.00      1.00      1.00         5\n",
      "         156       0.80      1.00      0.89         4\n",
      "         157       0.94      0.94      0.94        33\n",
      "         158       1.00      0.44      0.61        16\n",
      "         159       0.79      1.00      0.88        41\n",
      "         160       1.00      0.71      0.83         7\n",
      "         161       0.90      1.00      0.95         9\n",
      "         162       0.85      0.52      0.65        21\n",
      "         163       1.00      1.00      1.00         6\n",
      "         164       1.00      0.20      0.33         5\n",
      "         165       1.00      1.00      1.00        13\n",
      "         166       0.83      0.83      0.83         6\n",
      "         167       0.57      0.73      0.64        11\n",
      "         168       0.00      0.00      0.00         3\n",
      "         169       1.00      0.25      0.40         4\n",
      "         170       1.00      0.33      0.50         3\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       1.00      1.00      1.00         3\n",
      "         173       1.00      1.00      1.00        23\n",
      "         174       1.00      1.00      1.00         6\n",
      "         175       1.00      1.00      1.00        25\n",
      "         176       0.64      0.58      0.61        12\n",
      "         177       0.75      0.60      0.67         5\n",
      "         178       1.00      0.67      0.80         3\n",
      "         179       1.00      0.57      0.73         7\n",
      "         180       1.00      0.89      0.94         9\n",
      "         181       0.00      0.00      0.00         4\n",
      "         182       0.80      1.00      0.89         4\n",
      "         183       0.78      0.50      0.61        36\n",
      "         184       0.91      0.91      0.91        11\n",
      "         185       1.00      0.83      0.91         6\n",
      "         186       0.00      0.00      0.00         6\n",
      "         187       0.92      1.00      0.96        11\n",
      "         188       0.90      1.00      0.95         9\n",
      "         189       0.92      0.76      0.83        46\n",
      "         190       0.64      0.95      0.76        37\n",
      "         191       0.81      0.99      0.89        69\n",
      "         192       0.73      0.92      0.81        12\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       1.00      0.83      0.91         6\n",
      "         196       0.86      1.00      0.92         6\n",
      "         198       0.94      0.91      0.93        34\n",
      "         199       1.00      1.00      1.00        17\n",
      "         200       1.00      1.00      1.00         6\n",
      "         201       1.00      1.00      1.00        21\n",
      "         202       1.00      0.67      0.80         3\n",
      "         203       1.00      0.78      0.88         9\n",
      "         204       1.00      1.00      1.00         7\n",
      "         205       0.82      1.00      0.90        14\n",
      "         206       0.86      0.91      0.88        65\n",
      "         207       1.00      1.00      1.00         4\n",
      "         208       1.00      0.33      0.50         3\n",
      "         209       1.00      1.00      1.00         5\n",
      "         210       0.00      0.00      0.00         1\n",
      "         211       0.96      0.96      0.96        91\n",
      "         212       0.97      0.89      0.93        38\n",
      "         213       0.67      0.50      0.57         4\n",
      "         214       1.00      1.00      1.00         5\n",
      "         215       0.75      0.86      0.80         7\n",
      "         216       1.00      0.50      0.67         4\n",
      "         217       1.00      1.00      1.00         1\n",
      "         218       1.00      1.00      1.00        47\n",
      "         219       0.98      1.00      0.99       110\n",
      "         220       1.00      0.89      0.94        19\n",
      "         221       0.62      0.83      0.71        12\n",
      "         222       0.90      0.95      0.92        19\n",
      "         223       1.00      1.00      1.00        19\n",
      "         224       1.00      1.00      1.00         2\n",
      "         226       1.00      1.00      1.00        11\n",
      "         227       0.00      0.00      0.00         1\n",
      "         228       0.96      0.83      0.89        29\n",
      "         229       0.00      0.00      0.00         5\n",
      "         230       0.80      1.00      0.89         4\n",
      "         231       1.00      1.00      1.00        10\n",
      "         232       1.00      1.00      1.00        28\n",
      "         233       1.00      0.88      0.93         8\n",
      "         234       0.64      0.90      0.75        10\n",
      "         235       1.00      0.33      0.50         6\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       0.00      0.00      0.00         9\n",
      "         238       0.88      0.85      0.87        27\n",
      "         239       0.52      0.87      0.65        15\n",
      "         240       0.79      1.00      0.88        23\n",
      "         241       1.00      1.00      1.00         5\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.69      0.93      0.79        29\n",
      "         244       1.00      0.33      0.50         3\n",
      "         245       0.00      0.00      0.00         4\n",
      "         246       0.00      0.00      0.00         5\n",
      "         247       0.90      0.82      0.86        11\n",
      "         248       0.67      1.00      0.80         2\n",
      "         249       1.00      0.56      0.71         9\n",
      "         250       1.00      1.00      1.00         1\n",
      "         251       1.00      1.00      1.00         2\n",
      "         252       0.96      1.00      0.98        51\n",
      "         253       1.00      0.88      0.93        16\n",
      "         254       0.00      0.00      0.00         2\n",
      "         255       1.00      1.00      1.00        18\n",
      "         256       1.00      0.33      0.50        12\n",
      "         257       0.80      1.00      0.89        20\n",
      "         258       1.00      0.40      0.57        10\n",
      "         259       0.87      0.87      0.87        15\n",
      "         260       0.70      1.00      0.82        16\n",
      "         261       0.67      0.40      0.50         5\n",
      "         262       1.00      0.33      0.50         3\n",
      "         263       1.00      0.83      0.91         6\n",
      "         264       0.00      0.00      0.00        10\n",
      "         265       0.83      1.00      0.91         5\n",
      "         266       0.80      0.50      0.62         8\n",
      "         267       0.25      0.25      0.25         4\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.12      0.50      0.20         2\n",
      "         270       1.00      0.83      0.91        12\n",
      "         271       0.59      0.77      0.67        13\n",
      "         272       1.00      0.75      0.86         4\n",
      "         273       1.00      0.90      0.95        10\n",
      "         274       1.00      0.88      0.93        16\n",
      "         275       0.81      1.00      0.90        13\n",
      "         276       0.79      1.00      0.88        23\n",
      "         277       0.85      0.94      0.89        36\n",
      "         278       0.95      0.97      0.96        91\n",
      "         279       1.00      1.00      1.00        11\n",
      "         280       1.00      1.00      1.00        29\n",
      "         281       0.93      0.88      0.90        16\n",
      "         282       0.00      0.00      0.00         3\n",
      "         283       0.76      0.94      0.84        17\n",
      "         284       0.92      0.95      0.94        61\n",
      "         285       1.00      0.18      0.31        11\n",
      "         286       0.00      0.00      0.00         8\n",
      "         287       1.00      0.80      0.89         5\n",
      "         288       0.71      0.77      0.74        13\n",
      "         289       0.88      1.00      0.94        22\n",
      "         290       0.82      1.00      0.90        28\n",
      "         291       0.44      1.00      0.62         4\n",
      "         292       0.77      1.00      0.87        10\n",
      "         293       1.00      0.67      0.80         3\n",
      "         294       1.00      1.00      1.00         1\n",
      "         295       0.89      0.89      0.89         9\n",
      "         296       1.00      1.00      1.00         2\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       1.00      1.00      1.00        10\n",
      "         299       1.00      1.00      1.00         5\n",
      "         300       1.00      1.00      1.00        34\n",
      "         301       0.00      0.00      0.00         4\n",
      "         302       1.00      1.00      1.00        15\n",
      "         303       0.00      0.00      0.00         2\n",
      "         304       0.89      0.71      0.79        24\n",
      "         305       0.82      1.00      0.90         9\n",
      "         306       1.00      0.80      0.89         5\n",
      "         307       0.90      1.00      0.95        19\n",
      "         308       0.73      1.00      0.84         8\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       1.00      1.00      1.00         3\n",
      "         311       0.92      1.00      0.96       358\n",
      "         312       1.00      0.62      0.76        13\n",
      "         313       0.67      1.00      0.80         2\n",
      "         314       0.97      0.97      0.97      4512\n",
      "         315       0.68      0.52      0.59       300\n",
      "\n",
      "    accuracy                           0.92     10009\n",
      "   macro avg       0.74      0.73      0.71     10009\n",
      "weighted avg       0.91      0.92      0.91     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 4\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_4 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_3.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_4.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_4 = model_4.fit(X_train_combined_3, y_train_3, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_3, y_test_3),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_4, 'model_4_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_4, accuracy_4 = model_4.evaluate(X_test_combined_3, y_test_3)\n",
    "print(\"Accuracy for Stage 4 model:\", accuracy_4)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_4 = model_4.predict(X_test_combined_3)\n",
    "y_pred_4 = np.argmax(y_pred_probabilities_4, axis=1)\n",
    "report_4 = classification_report(y_test_3, y_pred_4)\n",
    "print(\"Classification Report for Stage 4 model:\")\n",
    "print(report_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 5 model...\n",
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.5513 - accuracy: 0.7560 - val_loss: 0.6146 - val_accuracy: 0.8753 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6798 - accuracy: 0.8590 - val_loss: 0.4545 - val_accuracy: 0.8861 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5438 - accuracy: 0.8747 - val_loss: 0.3272 - val_accuracy: 0.9101 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4635 - accuracy: 0.8846 - val_loss: 0.2750 - val_accuracy: 0.9254 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4108 - accuracy: 0.8932 - val_loss: 0.2406 - val_accuracy: 0.9251 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3656 - accuracy: 0.9000 - val_loss: 0.2381 - val_accuracy: 0.9328 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3448 - accuracy: 0.9055 - val_loss: 0.2138 - val_accuracy: 0.9377 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3296 - accuracy: 0.9081 - val_loss: 0.2063 - val_accuracy: 0.9366 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3027 - accuracy: 0.9118 - val_loss: 0.1957 - val_accuracy: 0.9401 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2800 - accuracy: 0.9189 - val_loss: 0.1862 - val_accuracy: 0.9425 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2724 - accuracy: 0.9200 - val_loss: 0.1855 - val_accuracy: 0.9433 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2650 - accuracy: 0.9219 - val_loss: 0.1788 - val_accuracy: 0.9447 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2586 - accuracy: 0.9242 - val_loss: 0.1814 - val_accuracy: 0.9461 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2567 - accuracy: 0.9250 - val_loss: 0.1757 - val_accuracy: 0.9469 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2470 - accuracy: 0.9262 - val_loss: 0.1739 - val_accuracy: 0.9477 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2433 - accuracy: 0.9267 - val_loss: 0.1727 - val_accuracy: 0.9479 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2407 - accuracy: 0.9278 - val_loss: 0.1718 - val_accuracy: 0.9484 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2378 - accuracy: 0.9290 - val_loss: 0.1716 - val_accuracy: 0.9498 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2352 - accuracy: 0.9301 - val_loss: 0.1707 - val_accuracy: 0.9493 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2357 - accuracy: 0.9283 - val_loss: 0.1714 - val_accuracy: 0.9491 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.2291 - accuracy: 0.9312 - val_loss: 0.1716 - val_accuracy: 0.9493 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2346 - accuracy: 0.9289 - val_loss: 0.1724 - val_accuracy: 0.9494 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2336 - accuracy: 0.9307 - val_loss: 0.1712 - val_accuracy: 0.9494 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2301 - accuracy: 0.9319 - val_loss: 0.1708 - val_accuracy: 0.9494 - lr: 2.2922e-06\n",
      " 40/313 [==>...........................] - ETA: 0s - loss: 0.1354 - accuracy: 0.9547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_12788\\646761526.py:40: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_5, 'model_5_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9493\n",
      "Accuracy for Stage 5 model: 0.949345588684082\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 5 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        13\n",
      "           1       0.94      0.94      0.94        18\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.50      0.67         4\n",
      "           4       1.00      0.80      0.89         5\n",
      "           5       0.78      1.00      0.88        71\n",
      "           6       1.00      0.60      0.75         5\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       1.00      1.00      1.00         1\n",
      "          10       0.93      0.78      0.85        32\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.75      1.00      0.86         6\n",
      "          13       0.71      0.56      0.62         9\n",
      "          14       0.00      0.00      0.00         4\n",
      "          15       0.59      1.00      0.74        10\n",
      "          16       0.57      0.71      0.63        17\n",
      "          17       0.77      0.91      0.83        65\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.55      1.00      0.71         6\n",
      "          20       0.75      0.60      0.67         5\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       1.00      0.92      0.96        13\n",
      "          23       1.00      0.50      0.67         2\n",
      "          24       0.78      1.00      0.88         7\n",
      "          25       0.50      0.86      0.63        14\n",
      "          26       0.90      0.98      0.93        88\n",
      "          27       0.50      1.00      0.67         1\n",
      "          28       0.00      0.00      0.00         8\n",
      "          29       1.00      1.00      1.00         3\n",
      "          31       0.75      0.86      0.80         7\n",
      "          32       1.00      0.73      0.84        11\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      1.00      1.00        14\n",
      "          35       0.99      0.99      0.99       140\n",
      "          36       0.63      0.92      0.75        13\n",
      "          37       0.81      0.93      0.87        28\n",
      "          38       0.67      0.31      0.42        13\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.50      0.67         4\n",
      "          41       0.54      1.00      0.70         7\n",
      "          42       0.62      1.00      0.77         5\n",
      "          43       0.00      0.00      0.00         3\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00         9\n",
      "          46       0.84      1.00      0.91        16\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       1.00      1.00      1.00        54\n",
      "          49       0.00      0.00      0.00         6\n",
      "          50       0.86      1.00      0.92         6\n",
      "          51       0.00      0.00      0.00         5\n",
      "          52       1.00      1.00      1.00         9\n",
      "          53       0.87      1.00      0.93        13\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       0.75      0.60      0.67         5\n",
      "          57       1.00      0.50      0.67         2\n",
      "          58       0.51      1.00      0.68        19\n",
      "          59       0.67      1.00      0.80         4\n",
      "          60       0.60      0.60      0.60         5\n",
      "          61       0.47      0.88      0.61         8\n",
      "          62       0.96      1.00      0.98        54\n",
      "          63       1.00      0.33      0.50         3\n",
      "          64       0.90      0.75      0.82        12\n",
      "          65       1.00      0.30      0.46        10\n",
      "          66       0.48      1.00      0.65        11\n",
      "          67       0.50      0.17      0.25         6\n",
      "          68       0.83      0.62      0.71         8\n",
      "          69       0.14      0.50      0.22         2\n",
      "          70       0.50      0.25      0.33         4\n",
      "          71       0.00      0.00      0.00         6\n",
      "          72       0.75      1.00      0.86         6\n",
      "          73       0.76      0.86      0.81        22\n",
      "          74       0.73      0.69      0.71        16\n",
      "          75       1.00      0.33      0.50         3\n",
      "          76       0.71      0.67      0.69        15\n",
      "          77       0.56      1.00      0.71         5\n",
      "          78       0.00      0.00      0.00         7\n",
      "          80       0.00      0.00      0.00         4\n",
      "          81       0.90      1.00      0.95         9\n",
      "          82       1.00      0.94      0.97        17\n",
      "          83       1.00      1.00      1.00        10\n",
      "          84       1.00      1.00      1.00         6\n",
      "          85       0.50      0.40      0.44         5\n",
      "          86       0.00      0.00      0.00         9\n",
      "          87       0.91      1.00      0.95        30\n",
      "          88       1.00      0.67      0.80         3\n",
      "          89       0.75      1.00      0.86         9\n",
      "          90       0.41      0.64      0.50        11\n",
      "          91       1.00      1.00      1.00         1\n",
      "          92       0.00      0.00      0.00         1\n",
      "          93       0.67      1.00      0.80        12\n",
      "          94       0.00      0.00      0.00         5\n",
      "          95       0.60      0.43      0.50         7\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       1.00      1.00      1.00         2\n",
      "          98       0.20      0.33      0.25         3\n",
      "          99       1.00      1.00      1.00        13\n",
      "         100       0.91      1.00      0.95        10\n",
      "         101       0.69      1.00      0.82         9\n",
      "         102       1.00      0.50      0.67         4\n",
      "         103       0.00      0.00      0.00         2\n",
      "         104       0.67      1.00      0.80         2\n",
      "         105       0.86      1.00      0.92         6\n",
      "         106       0.94      1.00      0.97        15\n",
      "         107       1.00      0.33      0.50         3\n",
      "         108       0.91      1.00      0.95        10\n",
      "         109       0.38      1.00      0.55        20\n",
      "         110       0.89      1.00      0.94         8\n",
      "         111       1.00      1.00      1.00        29\n",
      "         112       0.00      0.00      0.00         2\n",
      "         113       0.67      0.67      0.67         6\n",
      "         114       0.76      1.00      0.87        13\n",
      "         115       0.00      0.00      0.00        14\n",
      "         116       0.50      0.93      0.65        15\n",
      "         117       1.00      0.11      0.20        18\n",
      "         118       1.00      0.80      0.89         5\n",
      "         119       0.89      1.00      0.94        42\n",
      "         120       0.33      0.50      0.40         2\n",
      "         121       1.00      0.25      0.40         4\n",
      "         122       1.00      0.33      0.50         3\n",
      "         123       1.00      0.67      0.80         3\n",
      "         124       0.00      0.00      0.00         1\n",
      "         125       0.80      1.00      0.89         4\n",
      "         126       1.00      1.00      1.00         5\n",
      "         127       1.00      1.00      1.00         6\n",
      "         128       0.00      0.00      0.00         6\n",
      "         129       1.00      0.50      0.67         4\n",
      "         130       0.00      0.00      0.00         2\n",
      "         131       1.00      0.50      0.67         4\n",
      "         132       0.75      0.38      0.50         8\n",
      "         133       0.00      0.00      0.00         5\n",
      "         134       0.47      1.00      0.64        14\n",
      "         135       1.00      0.67      0.80         3\n",
      "         136       1.00      1.00      1.00         3\n",
      "         137       1.00      1.00      1.00         3\n",
      "         138       0.50      0.10      0.17        10\n",
      "         139       0.00      0.00      0.00         1\n",
      "         140       0.58      0.78      0.67         9\n",
      "         141       0.50      0.50      0.50         4\n",
      "         142       0.47      1.00      0.64         7\n",
      "         143       0.70      0.84      0.76        19\n",
      "         144       0.85      1.00      0.92        40\n",
      "         145       0.00      0.00      0.00         5\n",
      "         146       0.87      0.93      0.90        14\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.78      1.00      0.88         7\n",
      "         149       1.00      1.00      1.00         6\n",
      "         150       0.50      0.75      0.60         8\n",
      "         151       0.00      0.00      0.00         8\n",
      "         152       1.00      0.67      0.80         6\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.42      1.00      0.59        13\n",
      "         155       0.00      0.00      0.00         2\n",
      "         156       0.00      0.00      0.00         3\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.79      0.78      0.78        40\n",
      "         159       0.95      0.69      0.80        26\n",
      "         160       0.99      0.99      0.99      8119\n",
      "         161       0.77      0.53      0.63       106\n",
      "\n",
      "    accuracy                           0.95     10009\n",
      "   macro avg       0.62      0.62      0.59     10009\n",
      "weighted avg       0.94      0.95      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 5\n",
    "print(\"Running Stage 5 model...\")\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "# Create model\n",
    "model_5 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_4.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_5 = model_5.fit(X_train_combined_4, y_train_4, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_4, y_test_4),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_5, 'model_5_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_5, accuracy_5 = model_5.evaluate(X_test_combined_4, y_test_4)\n",
    "print(\"Accuracy for Stage 5 model:\", accuracy_5)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_5 = model_5.predict(X_test_combined_4)\n",
    "y_pred_5 = np.argmax(y_pred_probabilities_5, axis=1)\n",
    "report_5 = classification_report(y_test_4, y_pred_5)\n",
    "print(\"Classification Report for Stage 5 model:\")\n",
    "print(report_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_1_preberttune.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_2_preberttune.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m user_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[name, description, price, \u001b[38;5;28mtype\u001b[39m, manufacturer]]) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "model_2 = load_model('model_2_preberttune.h5')\n",
    "\n",
    "\n",
    "user_input = np.array([[name, description, price, type, manufacturer]]) \n",
    "#Se le tiene que aplicar las funciones de normalizacion al input del usuario texto(name,desccription), usar scale para precio y one hot para categoricas.\n",
    "#usar label encoder para target category\n",
    "target_1 = [parent_category]\n",
    "\n",
    "#Preprocessed user input\n",
    "#                                                          Usar las categorias directo del df?\n",
    "#Main category pred\n",
    "main_category_pred_probs = model_1.predict(user_input)\n",
    "\n",
    "# get labels for main category\n",
    "main_category_pred_labels = np.argmax(main_category_pred_probs, axis=1) #Houseware [0.9,0.8,0.5,0.4]----> Houseware,0\n",
    "\n",
    "#Concat user input with label  and preprocess to enter second model\n",
    "#se tiene que aplicar one hot al output del primer modelo y usarlo como feature en el segundo, \n",
    "#se puede usar el output preprocesado del primero y solo unir con el one hot\n",
    "user_input_with_pred = np.concatenate((user_input, main_category_pred_labels.reshape(-1, 1)), axis=1) \n",
    "\n",
    "#Predict subcategory using model_2 with the concatenated input\n",
    "subcategory_pred_probs = model_2.predict(user_input_with_pred) #vector\n",
    "\n",
    "# Getting index\n",
    "subcategory_pred_labels = np.argmax(subcategory_pred_probs, axis=1) # indice del valor mas alto\n",
    "\n",
    "# Print the subcategory prediction labels\n",
    "print(subcategory_pred_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
