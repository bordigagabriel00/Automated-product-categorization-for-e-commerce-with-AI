{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjKToYfc6adY",
    "outputId": "e010d05c-aab8-4947-c72f-255032e61ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\2213573283.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lASrrK4Z6msI",
    "outputId": "a79cb80c-5c32-4ecc-82b2-1689a695c3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50041 entries, 0 to 50040\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   name             50040 non-null  object \n",
      " 1   type             50041 non-null  object \n",
      " 2   price            50041 non-null  float64\n",
      " 3   description      50041 non-null  object \n",
      " 4   manufacturer     49975 non-null  object \n",
      " 5   url              50040 non-null  object \n",
      " 6   parent_category  50041 non-null  object \n",
      " 7   sub_category_1   49294 non-null  object \n",
      " 8   sub_category_2   43948 non-null  object \n",
      " 9   sub_category_3   27955 non-null  object \n",
      " 10  sub_category_4   9788 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\assignment\\alpha2_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(pd.NA)\n",
    "\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OUzBSTQi6p8v"
   },
   "outputs": [],
   "source": [
    "# stemmer, lemmatizer and stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import Optional\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8martjTt6sdd",
    "outputId": "04595e57-b78b-4702-9291-3d7251bcc274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50041, 13)\n"
     ]
    }
   ],
   "source": [
    "normalization = ['name', 'description']\n",
    "for column in normalization:\n",
    "    df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HYTfY_RArpEX"
   },
   "outputs": [],
   "source": [
    "#df['sub_category_1'].fillna('0', inplace=True)\n",
    "#df['sub_category_2'].fillna('0', inplace=True)\n",
    "#df['sub_category_3'].fillna('0', inplace=True)\n",
    "#df['sub_category_4'].fillna('0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwLSa6uB6upR",
    "outputId": "3b01d80e-984e-4e8f-ffb0-73b48cd31694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape\n",
    "X = df.drop(columns=['parent_category'])\n",
    "y = df['parent_category']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "woBvshjI-gHS"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "X_1 = df.drop(columns=['sub_category_1'])\n",
    "y_1 = df['sub_category_1']\n",
    "y_1.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wYMN3-Aj-f-n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_2 = df.drop(columns=['sub_category_2'])\n",
    "y_2 = df['sub_category_2']\n",
    "y_2.fillna('missing', inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_2_encoded = label_encoder.fit_transform(y_2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sLrnJPS1-f3n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_3 = df.drop(columns=['sub_category_3'])\n",
    "y_3 = df['sub_category_3']\n",
    "y_3.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_3_encoded = label_encoder.fit_transform(y_3)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MmO58b37-foH"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_4 = df.drop(columns=['sub_category_4'])\n",
    "y_4 = df['sub_category_4']\n",
    "y_4.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_4_encoded = label_encoder.fit_transform(y_4)\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4, y_4_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxJvoJcBsGfp",
    "outputId": "8064db2d-ac1c-4896-9349-64285425f5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50041, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_ArCp4U6xB_",
    "outputId": "d5cd1931-8036-4eea-de8f-8f3daa08bcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40032, 12)\n",
      "(10009, 12)\n",
      "(40032,)\n",
      "(10009,)\n",
      "9233                            star fox preowned nintendo\n",
      "25631    pioneer networkready ultra hd passthrough av h...\n",
      "19030                     evolve ultimate edition xbox one\n",
      "12044    joby pro series ultraplate quickrelease plate ...\n",
      "18967    aluratek bump w home audio speaker system ipod...\n",
      "                               ...                        \n",
      "11284    samsung class diag lead curved smart ultra hd ...\n",
      "44732    hifonics brutus class mono mosfet subwoofer am...\n",
      "38158    mobile edge premium laptop backpack apple macb...\n",
      "860                                 presonus presonus gray\n",
      "15795      insignia portable bluetooth stereo speaker blue\n",
      "Name: name_normalized, Length: 40032, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train['name_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXktfLf6y1M",
    "outputId": "4b9e8a29-b99b-4ab7-f153-2346eeee9663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55,  5, 55, ..., 16, 43,  5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdo3ct0S6zga"
   },
   "source": [
    "One Hot Encoder y Scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMYODyT5cLA"
   },
   "source": [
    "Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHamdvAY64SL",
    "outputId": "c312cd27-8ae2-4f37-bdde-f90f6b6e38ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded: (40032, 2195)\n",
      "Data type of X_train_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded: float64\n",
      "Shape of X_test_encoded: (10009, 2195)\n",
      "Data type of X_test_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded: float64\n",
      "Shape of X_train_scaled: (40032, 1)\n",
      "Data type of elements in X_train_scaled: float64\n",
      "Shape of X_test_scaled: (10009, 1)\n",
      "Data type of elements in X_test_scaled: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "numerical_columns = ['price']\n",
    "text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Data type of elements in X_train_scaled:\", X_train_scaled.dtype)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14kj_745e3A"
   },
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rrKOHZ_4_AK",
    "outputId": "bd1b3f38-75f9-494e-dbf6-9bd87d1f6832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_1: (40032, 2253)\n",
      "Data type of X_train_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_1: float64\n",
      "Shape of X_test_encoded_1: (10009, 2253)\n",
      "Data type of X_test_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_1: float64\n",
      "Shape of X_train_scaled_1: (40032, 1)\n",
      "Data type of elements in X_train_scaled_1: float64\n",
      "Shape of X_test_scaled_1: (10009, 1)\n",
      "Data type of elements in X_test_scaled_1: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_1 = ['type', 'manufacturer', 'parent_category']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_1[categorical_columns_1] = X_train_1[categorical_columns_1].fillna('missing')\n",
    "X_test_1[categorical_columns_1] = X_test_1[categorical_columns_1].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_1 = encoder_1.fit_transform(X_train_1[categorical_columns_1])\n",
    "X_test_encoded_1 = encoder_1.transform(X_test_1[categorical_columns_1])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_1:\", X_train_encoded_1.shape)\n",
    "print(\"Data type of X_train_encoded_1:\", type(X_train_encoded_1))\n",
    "print(\"Data type of elements in X_train_encoded_1:\", X_train_encoded_1.dtype)\n",
    "print(\"Shape of X_test_encoded_1:\", X_test_encoded_1.shape)\n",
    "print(\"Data type of X_test_encoded_1:\", type(X_test_encoded_1))\n",
    "print(\"Data type of elements in X_test_encoded_1:\", X_test_encoded_1.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled_1 = scaler_1.fit_transform(X_train_1[numerical_columns])\n",
    "X_test_scaled_1 = scaler_1.transform(X_test_1[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_1:\", X_train_scaled_1.shape)\n",
    "print(\"Data type of elements in X_train_scaled_1:\", X_train_scaled_1.dtype)\n",
    "print(\"Shape of X_test_scaled_1:\", X_test_scaled_1.shape)\n",
    "print(\"Data type of elements in X_test_scaled_1:\", X_test_scaled_1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4uI4css5hqx"
   },
   "source": [
    "Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSlE-ZXJ4-5Q",
    "outputId": "cb5f926f-438b-42f9-ab8b-6c1f9f7ba5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_2: (40032, 2398)\n",
      "Data type of X_train_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_2: float64\n",
      "Shape of X_test_encoded_2: (10009, 2398)\n",
      "Data type of X_test_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_2: float64\n",
      "Shape of X_train_scaled_2: (40032, 1)\n",
      "Data type of elements in X_train_scaled_2: float64\n",
      "Shape of X_test_scaled_2: (10009, 1)\n",
      "Data type of elements in X_test_scaled_2: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_2 = ['type', 'manufacturer', 'parent_category', 'sub_category_1']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_2[categorical_columns_2] = X_train_2[categorical_columns_2].fillna('missing')\n",
    "X_test_2[categorical_columns_2] = X_test_2[categorical_columns_2].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_2 = encoder_2.fit_transform(X_train_2[categorical_columns_2])\n",
    "X_test_encoded_2 = encoder_2.transform(X_test_2[categorical_columns_2])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_2:\", X_train_encoded_2.shape)\n",
    "print(\"Data type of X_train_encoded_2:\", type(X_train_encoded_2))\n",
    "print(\"Data type of elements in X_train_encoded_2:\", X_train_encoded_2.dtype)\n",
    "print(\"Shape of X_test_encoded_2:\", X_test_encoded_2.shape)\n",
    "print(\"Data type of X_test_encoded_2:\", type(X_test_encoded_2))\n",
    "print(\"Data type of elements in X_test_encoded_2:\", X_test_encoded_2.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scaled_2 = scaler_2.fit_transform(X_train_2[numerical_columns])\n",
    "X_test_scaled_2 = scaler_2.transform(X_test_2[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_2:\", X_train_scaled_2.shape)\n",
    "print(\"Data type of elements in X_train_scaled_2:\", X_train_scaled_2.dtype)\n",
    "print(\"Shape of X_test_scaled_2:\", X_test_scaled_2.shape)\n",
    "print(\"Data type of elements in X_test_scaled_2:\", X_test_scaled_2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjEeJaua5jYm"
   },
   "source": [
    "Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UWrFjTJ4-y7",
    "outputId": "14c5078c-21d3-4614-8393-a3daa5372ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_3: (40032, 2943)\n",
      "Data type of X_train_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_3: float64\n",
      "Shape of X_test_encoded_3: (10009, 2943)\n",
      "Data type of X_test_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_3: float64\n",
      "Shape of X_train_scaled_3: (40032, 1)\n",
      "Data type of elements in X_train_scaled_3: float64\n",
      "Shape of X_test_scaled_3: (10009, 1)\n",
      "Data type of elements in X_test_scaled_3: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_3 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_3[categorical_columns_3] = X_train_3[categorical_columns_3].fillna('missing')\n",
    "X_test_3[categorical_columns_3] = X_test_3[categorical_columns_3].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_3 = encoder_3.fit_transform(X_train_3[categorical_columns_3])\n",
    "X_test_encoded_3 = encoder_3.transform(X_test_3[categorical_columns_3])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_3:\", X_train_encoded_3.shape)\n",
    "print(\"Data type of X_train_encoded_3:\", type(X_train_encoded_3))\n",
    "print(\"Data type of elements in X_train_encoded_3:\", X_train_encoded_3.dtype)\n",
    "print(\"Shape of X_test_encoded_3:\", X_test_encoded_3.shape)\n",
    "print(\"Data type of X_test_encoded_3:\", type(X_test_encoded_3))\n",
    "print(\"Data type of elements in X_test_encoded_3:\", X_test_encoded_3.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_3 = StandardScaler()\n",
    "X_train_scaled_3 = scaler_3.fit_transform(X_train_3[numerical_columns])\n",
    "X_test_scaled_3 = scaler_3.transform(X_test_3[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_3:\", X_train_scaled_3.shape)\n",
    "print(\"Data type of elements in X_train_scaled_3:\", X_train_scaled_3.dtype)\n",
    "print(\"Shape of X_test_scaled_3:\", X_test_scaled_3.shape)\n",
    "print(\"Data type of elements in X_test_scaled_3:\", X_test_scaled_3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX9LC3rA5k-T"
   },
   "source": [
    "Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E-BaTiE4-rh",
    "outputId": "0fc98aa5-895c-4d65-e0c7-d19d5a80f750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_4: (40032, 3632)\n",
      "Data type of X_train_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_4: float64\n",
      "Shape of X_test_encoded_4: (10009, 3632)\n",
      "Data type of X_test_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_4: float64\n",
      "Shape of X_train_scaled_4: (40032, 1)\n",
      "Data type of elements in X_train_scaled_4: float64\n",
      "Shape of X_test_scaled_4: (10009, 1)\n",
      "Data type of elements in X_test_scaled_4: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_4 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2', 'sub_category_3']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_4[categorical_columns_4] = X_train_4[categorical_columns_4].fillna('missing')\n",
    "X_test_4[categorical_columns_4] = X_test_4[categorical_columns_4].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_4 = encoder_4.fit_transform(X_train_4[categorical_columns_4])\n",
    "X_test_encoded_4 = encoder_4.transform(X_test_4[categorical_columns_4])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_4:\", X_train_encoded_4.shape)\n",
    "print(\"Data type of X_train_encoded_4:\", type(X_train_encoded_4))\n",
    "print(\"Data type of elements in X_train_encoded_4:\", X_train_encoded_4.dtype)\n",
    "print(\"Shape of X_test_encoded_4:\", X_test_encoded_4.shape)\n",
    "print(\"Data type of X_test_encoded_4:\", type(X_test_encoded_4))\n",
    "print(\"Data type of elements in X_test_encoded_4:\", X_test_encoded_4.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_4 = StandardScaler()\n",
    "X_train_scaled_4 = scaler_4.fit_transform(X_train_4[numerical_columns])\n",
    "X_test_scaled_4 = scaler_4.transform(X_test_4[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_4:\", X_train_scaled_4.shape)\n",
    "print(\"Data type of elements in X_train_scaled_4:\", X_train_scaled_4.dtype)\n",
    "print(\"Shape of X_test_scaled_4:\", X_test_scaled_4.shape)\n",
    "print(\"Data type of elements in X_test_scaled_4:\", X_test_scaled_4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CgYBDhCwHKQA"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Stage 1\n",
    "X_train_processed = hstack([X_train_encoded, X_train_scaled]).astype(np.float32).toarray()\n",
    "X_test_processed = hstack([X_test_encoded, X_test_scaled]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 2\n",
    "X_train_processed_1 = hstack([X_train_encoded_1, X_train_scaled_1]).astype(np.float32).toarray()\n",
    "X_test_processed_1 = hstack([X_test_encoded_1, X_test_scaled_1]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 3\n",
    "X_train_processed_2 = hstack([X_train_encoded_2, X_train_scaled_2]).astype(np.float32).toarray()\n",
    "X_test_processed_2 = hstack([X_test_encoded_2, X_test_scaled_2]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 4\n",
    "X_train_processed_3 = hstack([X_train_encoded_3, X_train_scaled_3]).astype(np.float32).toarray()\n",
    "X_test_processed_3 = hstack([X_test_encoded_3, X_test_scaled_3]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 5\n",
    "X_train_processed_4 = hstack([X_train_encoded_4, X_train_scaled_4]).astype(np.float32).toarray()\n",
    "X_test_processed_4 = hstack([X_test_encoded_4, X_test_scaled_4]).astype(np.float32).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvYoUCeHHjx_",
    "outputId": "61b6f3c1-d6bf-4b4a-fcd5-cc6f7e54ab90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed: float32\n",
      "Data type of X_test_processed: float32\n",
      "X_train_processed shape: (40032, 2196)\n",
      "X_test_processed shape: (10009, 2196)\n"
     ]
    }
   ],
   "source": [
    "# Dim 1\n",
    "print(\"Data type of X_train_processed:\", X_train_processed.dtype)\n",
    "print(\"Data type of X_test_processed:\", X_test_processed.dtype)\n",
    "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhfSkN9I1vK",
    "outputId": "180da8c5-683c-4361-98ce-f51e50cd01ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_1: float32\n",
      "Data type of X_test_processed_1: float32\n",
      "X_train_processed_1 shape: (40032, 2254)\n",
      "X_test_processed_1 shape: (10009, 2254)\n"
     ]
    }
   ],
   "source": [
    "# Dim 2\n",
    "print(\"Data type of X_train_processed_1:\", X_train_processed_1.dtype)\n",
    "print(\"Data type of X_test_processed_1:\", X_test_processed_1.dtype)\n",
    "print(\"X_train_processed_1 shape:\", X_train_processed_1.shape)\n",
    "print(\"X_test_processed_1 shape:\", X_test_processed_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Az3OeCXI1rI",
    "outputId": "3b70f69e-056d-4351-9485-bd9313c99b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_2: float32\n",
      "Data type of X_test_processed_2: float32\n",
      "X_train_processed_2 shape: (40032, 2399)\n",
      "X_test_processed_2 shape: (10009, 2399)\n"
     ]
    }
   ],
   "source": [
    "# Dim 3\n",
    "print(\"Data type of X_train_processed_2:\", X_train_processed_2.dtype)\n",
    "print(\"Data type of X_test_processed_2:\", X_test_processed_2.dtype)\n",
    "print(\"X_train_processed_2 shape:\", X_train_processed_2.shape)\n",
    "print(\"X_test_processed_2 shape:\", X_test_processed_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBSG8gbeI1ng",
    "outputId": "902e994e-05fa-4a26-997f-66ff75d4eef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_3: float32\n",
      "Data type of X_test_processed_3: float32\n",
      "X_train_processed_3 shape: (40032, 2944)\n",
      "X_test_processed_3 shape: (10009, 2944)\n"
     ]
    }
   ],
   "source": [
    "# Dim 4\n",
    "print(\"Data type of X_train_processed_3:\", X_train_processed_3.dtype)\n",
    "print(\"Data type of X_test_processed_3:\",X_test_processed_3.dtype)\n",
    "print(\"X_train_processed_3 shape:\", X_train_processed_3.shape)\n",
    "print(\"X_test_processed_3 shape:\", X_test_processed_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIkzAILmI5Iv",
    "outputId": "945746cf-2be2-42d4-f3d9-49007732e383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_4: float32\n",
      "Data type of X_test_processed_4: float32\n",
      "X_train_processed_4 shape: (40032, 3633)\n",
      "X_test_processed_4 shape: (10009, 3633)\n"
     ]
    }
   ],
   "source": [
    "# Dim 5\n",
    "print(\"Data type of X_train_processed_4:\", X_train_processed_4.dtype)\n",
    "print(\"Data type of X_test_processed_4:\", X_test_processed_4.dtype)\n",
    "print(\"X_train_processed_4 shape:\", X_train_processed_4.shape)\n",
    "print(\"X_test_processed_4 shape:\", X_test_processed_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define file paths to save the arrays\n",
    "file_paths = {\n",
    "    'X_train_processed.npy': X_train_processed,\n",
    "    'X_test_processed.npy': X_test_processed,\n",
    "    'X_train_processed_1.npy': X_train_processed_1,\n",
    "    'X_test_processed_1.npy': X_test_processed_1,\n",
    "    'X_train_processed_2.npy': X_train_processed_2,\n",
    "    'X_test_processed_2.npy': X_test_processed_2,\n",
    "    'X_train_processed_3.npy': X_train_processed_3,\n",
    "    'X_test_processed_3.npy': X_test_processed_3,\n",
    "    'X_train_processed_4.npy': X_train_processed_4,\n",
    "    'X_test_processed_4.npy': X_test_processed_4\n",
    "}\n",
    "\n",
    "# Save each array\n",
    "for file_name, array in file_paths.items():\n",
    "    np.save(file_name, array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EVfWlKtNKCOR"
   },
   "outputs": [],
   "source": [
    "X_train_name_embeddings_loaded = np.load('X_train_name_embeddings.npy', allow_pickle= True)\n",
    "X_train_description_embeddings_loaded = np.load('X_train_description_embeddings.npy',allow_pickle= True)\n",
    "X_test_name_embeddings_loaded = np.load('X_test_name_embeddings.npy', allow_pickle= True)\n",
    "X_test_description_embeddings_loaded = np.load('X_test_description_embeddings.npy', allow_pickle= True)\n",
    "\n",
    "# Load the saved NumPy arrays\n",
    "X_train_processed_loaded = np.load('X_train_processed.npy', allow_pickle=True)\n",
    "X_test_processed_loaded = np.load('X_test_processed.npy', allow_pickle=True)\n",
    "X_train_processed_1_loaded = np.load('X_train_processed_1.npy', allow_pickle=True)\n",
    "X_test_processed_1_loaded = np.load('X_test_processed_1.npy', allow_pickle=True)\n",
    "X_train_processed_2_loaded = np.load('X_train_processed_2.npy', allow_pickle=True)\n",
    "X_test_processed_2_loaded = np.load('X_test_processed_2.npy', allow_pickle=True)\n",
    "X_train_processed_3_loaded = np.load('X_train_processed_3.npy', allow_pickle=True)\n",
    "X_test_processed_3_loaded = np.load('X_test_processed_3.npy', allow_pickle=True)\n",
    "X_train_processed_4_loaded = np.load('X_train_processed_4.npy', allow_pickle=True)\n",
    "X_test_processed_4_loaded = np.load('X_test_processed_4.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name_last_hidden = np.array([x[0][-1] for x in X_train_name_embeddings_loaded])\n",
    "X_train_description_last_hidden = np.array([x[0][-1] for x in X_train_description_embeddings_loaded])\n",
    "X_test_name_last_hidden = np.array([x[0][-1] for x in X_test_name_embeddings_loaded])\n",
    "X_test_description_last_hidden = np.array([x[0][-1] for x in X_test_description_embeddings_loaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of X_train_name_last_hidden: [ 6.20537519e-01 -1.57028392e-01 -4.39105332e-01  5.87245941e-01\n",
      " -4.92567480e-01 -6.75922573e-01  3.66373003e-01 -7.91122556e-01\n",
      "  6.63730741e-01  4.76211309e-02  4.97472771e-02 -3.77824754e-01\n",
      " -5.29451743e-02 -3.52970883e-03 -7.33473897e-01 -2.74465412e-01\n",
      " -8.19092095e-02 -1.52533606e-01  7.26244077e-02 -4.86062840e-02\n",
      "  3.07591200e-01 -1.32735014e-01  8.47112298e-01  2.39423364e-01\n",
      "  1.34860486e-01  3.51377934e-01 -4.78440404e-01 -1.80202276e-02\n",
      " -2.26639926e-01 -3.52599382e-01 -6.21600211e-01 -2.67526209e-01\n",
      " -1.92636084e-02  3.99675339e-01  1.55551210e-01 -8.61436129e-02\n",
      "  3.39647025e-01 -4.60110493e-02 -5.71235001e-01 -4.14561182e-01\n",
      " -3.34018916e-01 -1.61173679e-02 -1.93653673e-01  6.32311583e-01\n",
      "  5.47741950e-02 -6.00057304e-01  6.22461319e-01  3.42181236e-01\n",
      " -2.25496083e-01  4.89310294e-01  2.93806463e-01  1.60992384e-01\n",
      " -7.31535181e-02  8.67539346e-02  5.76151237e-02  1.21880889e-01\n",
      "  2.94119507e-01 -4.34998453e-01  1.54902250e-01  2.93051004e-01\n",
      " -5.07322550e-02  6.77793384e-01 -2.17716694e-01 -1.94208890e-01\n",
      "  6.51268899e-01 -1.67188928e-01  2.17404403e-02 -3.18291545e-01\n",
      " -6.07384741e-01 -3.05744082e-01 -2.39384830e-01 -1.02852356e+00\n",
      "  6.55281067e-01  3.35646331e-01  3.62706840e-01  2.41505504e-01\n",
      " -3.31891775e-01  7.45274603e-01  1.18391871e-01  3.59835505e-01\n",
      "  3.76000077e-01 -3.06956947e-01 -1.38037503e-01  7.29502961e-02\n",
      "  5.02546012e-01  1.56718288e-02 -2.42989823e-01  4.20575216e-02\n",
      " -4.59762454e-01 -1.26091674e-01  3.46461594e-01  2.71966606e-01\n",
      "  3.90934289e-01 -3.59424323e-01 -1.19082555e-01  3.55039179e-01\n",
      " -6.93194717e-02 -9.02826637e-02 -2.01892227e-01 -1.14928253e-01\n",
      " -3.77589911e-01 -1.72685519e-01 -5.20075373e-02  8.87612224e-01\n",
      " -1.62285715e-01 -2.95471847e-01  4.46627766e-01  5.57538629e-01\n",
      "  2.56792873e-01  1.03553367e+00  5.43226182e-01  2.22238839e-01\n",
      "  1.73543707e-01 -6.79688826e-02 -4.89283442e-01 -5.52646935e-01\n",
      "  2.85861969e-01  2.83273697e-01  3.80976677e-01  2.35821024e-01\n",
      " -6.67967856e-01 -4.66061831e-01  5.99375069e-01  1.08154857e+00\n",
      " -1.25421882e-01 -3.18098664e-02  2.50165947e-02 -7.18430936e-01\n",
      "  9.47480053e-02 -7.66403854e-01 -3.98136169e-01  6.48566902e-01\n",
      "  2.57072091e-01  8.71164203e-01 -1.05473615e-01  2.04653174e-01\n",
      " -3.08098674e-01  5.59956357e-02 -6.77700162e-01  1.01367123e-01\n",
      " -3.72281134e-01  9.31154370e-01  6.59990549e-01 -1.15582418e+00\n",
      "  5.36541790e-02  6.25010610e-01  5.59506238e-01 -3.16040702e-02\n",
      "  4.84125316e-02 -3.06834847e-01  7.54427016e-01 -1.69089168e-01\n",
      " -2.54145682e-01 -2.30913237e-02 -5.35159945e-01  2.63796747e-01\n",
      " -1.52081802e-01  1.89791620e-02  4.29030150e-01  7.86521673e-01\n",
      "  2.92653799e-01  3.41052055e-01  3.65956217e-01  5.37699759e-01\n",
      " -5.17349124e-01  2.24996656e-01 -1.17838478e+00 -1.76467597e-01\n",
      "  1.01287454e-01  4.32819426e-01 -4.23639059e-01 -4.54228640e-01\n",
      "  7.73141310e-02  3.79181325e-01 -3.45093966e-01  3.52966428e-01\n",
      " -1.62603110e-01  8.23184326e-02 -2.84529865e-01 -6.81628287e-01\n",
      " -8.46690559e+00 -2.35476375e-01 -9.76541191e-02  2.22685993e-01\n",
      " -3.64512168e-02 -3.91305238e-01 -8.25014532e-01 -1.73412472e-01\n",
      "  3.74747545e-01 -8.75730038e-01 -1.22479826e-01 -1.77380100e-01\n",
      " -9.63426471e-01  8.22258413e-01  3.41257453e-01 -2.38749608e-01\n",
      "  2.57653482e-02  2.47886866e-01 -3.30268562e-01  3.26861292e-01\n",
      " -1.97851792e-01 -3.42524916e-01  9.84942913e-02  6.40032470e-01\n",
      " -2.42812037e-01 -1.43791389e+00  2.46120483e-01 -2.29564041e-01\n",
      "  5.11401772e-01  5.85837178e-02 -1.31213963e+00 -1.12423360e-01\n",
      " -3.62519175e-03 -7.86731243e-01  1.56666100e-01 -5.34660220e-01\n",
      "  4.07156013e-02 -5.64395070e-01 -9.06348109e-01 -2.95303822e-01\n",
      " -5.59946835e-01 -3.20167810e-01 -1.84398927e-02  2.15938147e-02\n",
      "  5.29996753e-01 -1.46843731e+00  3.76851022e-01  7.21755743e-01\n",
      "  6.36393428e-01  2.52907544e-01 -8.59751627e-02 -1.71806812e-01\n",
      "  7.80056894e-01  1.51630104e-01  2.96293616e-01  1.60506576e-01\n",
      " -6.33846670e-02 -5.69900796e-02 -5.88919036e-02 -6.84572160e-01\n",
      " -7.04938546e-04  2.46744156e-01  2.25677311e-01 -8.23454112e-02\n",
      " -5.27979791e-01 -5.08032322e-01 -1.84435844e-01  9.23074484e-01\n",
      "  7.35809445e-01 -4.10479635e-01 -2.21967623e-01 -5.50180316e-01\n",
      "  3.03729922e-01 -4.96013671e-01  5.50130188e-01  7.58742869e-01\n",
      "  5.94226606e-02 -4.39812601e-01  1.16967094e+00 -5.56134701e-01\n",
      "  4.20879245e-01  7.08146334e-01  3.12095106e-01  1.29798785e-01\n",
      " -6.85978770e-01  6.66718185e-01  4.30409670e-01  2.85361290e-01\n",
      " -3.43734175e-01  3.40557098e-01  4.58756328e-01 -1.30768120e-01\n",
      " -4.67268795e-01  5.87800920e-01 -1.30464152e-01 -1.07199299e+00\n",
      "  6.74340010e-01 -8.66399184e-02  3.83873284e-01 -1.83765680e-01\n",
      " -3.93801302e-01  2.69577444e-01 -5.39098233e-02  1.67211309e-01\n",
      "  2.02100113e-01 -4.57387537e-01 -8.52275074e-01 -1.42785877e-01\n",
      "  3.35738182e-01 -6.06438577e-01 -1.52843416e-01 -3.03091884e-01\n",
      " -8.47115964e-02  6.67190492e-01 -6.68784678e-02  6.33220226e-02\n",
      "  5.23253739e-01 -1.75359398e-02 -4.24083918e-01 -2.04071775e-01\n",
      "  2.14214072e-01 -7.02822745e-01  1.12920888e-01 -3.13348323e-02\n",
      " -2.11466819e-01 -1.50750086e-01  4.50119257e-01  1.24140903e-01\n",
      "  7.55189002e-01 -1.10170841e-01  2.85554498e-01 -6.85934961e-01\n",
      "  2.90729553e-01 -1.57349512e-01 -2.48254389e-02  7.73136169e-02\n",
      " -1.95251510e-01 -1.05724163e-01  1.81615889e-01 -4.38701361e-01\n",
      "  2.71253109e-01  2.69789636e-01  1.97617477e-03  4.88260210e-01\n",
      " -6.08754009e-02 -6.35233000e-02 -8.48654211e-01 -4.67859924e-01\n",
      " -4.04063538e-02  3.92924100e-01  4.71455455e-01 -1.16113812e-01\n",
      "  6.34941638e-01 -2.62468249e-01 -4.95653987e-01 -3.08744982e-02\n",
      "  3.34599704e-01 -8.20729434e-02 -3.69854458e-02 -4.42685336e-01\n",
      " -5.49810231e-01  7.30598047e-02  2.70330966e-01 -1.09698489e-01\n",
      " -2.68085241e-01  5.26564658e-01  2.56138742e-01 -1.54329166e-01\n",
      "  2.68264174e-01  6.50385201e-01  1.24321252e-01 -8.18036258e-01\n",
      " -7.96757817e-01 -3.77051294e-01 -2.34010071e-01 -4.08915311e-01\n",
      " -4.76937480e-02  3.31464559e-01  7.78878927e-01  2.71527432e-02\n",
      " -3.22321892e-01  6.98770434e-02 -4.85345066e-01 -7.42784023e-01\n",
      " -4.95838150e-02  2.48317659e-01 -6.83598459e-01 -2.63965130e-01\n",
      " -3.53393927e-02 -4.28863764e-01  3.14770162e-01  1.37096956e-01\n",
      " -3.14997852e-01  8.23536664e-02  2.95320004e-02  5.10599613e-01\n",
      " -3.71191561e-01 -7.13964030e-02  2.93738581e-02 -3.19731176e-01\n",
      " -5.60358286e-01 -6.47917867e-01 -4.73039091e-01  1.92232206e-01\n",
      " -4.33800578e-01  1.69615537e-01  2.53361315e-01  1.28410399e-01\n",
      "  2.39444315e-01 -5.11149056e-02  2.00972959e-01  4.52268481e-01\n",
      " -9.15191919e-02  7.91754797e-02  6.82296529e-02 -2.93338329e-01\n",
      "  4.06531304e-01 -4.03168797e-01  3.09270024e-01  6.50663003e-02\n",
      "  1.05046615e-01 -8.64279717e-02 -1.53797373e-01  4.02077079e-01\n",
      "  5.03367066e-01  7.63851777e-02  2.09626377e-01  1.53182715e-01\n",
      " -5.65243840e-01 -1.32839739e-01  6.27765238e-01 -3.82222414e-01\n",
      " -3.88237000e-01  4.48667586e-01 -4.94298071e-01 -4.10451323e-01\n",
      " -4.89565134e-01 -4.12086457e-01  5.01163781e-01 -6.34864271e-01\n",
      "  4.93994743e-01  3.40991318e-02 -7.70612210e-02 -5.23390770e-01\n",
      " -3.90536785e-01  9.76225793e-01 -2.31955290e-01  4.92922813e-01\n",
      "  2.07122847e-01  3.43242586e-01  6.75997019e-01  3.43689382e-01\n",
      " -7.34328479e-03 -7.62901306e-02 -1.36262581e-01 -3.49554360e-01\n",
      "  2.33103514e-01 -2.97866374e-01  1.42095387e-01  2.46145166e-02\n",
      "  6.91457465e-02 -7.26145744e-01 -2.68725991e-01  4.27473038e-01\n",
      "  8.14948857e-01  1.12877059e+00  5.72656631e-01  6.83157027e-01\n",
      "  2.91611552e-01  4.32616830e-01 -8.02971900e-01 -2.75246471e-01\n",
      "  1.94451630e-01  6.13069296e-01 -5.72932422e-01  2.11954072e-01\n",
      "  1.04959095e+00 -7.37529248e-02  1.31617635e-01  5.35595417e-01\n",
      "  5.16681373e-01 -9.72960353e-01  2.33713865e-01 -6.80972874e-01\n",
      " -4.43347394e-02  6.06121868e-02 -8.95191729e-01  6.56643152e-01\n",
      " -4.67941612e-01  1.72935054e-01  3.06211114e-01 -2.09628448e-01\n",
      " -4.73560929e-01 -7.77069032e-01  5.25709808e-01 -5.47120214e-01\n",
      " -4.76924509e-01  3.44251424e-01  5.33317804e-01 -1.14928357e-01\n",
      "  8.92056108e-01 -1.09298661e-01 -1.78937271e-01  4.77421999e-01\n",
      "  1.20972060e-02 -6.01830721e-01  1.15526125e-01  1.84282243e-01\n",
      "  1.20737918e-01 -8.41489851e-01  2.74324179e-01  6.53214380e-02\n",
      "  3.10614675e-01  1.87517211e-01 -3.63761693e-01 -4.86926019e-01\n",
      "  2.67216444e-01  7.08758384e-02  4.93382275e-01 -6.62216306e-01\n",
      " -8.03277671e-01  5.06674275e-02 -4.52651903e-02  8.58727932e-01\n",
      "  3.86713773e-01 -2.26233333e-01  2.94005901e-01  8.16828609e-02\n",
      "  4.67826068e-01  9.88701731e-02 -5.52253127e-01 -6.25416636e-02\n",
      " -4.36638117e-01 -1.23846389e-01  3.10184360e-01  1.10362805e-01\n",
      " -3.76352072e-01 -6.95727170e-01 -2.82260239e-01 -2.98604459e-01\n",
      " -5.82896113e-01 -7.70874918e-02  6.86300695e-01  4.38593328e-01\n",
      "  2.09501594e-01  1.93385899e-01  1.10943151e+00 -6.18437052e-01\n",
      " -5.46705842e-01 -3.53303879e-01 -4.70864028e-03  2.02219620e-01\n",
      "  1.96173102e-01 -8.64403367e-01 -6.46913230e-01  1.45509869e-01\n",
      " -6.57370865e-01  2.44000092e-01  6.11473203e-01  5.38556337e-01\n",
      " -6.71112657e-01 -4.79236394e-02  1.10620749e+00  2.27204785e-01\n",
      " -2.44950041e-01 -4.12875004e-02 -4.92067188e-02 -3.21430787e-02\n",
      " -3.99663895e-01 -2.86540627e-01 -4.12346452e-01  3.79928648e-02\n",
      "  3.89240086e-01 -1.03023134e-01  3.54121447e-01  1.34739459e+00\n",
      " -1.63724944e-02 -6.84470415e-01 -2.10255329e-02 -1.12153515e-01\n",
      "  4.70662117e-01  5.75038865e-02 -6.25386477e-01 -3.15227062e-01\n",
      " -4.69694585e-01 -4.81898487e-02 -1.43374532e-01 -3.73101383e-01\n",
      "  1.92334324e-01 -1.23024061e-01  1.10399842e+00 -8.03215504e-01\n",
      "  3.67378086e-01  8.49291235e-02 -2.87107050e-01 -3.32370818e-01\n",
      " -2.46271253e-01  4.15957689e-01  1.03018686e-01  4.71279770e-01\n",
      " -1.91447139e-01  3.02230835e-01 -6.40176773e-01  4.34333146e-01\n",
      "  4.13303167e-01 -2.25464195e-01 -2.43023872e-01 -2.54389524e-01\n",
      "  1.05514765e-01  4.62910652e-01 -4.52698588e-01  4.98102456e-02\n",
      " -2.12586150e-01 -5.03674865e-01  1.03526384e-01  2.60310173e-01\n",
      " -1.91697702e-01 -4.90235597e-01  9.73624110e-01  1.67644136e-02\n",
      " -3.09523135e-01 -5.45606792e-01 -3.38727415e-01  1.95590585e-01\n",
      " -3.95997971e-01  4.34039265e-01 -4.79470164e-01 -1.79774463e-01\n",
      " -1.33216277e-01  2.92425573e-01  1.17567241e-01 -2.01351941e-04\n",
      " -2.28282869e-01 -2.98369490e-03  1.14583872e-01  1.87827110e-01\n",
      " -6.16419092e-02 -2.99429864e-01 -5.49328566e-01  1.95448071e-01\n",
      "  3.30738798e-02  4.45143133e-01 -1.45918047e+00  2.76990116e-01\n",
      "  1.06867522e-01 -6.36705995e-01  1.49721503e-02  6.21623933e-01\n",
      "  4.35345471e-01  3.67273450e-01  2.21252292e-01 -1.68745726e-01\n",
      "  6.34661615e-01  3.05576414e-01 -6.60773218e-01 -3.48220825e-01\n",
      " -6.01494908e-01  1.62416726e-01  8.51277590e-01  5.64038873e-01\n",
      " -1.32888541e-01  5.78130960e-01  4.37329769e-01 -2.50419378e-01\n",
      "  1.27389640e-01  3.04841608e-01  1.69228345e-01 -6.80671453e-01\n",
      " -7.91647136e-02 -3.90768163e-02 -5.89853674e-02  3.26032192e-01\n",
      "  2.79810995e-01  2.01112419e-01  2.79959261e-01 -3.94225419e-01\n",
      " -5.61534055e-02 -2.80866593e-01  4.21973437e-01 -1.25217527e-01\n",
      " -2.44032800e-01  2.56264925e-01 -5.77546299e-01 -1.79337002e-02\n",
      "  3.75045419e-01  3.90127748e-01  3.08235317e-01  4.32461239e-02\n",
      " -3.19815397e-01 -7.31928587e-01 -8.57445300e-01 -4.56196636e-01\n",
      "  1.91792414e-01  1.12777472e+00  1.14098623e-01 -3.93427730e-01\n",
      " -5.31382799e-01  3.73633385e-01  6.49822295e-01  6.61252618e-01\n",
      " -5.59039831e-01 -5.07438898e-01  1.88692242e-01  4.30638194e-01\n",
      " -4.43620563e-01  4.72782195e-01 -6.59415543e-01  6.56473190e-02\n",
      "  5.67307770e-01 -2.75379837e-01 -4.96693611e-01  7.40167964e-03\n",
      "  3.16577435e-01 -4.18295413e-01 -4.22885656e-01 -5.52101433e-01\n",
      "  1.22607075e-01  9.61042792e-02  3.23055595e-01 -8.41065586e-01\n",
      " -7.24421814e-02 -1.82764068e-01  6.32212877e-01  1.67815357e-01\n",
      "  9.77995574e-01 -5.84435463e-03  5.92191696e-01  3.41564238e-01\n",
      "  6.90299749e-01 -1.18450627e-01 -7.60542631e-01  3.70549768e-01\n",
      "  7.42236316e-01 -1.17789373e-01  1.43134284e+00 -5.23150861e-01\n",
      "  7.84933329e-01 -1.76844656e-01 -7.12502420e-01 -9.65571776e-02\n",
      " -3.18498445e+00  3.49936277e-01  1.60431802e-01 -4.65894155e-02\n",
      "  1.10750936e-01  5.58575392e-01  3.64335775e-01 -2.92843252e-01\n",
      " -2.93849111e-01 -2.34929547e-01  4.15985495e-01 -2.82904059e-01\n",
      " -2.30949983e-01  4.11721736e-01  1.61260486e-01  7.83595920e-01\n",
      "  9.95904356e-02 -6.22513071e-02  6.16447330e-01  1.61838517e-01\n",
      "  1.10445112e-01  9.79023241e-03  1.38397485e-01 -5.57122171e-01\n",
      " -3.70276511e-01  4.98058796e-02 -4.47070837e-01 -5.60444772e-01\n",
      " -3.11797470e-01 -1.47280797e-01  5.75177312e-01  6.68729022e-02\n",
      "  5.60362518e-01 -1.32139444e-01  8.84481013e-01 -8.28564614e-02\n",
      "  2.30413064e-01 -3.49385709e-01  4.17937599e-02 -9.30054039e-02\n",
      " -3.41964692e-01 -6.47252619e-01  1.84758946e-01 -8.61653328e-01\n",
      " -2.63148900e-02  2.52711296e-01 -6.25010908e-01  6.25658184e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"First element of X_train_name_last_hidden:\", X_train_name_last_hidden[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_name_last_hidden: (40032, 768)\n",
      "Data type of X_train_name_last_hidden: float32\n",
      "Shape of X_train_description_last_hidden: (40032, 768)\n",
      "Data type of X_train_description_last_hidden: float32\n",
      "Shape of X_test_name_last_hidden: (10009, 768)\n",
      "Data type of X_test_name_last_hidden: float32\n",
      "Shape of X_test_description_last_hidden: (10009, 768)\n",
      "Data type of X_test_description_last_hidden: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_name_last_hidden:\", X_train_name_last_hidden.shape)\n",
    "print(\"Data type of X_train_name_last_hidden:\", X_train_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_train_description_last_hidden:\", X_train_description_last_hidden.shape)\n",
    "print(\"Data type of X_train_description_last_hidden:\", X_train_description_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_name_last_hidden:\", X_test_name_last_hidden.shape)\n",
    "print(\"Data type of X_test_name_last_hidden:\", X_test_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_description_last_hidden:\", X_test_description_last_hidden.shape)\n",
    "print(\"Data type of X_test_description_last_hidden:\", X_test_description_last_hidden.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concatenated = np.concatenate((X_train_name_last_hidden, X_train_description_last_hidden), axis=1)\n",
    "X_test_concatenated = np.concatenate((X_test_name_last_hidden, X_test_description_last_hidden), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_concatenated: (40032, 1536)\n",
      "Data type of X_train_concatenated: float32\n",
      "Shape of X_test_concatenated: (10009, 1536)\n",
      "Data type of X_test_concatenated: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_concatenated:\", X_train_concatenated.shape)\n",
    "print(\"Data type of X_train_concatenated:\", X_train_concatenated.dtype)\n",
    "\n",
    "print(\"Shape of X_test_concatenated:\", X_test_concatenated.shape)\n",
    "print(\"Data type of X_test_concatenated:\", X_test_concatenated.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "X_train_combined = np.concatenate((X_train_processed_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_processed_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 2\n",
    "X_train_combined_1 = np.concatenate((X_train_processed_1_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_1 = np.concatenate((X_test_processed_1_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 3\n",
    "X_train_combined_2 = np.concatenate((X_train_processed_2_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_2 = np.concatenate((X_test_processed_2_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 4\n",
    "X_train_combined_3 = np.concatenate((X_train_processed_3_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_3 = np.concatenate((X_test_processed_3_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 5\n",
    "X_train_combined_4 = np.concatenate((X_train_processed_4_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_4 = np.concatenate((X_test_processed_4_loaded, X_test_concatenated), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define num_clases\n",
    "\n",
    "num_classes = len(df['parent_category'].unique())\n",
    "num_classes_1 = len(df['sub_category_1'].unique())\n",
    "num_classes_2 = len(df['sub_category_2'].unique())\n",
    "num_classes_3 = len(df['sub_category_3'].unique())\n",
    "num_classes_4 = len(df['sub_category_4'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "print(num_classes_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 3ms/step - loss: 1.0572 - accuracy: 0.7400 - val_loss: 0.4429 - val_accuracy: 0.8809 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5946 - accuracy: 0.8409 - val_loss: 0.4867 - val_accuracy: 0.8600 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4880 - accuracy: 0.8724 - val_loss: 0.3336 - val_accuracy: 0.9112 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4272 - accuracy: 0.8873 - val_loss: 0.2568 - val_accuracy: 0.9350 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3738 - accuracy: 0.9022 - val_loss: 0.2424 - val_accuracy: 0.9412 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3428 - accuracy: 0.9109 - val_loss: 0.2655 - val_accuracy: 0.9308 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3217 - accuracy: 0.9173 - val_loss: 0.2384 - val_accuracy: 0.9404 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2979 - accuracy: 0.9233 - val_loss: 0.2216 - val_accuracy: 0.9451 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2754 - accuracy: 0.9283 - val_loss: 0.2074 - val_accuracy: 0.9481 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2605 - accuracy: 0.9344 - val_loss: 0.2072 - val_accuracy: 0.9486 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2513 - accuracy: 0.9362 - val_loss: 0.1954 - val_accuracy: 0.9523 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2329 - accuracy: 0.9410 - val_loss: 0.1947 - val_accuracy: 0.9541 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2283 - accuracy: 0.9420 - val_loss: 0.1920 - val_accuracy: 0.9548 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2139 - accuracy: 0.9451 - val_loss: 0.1921 - val_accuracy: 0.9546 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2157 - accuracy: 0.9451 - val_loss: 0.1926 - val_accuracy: 0.9557 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2059 - accuracy: 0.9478 - val_loss: 0.1901 - val_accuracy: 0.9551 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2070 - accuracy: 0.9473 - val_loss: 0.1906 - val_accuracy: 0.9564 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2006 - accuracy: 0.9478 - val_loss: 0.1923 - val_accuracy: 0.9553 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2018 - accuracy: 0.9487 - val_loss: 0.1912 - val_accuracy: 0.9553 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1987 - accuracy: 0.9482 - val_loss: 0.1899 - val_accuracy: 0.9560 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1962 - accuracy: 0.9497 - val_loss: 0.1897 - val_accuracy: 0.9564 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1932 - accuracy: 0.9500 - val_loss: 0.1893 - val_accuracy: 0.9564 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1954 - accuracy: 0.9508 - val_loss: 0.1901 - val_accuracy: 0.9566 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1976 - accuracy: 0.9491 - val_loss: 0.1902 - val_accuracy: 0.9562 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1938 - accuracy: 0.9513 - val_loss: 0.1893 - val_accuracy: 0.9562 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1954 - accuracy: 0.9515 - val_loss: 0.1895 - val_accuracy: 0.9566 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1917 - accuracy: 0.9512 - val_loss: 0.1895 - val_accuracy: 0.9563 - lr: 4.9511e-07\n",
      "  1/313 [..............................] - ETA: 19s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\2787510625.py:42: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, 'model_1_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9564\n",
      "Accuracy: 0.956439197063446\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           4       0.98      0.98      0.98      1712\n",
      "           5       0.93      0.96      0.94       715\n",
      "           6       0.97      0.98      0.97        91\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       1.00      1.00      1.00         2\n",
      "          11       0.95      0.97      0.96       649\n",
      "          12       0.96      0.98      0.97       505\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.96      0.97      0.97      1392\n",
      "          16       0.94      0.95      0.95      1247\n",
      "          17       0.93      0.93      0.93       890\n",
      "          18       0.00      0.00      0.00         6\n",
      "          25       0.00      0.00      0.00         2\n",
      "          27       0.79      0.94      0.86        16\n",
      "          28       0.00      0.00      0.00         3\n",
      "          31       0.00      0.00      0.00         3\n",
      "          33       0.95      0.94      0.94       245\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         1\n",
      "          39       1.00      1.00      1.00        15\n",
      "          40       0.00      0.00      0.00         2\n",
      "          41       0.93      0.52      0.67        27\n",
      "          42       1.00      0.80      0.89        10\n",
      "          43       0.95      0.96      0.95       624\n",
      "          44       0.83      0.53      0.64        36\n",
      "          46       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.92      0.92      0.92       417\n",
      "          54       0.95      0.90      0.93       132\n",
      "          55       0.99      0.98      0.99      1194\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       0.98      0.79      0.87        57\n",
      "          61       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.96     10009\n",
      "   macro avg       0.51      0.49      0.50     10009\n",
      "weighted avg       0.95      0.96      0.95     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Compile the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model fit\n",
    "run_1 = model.fit(X_train_combined, y_train, epochs=60, batch_size=32,\n",
    "                  validation_data=(X_test_combined, y_test),\n",
    "                  callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "save_model(model, 'model_1_preberttune.h5')\n",
    "\n",
    "y_pred_probabilities = model.predict(X_test_combined)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.8049 - accuracy: 0.6070 - val_loss: 0.5957 - val_accuracy: 0.8372 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8570 - accuracy: 0.7700 - val_loss: 0.4661 - val_accuracy: 0.8711 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6992 - accuracy: 0.8019 - val_loss: 0.4215 - val_accuracy: 0.8750 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6005 - accuracy: 0.8276 - val_loss: 0.3212 - val_accuracy: 0.9034 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5537 - accuracy: 0.8380 - val_loss: 0.3051 - val_accuracy: 0.9079 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4990 - accuracy: 0.8512 - val_loss: 0.2627 - val_accuracy: 0.9162 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4714 - accuracy: 0.8585 - val_loss: 0.2487 - val_accuracy: 0.9283 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4521 - accuracy: 0.8649 - val_loss: 0.2344 - val_accuracy: 0.9313 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4175 - accuracy: 0.8754 - val_loss: 0.2331 - val_accuracy: 0.9324 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3955 - accuracy: 0.8815 - val_loss: 0.2226 - val_accuracy: 0.9333 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3852 - accuracy: 0.8834 - val_loss: 0.2120 - val_accuracy: 0.9378 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3710 - accuracy: 0.8888 - val_loss: 0.2077 - val_accuracy: 0.9380 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3570 - accuracy: 0.8931 - val_loss: 0.2032 - val_accuracy: 0.9413 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8946 - val_loss: 0.2005 - val_accuracy: 0.9423 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3424 - accuracy: 0.8969 - val_loss: 0.1980 - val_accuracy: 0.9439 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3334 - accuracy: 0.8996 - val_loss: 0.1948 - val_accuracy: 0.9446 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3300 - accuracy: 0.9009 - val_loss: 0.1945 - val_accuracy: 0.9445 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3257 - accuracy: 0.9016 - val_loss: 0.1938 - val_accuracy: 0.9445 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3148 - accuracy: 0.9058 - val_loss: 0.1933 - val_accuracy: 0.9446 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3167 - accuracy: 0.9050 - val_loss: 0.1927 - val_accuracy: 0.9451 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3160 - accuracy: 0.9040 - val_loss: 0.1923 - val_accuracy: 0.9446 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3172 - accuracy: 0.9037 - val_loss: 0.1925 - val_accuracy: 0.9450 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3160 - accuracy: 0.9049 - val_loss: 0.1920 - val_accuracy: 0.9452 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3127 - accuracy: 0.9065 - val_loss: 0.1921 - val_accuracy: 0.9451 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3102 - accuracy: 0.9079 - val_loss: 0.1924 - val_accuracy: 0.9451 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3145 - accuracy: 0.9052 - val_loss: 0.1922 - val_accuracy: 0.9451 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3135 - accuracy: 0.9038 - val_loss: 0.1924 - val_accuracy: 0.9453 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3108 - accuracy: 0.9066 - val_loss: 0.1924 - val_accuracy: 0.9452 - lr: 2.9707e-07\n",
      " 43/313 [===>..........................] - ETA: 0s - loss: 0.1783 - accuracy: 0.9469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\1687609008.py:40: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_2, 'model_2_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9452\n",
      "Accuracy for Stage 2 model: 0.9452492594718933\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 2 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.94      0.93        35\n",
      "           2       0.76      0.70      0.73        23\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       0.95      1.00      0.97        54\n",
      "           5       0.91      0.92      0.92       128\n",
      "           6       0.93      1.00      0.96        13\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       1.00      0.99      0.99        79\n",
      "          10       1.00      1.00      1.00         9\n",
      "          11       0.96      0.94      0.95       156\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.83      0.55      0.66        71\n",
      "          14       1.00      0.91      0.95        11\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       0.33      1.00      0.50         1\n",
      "          18       1.00      0.33      0.50         3\n",
      "          19       0.87      0.98      0.92       128\n",
      "          20       0.86      1.00      0.92         6\n",
      "          21       0.95      0.96      0.96       278\n",
      "          22       0.82      0.75      0.78        12\n",
      "          23       1.00      0.58      0.74        12\n",
      "          24       0.88      0.70      0.78        10\n",
      "          25       0.00      0.00      0.00         3\n",
      "          26       0.99      1.00      1.00      1242\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.50      0.50      0.50         4\n",
      "          29       0.96      0.98      0.97       464\n",
      "          30       0.99      0.89      0.94        74\n",
      "          31       0.94      0.97      0.95       125\n",
      "          32       0.92      0.62      0.74        39\n",
      "          33       1.00      0.99      0.99        83\n",
      "          34       0.90      0.98      0.94       372\n",
      "          35       0.99      0.93      0.96        88\n",
      "          36       0.98      0.96      0.97        57\n",
      "          37       1.00      0.25      0.40         8\n",
      "          38       0.83      0.71      0.77        28\n",
      "          39       0.00      0.00      0.00         7\n",
      "          40       1.00      1.00      1.00        14\n",
      "          42       1.00      1.00      1.00       245\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       0.97      0.91      0.94        43\n",
      "          45       0.92      0.93      0.93       105\n",
      "          46       0.97      0.94      0.95        32\n",
      "          47       1.00      0.50      0.67         4\n",
      "          48       1.00      1.00      1.00         6\n",
      "          49       1.00      1.00      1.00       132\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          54       0.88      1.00      0.93         7\n",
      "          55       0.92      0.96      0.94        24\n",
      "          56       0.99      1.00      0.99       207\n",
      "          57       0.96      0.98      0.97       200\n",
      "          59       0.93      0.98      0.96       275\n",
      "          60       0.94      0.85      0.89        34\n",
      "          61       0.00      0.00      0.00         1\n",
      "          62       0.97      0.88      0.93        43\n",
      "          63       0.92      0.98      0.95       134\n",
      "          64       0.67      1.00      0.80         2\n",
      "          65       1.00      0.79      0.88        19\n",
      "          66       1.00      0.71      0.83         7\n",
      "          67       0.90      0.76      0.83        25\n",
      "          70       0.99      0.99      0.99       113\n",
      "          72       0.96      1.00      0.98        27\n",
      "          73       0.85      0.63      0.72        27\n",
      "          74       1.00      1.00      1.00        27\n",
      "          75       0.69      0.84      0.76       100\n",
      "          77       1.00      1.00      1.00        58\n",
      "          79       1.00      1.00      1.00        52\n",
      "          80       1.00      1.00      1.00         6\n",
      "          81       1.00      1.00      1.00         4\n",
      "          83       0.62      0.81      0.70        99\n",
      "          84       0.98      0.93      0.96        58\n",
      "          85       0.00      0.00      0.00         3\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.91      0.67      0.77        30\n",
      "          88       0.97      0.94      0.95        32\n",
      "          89       0.91      0.62      0.74        16\n",
      "          90       0.97      0.98      0.97       217\n",
      "          91       0.94      0.94      0.94        53\n",
      "          92       0.96      0.92      0.94       108\n",
      "          93       1.00      0.62      0.77        32\n",
      "          94       1.00      0.10      0.18        10\n",
      "          95       0.00      0.00      0.00         2\n",
      "          96       1.00      0.82      0.90        22\n",
      "          97       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00        13\n",
      "          99       0.25      0.14      0.18        37\n",
      "         100       0.50      0.88      0.63        68\n",
      "         102       0.00      0.00      0.00         1\n",
      "         103       0.99      1.00      1.00       722\n",
      "         104       0.97      0.97      0.97        31\n",
      "         105       1.00      0.99      1.00       236\n",
      "         107       0.79      0.54      0.65        57\n",
      "         108       0.96      0.98      0.97       197\n",
      "         109       1.00      1.00      1.00         9\n",
      "         110       0.00      0.00      0.00         8\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       1.00      0.78      0.88         9\n",
      "         114       0.98      0.98      0.98       181\n",
      "         115       0.98      0.99      0.99       614\n",
      "         116       0.95      0.99      0.97        84\n",
      "         119       1.00      0.25      0.40         8\n",
      "         120       0.91      0.97      0.94       117\n",
      "         121       0.43      0.60      0.50         5\n",
      "         122       0.97      0.97      0.97       191\n",
      "         123       0.98      0.97      0.98        61\n",
      "         124       0.93      1.00      0.96        68\n",
      "         125       0.99      1.00      0.99        72\n",
      "         126       0.90      1.00      0.95        55\n",
      "         127       1.00      1.00      1.00         1\n",
      "         128       0.00      0.00      0.00         1\n",
      "         130       1.00      1.00      1.00        64\n",
      "         131       0.95      0.97      0.96        62\n",
      "         132       0.76      0.76      0.76        21\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         1\n",
      "         136       1.00      0.99      0.99        96\n",
      "         138       0.88      0.71      0.79        21\n",
      "         139       0.60      0.80      0.69        15\n",
      "         140       0.75      0.71      0.73        65\n",
      "         141       0.87      0.98      0.92        54\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.96      0.97      0.97       243\n",
      "         144       0.95      1.00      0.97        18\n",
      "         145       0.95      0.87      0.91        23\n",
      "         146       1.00      1.00      1.00        12\n",
      "         148       0.96      0.85      0.90       154\n",
      "\n",
      "    accuracy                           0.95     10009\n",
      "   macro avg       0.78      0.74      0.74     10009\n",
      "weighted avg       0.94      0.95      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_2 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_1.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_2 = model_2.fit(X_train_combined_1, y_train_1, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_1, y_test_1),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_2, 'model_2_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_2, accuracy_2 = model_2.evaluate(X_test_combined_1, y_test_1)\n",
    "print(\"Accuracy for Stage 2 model:\", accuracy_2)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_2 = model_2.predict(X_test_combined_1)\n",
    "y_pred_2 = np.argmax(y_pred_probabilities_2, axis=1)\n",
    "report_2 = classification_report(y_test_1, y_pred_2)\n",
    "print(\"Classification Report for Stage 2 model:\")\n",
    "print(report_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 2.9663 - accuracy: 0.4649 - val_loss: 1.7470 - val_accuracy: 0.6254 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.5692 - accuracy: 0.6442 - val_loss: 0.9047 - val_accuracy: 0.7719 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.2371 - accuracy: 0.6979 - val_loss: 0.6932 - val_accuracy: 0.8210 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.0676 - accuracy: 0.7298 - val_loss: 0.5935 - val_accuracy: 0.8368 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.9674 - accuracy: 0.7487 - val_loss: 0.5430 - val_accuracy: 0.8551 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.8983 - accuracy: 0.7654 - val_loss: 0.5427 - val_accuracy: 0.8568 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8381 - accuracy: 0.7776 - val_loss: 0.4848 - val_accuracy: 0.8698 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7931 - accuracy: 0.7838 - val_loss: 0.4613 - val_accuracy: 0.8720 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7687 - accuracy: 0.7912 - val_loss: 0.4482 - val_accuracy: 0.8786 - lr: 4.3541e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.7174 - accuracy: 0.8039 - val_loss: 0.4334 - val_accuracy: 0.8861 - lr: 3.7010e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6842 - accuracy: 0.8115 - val_loss: 0.4208 - val_accuracy: 0.8902 - lr: 3.1459e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6705 - accuracy: 0.8151 - val_loss: 0.4067 - val_accuracy: 0.8932 - lr: 2.6740e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6566 - accuracy: 0.8176 - val_loss: 0.4029 - val_accuracy: 0.8932 - lr: 2.2729e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6370 - accuracy: 0.8232 - val_loss: 0.3976 - val_accuracy: 0.8954 - lr: 1.9319e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6168 - accuracy: 0.8270 - val_loss: 0.3930 - val_accuracy: 0.8972 - lr: 1.6422e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6059 - accuracy: 0.8306 - val_loss: 0.3893 - val_accuracy: 0.8969 - lr: 1.3958e-04\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5968 - accuracy: 0.8324 - val_loss: 0.3848 - val_accuracy: 0.8985 - lr: 1.0469e-04\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5831 - accuracy: 0.8372 - val_loss: 0.3841 - val_accuracy: 0.8981 - lr: 7.8515e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5772 - accuracy: 0.8384 - val_loss: 0.3834 - val_accuracy: 0.8978 - lr: 5.8887e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5717 - accuracy: 0.8392 - val_loss: 0.3834 - val_accuracy: 0.8990 - lr: 4.1221e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5609 - accuracy: 0.8409 - val_loss: 0.3826 - val_accuracy: 0.8993 - lr: 2.8854e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5652 - accuracy: 0.8401 - val_loss: 0.3813 - val_accuracy: 0.8995 - lr: 2.0198e-05\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5622 - accuracy: 0.8439 - val_loss: 0.3803 - val_accuracy: 0.8994 - lr: 1.4139e-05\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5511 - accuracy: 0.8425 - val_loss: 0.3800 - val_accuracy: 0.9000 - lr: 9.8971e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5681 - accuracy: 0.8412 - val_loss: 0.3794 - val_accuracy: 0.9000 - lr: 6.9280e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5530 - accuracy: 0.8451 - val_loss: 0.3792 - val_accuracy: 0.8996 - lr: 4.8496e-06\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5556 - accuracy: 0.8410 - val_loss: 0.3795 - val_accuracy: 0.9002 - lr: 3.3947e-06\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5602 - accuracy: 0.8434 - val_loss: 0.3790 - val_accuracy: 0.9000 - lr: 2.3763e-06\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5579 - accuracy: 0.8440 - val_loss: 0.3801 - val_accuracy: 0.9008 - lr: 1.6634e-06\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5613 - accuracy: 0.8427 - val_loss: 0.3784 - val_accuracy: 0.8998 - lr: 1.1644e-06\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5606 - accuracy: 0.8417 - val_loss: 0.3786 - val_accuracy: 0.9002 - lr: 8.1507e-07\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5556 - accuracy: 0.8448 - val_loss: 0.3786 - val_accuracy: 0.8998 - lr: 5.7055e-07\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5608 - accuracy: 0.8424 - val_loss: 0.3780 - val_accuracy: 0.8997 - lr: 3.9938e-07\n",
      "Epoch 34/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5563 - accuracy: 0.8447 - val_loss: 0.3778 - val_accuracy: 0.9008 - lr: 2.7957e-07\n",
      "Epoch 35/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5582 - accuracy: 0.8423 - val_loss: 0.3795 - val_accuracy: 0.8993 - lr: 1.9570e-07\n",
      "Epoch 36/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5592 - accuracy: 0.8425 - val_loss: 0.3780 - val_accuracy: 0.9006 - lr: 1.3699e-07\n",
      "Epoch 37/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.5564 - accuracy: 0.8420 - val_loss: 0.3787 - val_accuracy: 0.8997 - lr: 9.5892e-08\n",
      "Epoch 38/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5555 - accuracy: 0.8432 - val_loss: 0.3788 - val_accuracy: 0.8997 - lr: 6.7124e-08\n",
      "Epoch 39/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5571 - accuracy: 0.8427 - val_loss: 0.3794 - val_accuracy: 0.9003 - lr: 4.6987e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\3792186953.py:41: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_3, 'model_3_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3778 - accuracy: 0.9008\n",
      "Accuracy for Stage 3 model: 0.9007892608642578\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 3 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00        18\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.78      1.00      0.88        70\n",
      "           6       1.00      0.33      0.50         3\n",
      "           8       0.67      0.86      0.75         7\n",
      "          10       0.50      1.00      0.67         6\n",
      "          12       0.75      0.75      0.75         8\n",
      "          13       1.00      1.00      1.00        10\n",
      "          14       0.83      0.97      0.89        39\n",
      "          15       1.00      0.83      0.91         6\n",
      "          16       0.50      1.00      0.67         3\n",
      "          17       0.89      0.91      0.90        35\n",
      "          18       0.90      0.93      0.92       101\n",
      "          19       0.92      0.92      0.92        25\n",
      "          20       0.00      0.00      0.00         3\n",
      "          22       0.91      0.95      0.93        21\n",
      "          23       1.00      1.00      1.00        83\n",
      "          24       0.62      0.97      0.76        34\n",
      "          25       0.88      0.33      0.48        21\n",
      "          26       1.00      1.00      1.00       113\n",
      "          27       0.93      1.00      0.96        25\n",
      "          28       0.95      1.00      0.97        55\n",
      "          29       0.00      0.00      0.00        18\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       1.00      1.00      1.00        16\n",
      "          32       0.69      1.00      0.82       135\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.91      0.98      0.95        53\n",
      "          35       0.98      1.00      0.99        63\n",
      "          37       0.75      0.60      0.67        10\n",
      "          38       1.00      1.00      1.00         9\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.89      1.00      0.94         8\n",
      "          42       1.00      1.00      1.00        43\n",
      "          43       1.00      0.67      0.80         3\n",
      "          44       0.86      0.50      0.63        12\n",
      "          46       1.00      0.67      0.80         6\n",
      "          47       0.43      0.50      0.46         6\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       1.00      0.90      0.95        10\n",
      "          50       0.90      1.00      0.95         9\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         2\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         6\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00         3\n",
      "          61       1.00      0.50      0.67         2\n",
      "          62       1.00      1.00      1.00        13\n",
      "          64       0.91      0.98      0.94        61\n",
      "          65       1.00      1.00      1.00         4\n",
      "          66       0.80      0.44      0.57         9\n",
      "          68       1.00      0.50      0.67         4\n",
      "          69       0.00      0.00      0.00         2\n",
      "          70       1.00      1.00      1.00         4\n",
      "          71       1.00      1.00      1.00         1\n",
      "          72       0.00      0.00      0.00         0\n",
      "          73       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         4\n",
      "          76       1.00      1.00      1.00         3\n",
      "          77       0.91      1.00      0.95        50\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.57      0.36      0.44        11\n",
      "          81       0.00      0.00      0.00         1\n",
      "          83       1.00      0.96      0.98        27\n",
      "          84       1.00      1.00      1.00         7\n",
      "          85       1.00      0.44      0.62         9\n",
      "          86       0.76      0.95      0.85        41\n",
      "          87       0.75      1.00      0.86         3\n",
      "          89       0.00      0.00      0.00         4\n",
      "          90       0.95      1.00      0.97        18\n",
      "          91       0.00      0.00      0.00         1\n",
      "          93       1.00      0.20      0.33         5\n",
      "          94       0.94      0.89      0.91        36\n",
      "          95       0.94      0.99      0.96        76\n",
      "          96       0.94      0.98      0.96        84\n",
      "          99       1.00      1.00      1.00         1\n",
      "         100       0.75      0.27      0.40        11\n",
      "         101       1.00      1.00      1.00         3\n",
      "         102       0.96      0.92      0.94        25\n",
      "         103       0.93      1.00      0.97       243\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       0.75      1.00      0.86         6\n",
      "         107       0.93      0.96      0.94        26\n",
      "         108       0.00      0.00      0.00         8\n",
      "         109       0.91      1.00      0.95        29\n",
      "         110       0.95      0.98      0.96        42\n",
      "         112       1.00      1.00      1.00         3\n",
      "         113       0.00      0.00      0.00         3\n",
      "         114       0.84      1.00      0.92       184\n",
      "         115       1.00      1.00      1.00         6\n",
      "         116       0.00      0.00      0.00         1\n",
      "         117       0.00      0.00      0.00         4\n",
      "         118       0.95      0.99      0.97       159\n",
      "         119       0.96      0.92      0.94       395\n",
      "         120       0.83      1.00      0.91         5\n",
      "         121       0.67      0.40      0.50         5\n",
      "         123       1.00      0.81      0.90        16\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       0.00      0.00      0.00         4\n",
      "         127       0.50      0.50      0.50         2\n",
      "         128       1.00      1.00      1.00         6\n",
      "         129       0.92      0.79      0.85        29\n",
      "         130       0.80      0.73      0.76        11\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.00      0.00      0.00        11\n",
      "         133       1.00      1.00      1.00         4\n",
      "         134       1.00      0.62      0.77         8\n",
      "         136       0.83      1.00      0.91         5\n",
      "         137       1.00      1.00      1.00       236\n",
      "         138       0.91      0.95      0.93       131\n",
      "         139       1.00      1.00      1.00       200\n",
      "         140       0.00      0.00      0.00         1\n",
      "         141       0.43      0.50      0.46         6\n",
      "         142       0.87      0.90      0.88        29\n",
      "         143       0.00      0.00      0.00         1\n",
      "         144       0.00      0.00      0.00         3\n",
      "         145       0.00      0.00      0.00         2\n",
      "         147       0.00      0.00      0.00         1\n",
      "         148       1.00      1.00      1.00         5\n",
      "         151       0.71      0.50      0.59        10\n",
      "         152       0.78      1.00      0.88        21\n",
      "         153       1.00      0.70      0.82        10\n",
      "         154       1.00      0.75      0.86         4\n",
      "         155       0.00      0.00      0.00         6\n",
      "         156       0.00      0.00      0.00         1\n",
      "         158       0.83      0.50      0.62        10\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       1.00      1.00      1.00         3\n",
      "         161       0.00      0.00      0.00         2\n",
      "         162       0.84      0.96      0.90        28\n",
      "         163       0.00      0.00      0.00         4\n",
      "         164       1.00      0.33      0.50         3\n",
      "         165       0.00      0.00      0.00         7\n",
      "         166       0.87      1.00      0.93        13\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       1.00      0.80      0.89         5\n",
      "         169       0.00      0.00      0.00         2\n",
      "         171       1.00      0.33      0.50         6\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.82      0.97      0.89        29\n",
      "         175       0.89      0.57      0.70        14\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       1.00      1.00      1.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         179       0.98      1.00      0.99        58\n",
      "         180       0.33      0.67      0.44         3\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.80      0.81      0.81        69\n",
      "         183       0.95      1.00      0.97        19\n",
      "         184       0.89      1.00      0.94        17\n",
      "         186       0.59      0.89      0.71        19\n",
      "         187       0.60      0.75      0.67         4\n",
      "         188       0.47      1.00      0.64         7\n",
      "         189       1.00      1.00      1.00         2\n",
      "         190       1.00      0.56      0.71         9\n",
      "         191       0.00      0.00      0.00         2\n",
      "         192       0.92      0.92      0.92        12\n",
      "         194       0.00      0.00      0.00         2\n",
      "         195       1.00      0.75      0.86         4\n",
      "         196       1.00      1.00      1.00         9\n",
      "         197       0.71      0.83      0.77         6\n",
      "         198       0.92      0.94      0.93        63\n",
      "         199       1.00      0.82      0.90        11\n",
      "         200       1.00      1.00      1.00        10\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.91      0.94      0.92        32\n",
      "         206       0.75      0.86      0.80        21\n",
      "         207       1.00      0.33      0.50         3\n",
      "         208       0.92      0.65      0.76        17\n",
      "         209       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         7\n",
      "         211       0.75      1.00      0.86         6\n",
      "         212       0.00      0.00      0.00         5\n",
      "         213       0.88      0.79      0.83        19\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.00      0.00      0.00         3\n",
      "         217       1.00      1.00      1.00         7\n",
      "         218       0.92      0.92      0.92        26\n",
      "         220       0.50      0.20      0.29         5\n",
      "         221       0.00      0.00      0.00         2\n",
      "         223       1.00      1.00      1.00         2\n",
      "         224       0.75      1.00      0.86         9\n",
      "         225       0.00      0.00      0.00         2\n",
      "         227       0.00      0.00      0.00         0\n",
      "         228       0.75      1.00      0.86         6\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00         1\n",
      "         231       1.00      0.33      0.50         6\n",
      "         232       0.71      1.00      0.83        35\n",
      "         233       0.75      1.00      0.86        18\n",
      "         234       1.00      0.62      0.77         8\n",
      "         236       0.99      0.97      0.98        71\n",
      "         238       0.00      0.00      0.00         5\n",
      "         239       0.00      0.00      0.00         2\n",
      "         240       0.83      0.71      0.77         7\n",
      "         241       0.95      0.97      0.96        37\n",
      "         242       0.86      1.00      0.92        12\n",
      "         243       0.00      0.00      0.00         2\n",
      "         244       0.82      0.50      0.62        18\n",
      "         245       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         1\n",
      "         248       0.92      0.85      0.88        13\n",
      "         249       0.00      0.00      0.00        10\n",
      "         250       0.75      0.69      0.72        26\n",
      "         251       0.97      1.00      0.98        32\n",
      "         252       0.00      0.00      0.00         1\n",
      "         253       0.00      0.00      0.00         2\n",
      "         254       1.00      1.00      1.00         1\n",
      "         256       1.00      1.00      1.00        19\n",
      "         257       0.00      0.00      0.00         3\n",
      "         258       0.59      0.46      0.52        28\n",
      "         259       1.00      0.33      0.50         3\n",
      "         260       1.00      0.50      0.67         2\n",
      "         262       0.88      1.00      0.93         7\n",
      "         263       1.00      0.25      0.40         4\n",
      "         266       1.00      0.25      0.40         4\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         1\n",
      "         273       0.85      0.89      0.87       112\n",
      "         274       0.00      0.00      0.00         4\n",
      "         275       0.54      1.00      0.70        28\n",
      "         278       0.67      1.00      0.80         4\n",
      "         280       0.50      0.33      0.40         3\n",
      "         282       0.90      0.95      0.92       146\n",
      "         283       0.89      0.96      0.93        26\n",
      "         284       0.60      0.60      0.60         5\n",
      "         285       0.86      0.80      0.83        15\n",
      "         288       0.90      0.75      0.82        12\n",
      "         289       0.00      0.00      0.00         8\n",
      "         290       0.58      0.82      0.68        17\n",
      "         291       0.81      0.96      0.88        26\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.89      1.00      0.94        33\n",
      "         295       1.00      1.00      1.00         7\n",
      "         297       1.00      1.00      1.00        23\n",
      "         299       0.00      0.00      0.00         2\n",
      "         300       0.45      0.71      0.56         7\n",
      "         301       0.00      0.00      0.00         1\n",
      "         302       0.00      0.00      0.00         2\n",
      "         303       0.81      1.00      0.89        17\n",
      "         306       0.88      0.98      0.93        60\n",
      "         307       0.00      0.00      0.00         1\n",
      "         308       1.00      0.80      0.89         5\n",
      "         309       0.80      0.95      0.87        59\n",
      "         311       0.80      1.00      0.89        12\n",
      "         313       1.00      0.71      0.83         7\n",
      "         314       0.96      0.92      0.94        26\n",
      "         315       0.97      0.89      0.93        35\n",
      "         317       1.00      0.50      0.67         2\n",
      "         318       0.00      0.00      0.00         1\n",
      "         321       0.88      0.67      0.76        21\n",
      "         322       1.00      0.72      0.84        25\n",
      "         323       1.00      0.80      0.89         5\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      1.00      1.00       191\n",
      "         327       0.00      0.00      0.00         1\n",
      "         329       1.00      0.25      0.40         4\n",
      "         330       0.61      0.95      0.74        21\n",
      "         333       1.00      1.00      1.00        16\n",
      "         334       0.00      0.00      0.00         1\n",
      "         335       0.00      0.00      0.00         1\n",
      "         336       0.00      0.00      0.00         4\n",
      "         337       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         2\n",
      "         340       0.80      1.00      0.89         4\n",
      "         341       0.00      0.00      0.00         1\n",
      "         342       1.00      1.00      1.00        25\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.97      1.00      0.98        31\n",
      "         345       1.00      1.00      1.00         2\n",
      "         347       0.50      1.00      0.67         2\n",
      "         349       0.98      1.00      0.99        52\n",
      "         350       0.00      0.00      0.00         2\n",
      "         351       1.00      1.00      1.00         3\n",
      "         352       0.86      1.00      0.92         6\n",
      "         353       0.00      0.00      0.00         1\n",
      "         355       0.94      1.00      0.97        88\n",
      "         356       1.00      0.88      0.93         8\n",
      "         357       0.67      0.95      0.78        80\n",
      "         358       0.00      0.00      0.00         1\n",
      "         359       0.00      0.00      0.00         6\n",
      "         360       0.75      1.00      0.86         3\n",
      "         361       0.95      0.95      0.95        21\n",
      "         362       0.50      0.50      0.50         4\n",
      "         364       0.38      0.45      0.42        11\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       1.00      0.50      0.67         2\n",
      "         368       1.00      1.00      1.00         8\n",
      "         369       1.00      1.00      1.00        13\n",
      "         370       0.50      0.50      0.50         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.92      1.00      0.96        33\n",
      "         373       0.93      1.00      0.96        13\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       1.00      1.00      1.00        54\n",
      "         378       1.00      1.00      1.00         2\n",
      "         379       0.78      1.00      0.88         7\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.93      0.98      0.95       102\n",
      "         382       1.00      0.62      0.76        13\n",
      "         383       0.88      1.00      0.93        14\n",
      "         384       1.00      0.89      0.94        35\n",
      "         385       0.00      0.00      0.00         0\n",
      "         387       0.96      1.00      0.98        47\n",
      "         389       1.00      0.29      0.44         7\n",
      "         390       0.80      1.00      0.89         4\n",
      "         391       0.00      0.00      0.00         2\n",
      "         392       0.60      0.92      0.73        13\n",
      "         393       0.59      1.00      0.74        10\n",
      "         394       0.00      0.00      0.00         1\n",
      "         395       0.33      0.25      0.29         8\n",
      "         396       1.00      1.00      1.00         6\n",
      "         397       0.00      0.00      0.00         1\n",
      "         398       0.50      0.45      0.48        11\n",
      "         399       0.67      1.00      0.80         4\n",
      "         400       0.00      0.00      0.00         1\n",
      "         401       1.00      0.86      0.92         7\n",
      "         402       1.00      1.00      1.00         3\n",
      "         403       1.00      1.00      1.00        15\n",
      "         404       0.98      1.00      0.99       213\n",
      "         405       1.00      1.00      1.00        19\n",
      "         408       1.00      1.00      1.00        31\n",
      "         409       0.75      1.00      0.86         3\n",
      "         410       1.00      1.00      1.00         2\n",
      "         411       1.00      1.00      1.00         8\n",
      "         412       1.00      0.12      0.22         8\n",
      "         413       0.83      0.83      0.83         6\n",
      "         415       0.83      0.97      0.90        31\n",
      "         416       0.89      1.00      0.94        25\n",
      "         417       0.79      1.00      0.88        15\n",
      "         419       0.00      0.00      0.00         2\n",
      "         421       0.86      0.75      0.80        16\n",
      "         422       0.93      0.93      0.93        15\n",
      "         425       0.00      0.00      0.00         2\n",
      "         426       1.00      0.50      0.67         2\n",
      "         427       1.00      1.00      1.00         2\n",
      "         429       1.00      0.78      0.88         9\n",
      "         430       1.00      1.00      1.00         9\n",
      "         431       0.00      0.00      0.00         1\n",
      "         432       0.52      0.81      0.63        21\n",
      "         433       1.00      1.00      1.00         1\n",
      "         434       1.00      0.92      0.96        12\n",
      "         435       1.00      0.62      0.77         8\n",
      "         437       1.00      0.40      0.57         5\n",
      "         439       0.93      0.72      0.81        39\n",
      "         440       0.77      1.00      0.87        53\n",
      "         442       0.82      0.93      0.87       134\n",
      "         443       0.76      1.00      0.87        13\n",
      "         444       0.00      0.00      0.00         7\n",
      "         445       0.64      0.88      0.74         8\n",
      "         446       1.00      1.00      1.00         3\n",
      "         447       0.00      0.00      0.00         6\n",
      "         448       0.90      0.75      0.82        12\n",
      "         449       0.79      0.88      0.84        26\n",
      "         450       0.82      1.00      0.90         9\n",
      "         451       0.95      1.00      0.97        18\n",
      "         452       0.67      0.67      0.67         3\n",
      "         453       1.00      1.00      1.00         2\n",
      "         454       0.46      1.00      0.63         6\n",
      "         455       0.97      0.96      0.96        67\n",
      "         456       1.00      1.00      1.00         2\n",
      "         458       0.00      0.00      0.00         1\n",
      "         459       0.83      1.00      0.91         5\n",
      "         461       1.00      1.00      1.00        34\n",
      "         462       0.94      0.96      0.95       170\n",
      "         465       0.90      0.92      0.91        65\n",
      "         466       0.89      1.00      0.94         8\n",
      "         467       0.89      0.57      0.70        14\n",
      "         470       0.00      0.00      0.00         2\n",
      "         471       1.00      0.33      0.50         3\n",
      "         472       1.00      0.38      0.55         8\n",
      "         473       1.00      0.75      0.86         8\n",
      "         474       0.82      0.82      0.82        28\n",
      "         475       1.00      1.00      1.00        19\n",
      "         478       0.40      0.40      0.40         5\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       1.00      1.00      1.00         1\n",
      "         482       1.00      0.60      0.75        10\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         1\n",
      "         485       1.00      0.73      0.85        15\n",
      "         486       0.96      0.96      0.96        27\n",
      "         488       0.00      0.00      0.00         1\n",
      "         489       0.83      0.96      0.89        25\n",
      "         490       1.00      1.00      1.00         4\n",
      "         492       1.00      1.00      1.00         1\n",
      "         493       1.00      1.00      1.00        17\n",
      "         494       0.00      0.00      0.00         1\n",
      "         495       1.00      0.92      0.96        24\n",
      "         496       1.00      1.00      1.00        79\n",
      "         498       1.00      0.86      0.93        22\n",
      "         499       0.92      1.00      0.96        33\n",
      "         500       1.00      0.75      0.86         4\n",
      "         501       0.00      0.00      0.00        12\n",
      "         502       0.00      0.00      0.00         1\n",
      "         503       1.00      1.00      1.00         1\n",
      "         504       0.50      1.00      0.67         3\n",
      "         506       1.00      0.67      0.80         3\n",
      "         507       0.76      0.96      0.85        26\n",
      "         508       1.00      0.88      0.93         8\n",
      "         509       1.00      0.95      0.97        55\n",
      "         510       0.00      0.00      0.00         2\n",
      "         511       1.00      0.60      0.75         5\n",
      "         512       0.73      1.00      0.84         8\n",
      "         513       0.64      0.69      0.67        13\n",
      "         514       0.86      1.00      0.93        38\n",
      "         516       0.00      0.00      0.00         1\n",
      "         517       1.00      1.00      1.00        10\n",
      "         519       1.00      0.60      0.75         5\n",
      "         520       1.00      1.00      1.00         3\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       1.00      0.60      0.75        10\n",
      "         523       1.00      1.00      1.00        28\n",
      "         524       1.00      1.00      1.00         2\n",
      "         525       0.00      0.00      0.00         1\n",
      "         526       0.93      1.00      0.96        25\n",
      "         527       1.00      1.00      1.00         2\n",
      "         528       1.00      0.50      0.67         2\n",
      "         529       0.00      0.00      0.00         1\n",
      "         531       1.00      0.67      0.80         3\n",
      "         532       0.95      1.00      0.97        18\n",
      "         533       0.75      1.00      0.86         3\n",
      "         535       1.00      0.90      0.95        10\n",
      "         536       1.00      0.18      0.31        11\n",
      "         537       0.80      0.50      0.62        16\n",
      "         538       0.00      0.00      0.00         9\n",
      "         539       1.00      1.00      1.00         3\n",
      "         540       0.80      0.80      0.80         5\n",
      "         541       1.00      1.00      1.00         6\n",
      "         543       0.98      1.00      0.99        55\n",
      "         544       1.00      1.00      1.00         3\n",
      "         545       0.84      1.00      0.91        16\n",
      "         546       0.00      0.00      0.00         1\n",
      "         547       1.00      0.97      0.99        37\n",
      "         548       0.64      0.88      0.74         8\n",
      "         549       0.00      0.00      0.00         9\n",
      "         550       0.91      0.97      0.94       403\n",
      "         551       1.00      1.00      1.00         7\n",
      "         552       0.00      0.00      0.00         1\n",
      "         554       1.00      1.00      1.00         5\n",
      "         558       0.96      0.94      0.95      1254\n",
      "\n",
      "    accuracy                           0.90     10009\n",
      "   macro avg       0.63      0.60      0.60     10009\n",
      "weighted avg       0.88      0.90      0.88     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 3\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.85\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.75\n",
    "    else:\n",
    "        return lr * 0.7\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "model_3 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_2.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_3 = model_3.fit(X_train_combined_2, y_train_2, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_2, y_test_2),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_3, 'model_3_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_3, accuracy_3 = model_3.evaluate(X_test_combined_2, y_test_2)\n",
    "print(\"Accuracy for Stage 3 model:\", accuracy_3)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_3 = model_3.predict(X_test_combined_2)\n",
    "y_pred_3 = np.argmax(y_pred_probabilities_3, axis=1)\n",
    "report_3 = classification_report(y_test_2, y_pred_3)\n",
    "print(\"Classification Report for Stage 3 model:\")\n",
    "print(report_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 4 model...\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 2.7567 - accuracy: 0.5621 - val_loss: 1.3178 - val_accuracy: 0.7329 - lr: 9.4000e-04\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.4781 - accuracy: 0.6983 - val_loss: 0.8923 - val_accuracy: 0.8070 - lr: 8.8360e-04\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.1642 - accuracy: 0.7439 - val_loss: 0.6919 - val_accuracy: 0.8489 - lr: 8.3058e-04\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.9945 - accuracy: 0.7685 - val_loss: 0.6251 - val_accuracy: 0.8496 - lr: 7.8075e-04\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.8986 - accuracy: 0.7845 - val_loss: 0.5608 - val_accuracy: 0.8665 - lr: 7.0267e-04\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.8236 - accuracy: 0.7990 - val_loss: 0.5089 - val_accuracy: 0.8765 - lr: 6.3241e-04\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7511 - accuracy: 0.8126 - val_loss: 0.4831 - val_accuracy: 0.8833 - lr: 5.6917e-04\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7028 - accuracy: 0.8208 - val_loss: 0.4633 - val_accuracy: 0.8879 - lr: 5.1225e-04\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6663 - accuracy: 0.8283 - val_loss: 0.4433 - val_accuracy: 0.8946 - lr: 4.0980e-04\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6336 - accuracy: 0.8374 - val_loss: 0.4314 - val_accuracy: 0.8982 - lr: 3.2784e-04\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6066 - accuracy: 0.8418 - val_loss: 0.4196 - val_accuracy: 0.9027 - lr: 2.6227e-04\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5781 - accuracy: 0.8482 - val_loss: 0.4080 - val_accuracy: 0.9069 - lr: 2.0982e-04\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5554 - accuracy: 0.8541 - val_loss: 0.4086 - val_accuracy: 0.9052 - lr: 1.6785e-04\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5375 - accuracy: 0.8585 - val_loss: 0.4023 - val_accuracy: 0.9079 - lr: 1.3428e-04\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5303 - accuracy: 0.8602 - val_loss: 0.4011 - val_accuracy: 0.9060 - lr: 1.0743e-04\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5203 - accuracy: 0.8627 - val_loss: 0.3976 - val_accuracy: 0.9099 - lr: 8.5941e-05\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5162 - accuracy: 0.8633 - val_loss: 0.3975 - val_accuracy: 0.9104 - lr: 6.0159e-05\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5063 - accuracy: 0.8662 - val_loss: 0.3954 - val_accuracy: 0.9111 - lr: 4.2111e-05\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5057 - accuracy: 0.8666 - val_loss: 0.3927 - val_accuracy: 0.9109 - lr: 2.9478e-05\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4940 - accuracy: 0.8684 - val_loss: 0.3936 - val_accuracy: 0.9116 - lr: 1.7687e-05\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4959 - accuracy: 0.8678 - val_loss: 0.3928 - val_accuracy: 0.9115 - lr: 1.0612e-05\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5004 - accuracy: 0.8655 - val_loss: 0.3916 - val_accuracy: 0.9116 - lr: 6.3672e-06\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4914 - accuracy: 0.8688 - val_loss: 0.3933 - val_accuracy: 0.9108 - lr: 3.8203e-06\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5025 - accuracy: 0.8675 - val_loss: 0.3923 - val_accuracy: 0.9115 - lr: 2.2922e-06\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4953 - accuracy: 0.8669 - val_loss: 0.3912 - val_accuracy: 0.9118 - lr: 1.3753e-06\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4951 - accuracy: 0.8678 - val_loss: 0.3930 - val_accuracy: 0.9111 - lr: 8.2519e-07\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4951 - accuracy: 0.8677 - val_loss: 0.3921 - val_accuracy: 0.9114 - lr: 4.9511e-07\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4994 - accuracy: 0.8672 - val_loss: 0.3938 - val_accuracy: 0.9111 - lr: 2.9707e-07\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4988 - accuracy: 0.8685 - val_loss: 0.3922 - val_accuracy: 0.9115 - lr: 1.7824e-07\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4932 - accuracy: 0.8680 - val_loss: 0.3932 - val_accuracy: 0.9108 - lr: 1.0694e-07\n",
      "  1/313 [..............................] - ETA: 7s - loss: 1.0816 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\2139566038.py:42: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_4, 'model_4_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3912 - accuracy: 0.9118\n",
      "Accuracy for Stage 4 model: 0.9117794036865234\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "Classification Report for Stage 4 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "           6       1.00      1.00      1.00        56\n",
      "           8       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.31      1.00      0.47         8\n",
      "          19       0.67      0.33      0.44         6\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.50      0.50      0.50         4\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       0.88      1.00      0.94        15\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.69      0.96      0.80        23\n",
      "          31       1.00      1.00      1.00        22\n",
      "          33       0.97      0.97      0.97        33\n",
      "          34       0.95      0.91      0.93        22\n",
      "          35       0.75      0.82      0.78        11\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       0.67      0.44      0.53         9\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       1.00      0.62      0.77         8\n",
      "          44       0.67      0.40      0.50         5\n",
      "          45       0.92      1.00      0.96        24\n",
      "          46       0.72      1.00      0.84        28\n",
      "          47       1.00      1.00      1.00         3\n",
      "          48       0.50      1.00      0.67         1\n",
      "          49       0.00      0.00      0.00         3\n",
      "          51       1.00      0.67      0.80         9\n",
      "          52       0.92      1.00      0.96        12\n",
      "          54       0.90      1.00      0.95        26\n",
      "          55       0.85      1.00      0.92        11\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       1.00      0.33      0.50         3\n",
      "          60       1.00      0.67      0.80         3\n",
      "          61       1.00      1.00      1.00         4\n",
      "          63       0.00      0.00      0.00         3\n",
      "          64       0.00      0.00      0.00         4\n",
      "          65       0.00      0.00      0.00         1\n",
      "          66       1.00      1.00      1.00        33\n",
      "          67       1.00      1.00      1.00       131\n",
      "          68       0.75      1.00      0.86         6\n",
      "          69       1.00      0.90      0.95        10\n",
      "          70       0.00      0.00      0.00         3\n",
      "          71       0.83      0.62      0.71         8\n",
      "          72       1.00      0.50      0.67         2\n",
      "          76       1.00      1.00      1.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.78      1.00      0.88        39\n",
      "          81       1.00      0.96      0.98        47\n",
      "          82       0.00      0.00      0.00         1\n",
      "          83       0.64      1.00      0.78         7\n",
      "          85       0.40      0.67      0.50         3\n",
      "          87       0.50      0.67      0.57         3\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         1\n",
      "          91       0.75      0.77      0.76        31\n",
      "          92       0.00      0.00      0.00         2\n",
      "          93       0.00      0.00      0.00         3\n",
      "          95       1.00      1.00      1.00         3\n",
      "          97       0.33      0.50      0.40         2\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       1.00      0.92      0.96        13\n",
      "         100       0.00      0.00      0.00         4\n",
      "         101       1.00      1.00      1.00         2\n",
      "         102       0.42      1.00      0.59         5\n",
      "         103       1.00      1.00      1.00         6\n",
      "         105       0.75      1.00      0.86         3\n",
      "         106       1.00      1.00      1.00       101\n",
      "         109       0.00      0.00      0.00         5\n",
      "         110       0.88      0.94      0.91        48\n",
      "         112       0.68      0.87      0.76        15\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       0.50      0.67      0.57         3\n",
      "         115       0.00      0.00      0.00         2\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.00      0.00      0.00         2\n",
      "         118       0.76      1.00      0.86        25\n",
      "         120       0.00      0.00      0.00         4\n",
      "         122       0.00      0.00      0.00         2\n",
      "         123       1.00      1.00      1.00        36\n",
      "         125       1.00      1.00      1.00         5\n",
      "         126       0.00      0.00      0.00         2\n",
      "         127       0.75      1.00      0.86         6\n",
      "         128       0.00      0.00      0.00         1\n",
      "         129       0.00      0.00      0.00         1\n",
      "         130       1.00      0.60      0.75         5\n",
      "         132       0.00      0.00      0.00         4\n",
      "         133       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       1.00      0.11      0.20         9\n",
      "         136       0.75      1.00      0.86         3\n",
      "         138       0.91      0.77      0.83        13\n",
      "         139       0.00      0.00      0.00         1\n",
      "         143       0.00      0.00      0.00         2\n",
      "         144       0.00      0.00      0.00         5\n",
      "         146       1.00      0.33      0.50         3\n",
      "         147       0.67      0.77      0.71        13\n",
      "         148       1.00      1.00      1.00         1\n",
      "         149       0.00      0.00      0.00         5\n",
      "         150       1.00      1.00      1.00         7\n",
      "         151       0.86      1.00      0.92        18\n",
      "         154       0.00      0.00      0.00         1\n",
      "         155       0.98      1.00      0.99        41\n",
      "         156       0.00      0.00      0.00         2\n",
      "         157       0.00      0.00      0.00         1\n",
      "         158       0.76      1.00      0.87        13\n",
      "         159       1.00      1.00      1.00         7\n",
      "         160       0.00      0.00      0.00         2\n",
      "         162       1.00      1.00      1.00       184\n",
      "         164       0.25      1.00      0.40         1\n",
      "         165       1.00      1.00      1.00        26\n",
      "         167       0.50      1.00      0.67         6\n",
      "         168       0.00      0.00      0.00         6\n",
      "         169       0.74      0.98      0.84        54\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.50      0.17      0.25         6\n",
      "         173       1.00      1.00      1.00        14\n",
      "         174       0.56      1.00      0.71         5\n",
      "         175       0.92      1.00      0.96       204\n",
      "         176       1.00      1.00      1.00         6\n",
      "         177       1.00      1.00      1.00         8\n",
      "         178       0.76      1.00      0.87        13\n",
      "         180       0.90      0.90      0.90        10\n",
      "         182       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         1\n",
      "         186       0.69      0.92      0.79        12\n",
      "         187       0.95      1.00      0.97        18\n",
      "         188       0.00      0.00      0.00         1\n",
      "         189       0.00      0.00      0.00         1\n",
      "         191       0.57      1.00      0.73         8\n",
      "         192       0.00      0.00      0.00         1\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       1.00      1.00      1.00         2\n",
      "         197       0.00      0.00      0.00         2\n",
      "         199       0.60      0.96      0.74        26\n",
      "         200       0.00      0.00      0.00         2\n",
      "         203       0.00      0.00      0.00         2\n",
      "         204       0.60      0.75      0.67         4\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       1.00      0.67      0.80         3\n",
      "         207       0.00      0.00      0.00         5\n",
      "         210       0.64      1.00      0.78        23\n",
      "         211       0.98      0.95      0.97        44\n",
      "         212       0.00      0.00      0.00         1\n",
      "         214       0.50      0.67      0.57         3\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       1.00      1.00      1.00        31\n",
      "         220       0.00      0.00      0.00         1\n",
      "         221       0.60      1.00      0.75         3\n",
      "         224       0.78      0.82      0.80        17\n",
      "         228       0.80      0.98      0.88        61\n",
      "         229       0.82      1.00      0.90        14\n",
      "         230       1.00      0.88      0.93         8\n",
      "         231       0.71      0.92      0.80        13\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       0.00      0.00      0.00         1\n",
      "         237       0.33      1.00      0.50         2\n",
      "         239       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         244       0.94      0.47      0.62        32\n",
      "         248       0.00      0.00      0.00         1\n",
      "         254       0.64      1.00      0.78         7\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.50      1.00      0.67         2\n",
      "         258       1.00      1.00      1.00         4\n",
      "         259       1.00      0.93      0.97        30\n",
      "         261       0.50      1.00      0.67         2\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.67      1.00      0.80         8\n",
      "         266       1.00      0.50      0.67         2\n",
      "         267       1.00      1.00      1.00        14\n",
      "         268       1.00      0.67      0.80         3\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.88      1.00      0.94        15\n",
      "         271       0.83      0.83      0.83        36\n",
      "         272       0.86      0.92      0.89        13\n",
      "         273       1.00      0.67      0.80         3\n",
      "         275       1.00      0.40      0.57         5\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.50      1.00      0.67         1\n",
      "         282       1.00      0.50      0.67         6\n",
      "         283       0.85      0.97      0.90        29\n",
      "         284       1.00      1.00      1.00         4\n",
      "         285       1.00      1.00      1.00         1\n",
      "         286       0.00      0.00      0.00         1\n",
      "         288       0.56      0.82      0.67        11\n",
      "         289       0.00      0.00      0.00         1\n",
      "         291       0.93      1.00      0.96        13\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.90      1.00      0.95        26\n",
      "         295       0.62      0.62      0.62         8\n",
      "         297       0.00      0.00      0.00         2\n",
      "         298       0.39      0.90      0.55        10\n",
      "         299       0.00      0.00      0.00         4\n",
      "         300       0.00      0.00      0.00         2\n",
      "         301       0.73      0.95      0.83        40\n",
      "         302       1.00      0.30      0.46        10\n",
      "         304       1.00      1.00      1.00        19\n",
      "         306       1.00      0.83      0.91         6\n",
      "         307       1.00      0.91      0.95        11\n",
      "         308       0.00      0.00      0.00         2\n",
      "         309       0.96      0.90      0.92        48\n",
      "         312       0.70      0.50      0.58        14\n",
      "         313       0.75      0.75      0.75         8\n",
      "         314       0.00      0.00      0.00         1\n",
      "         315       1.00      1.00      1.00        13\n",
      "         317       0.00      0.00      0.00         1\n",
      "         320       0.00      0.00      0.00         3\n",
      "         324       0.00      0.00      0.00         1\n",
      "         328       1.00      1.00      1.00         1\n",
      "         329       0.50      1.00      0.67         2\n",
      "         330       0.00      0.00      0.00         1\n",
      "         332       0.00      0.00      0.00         3\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.98      1.00      0.99        80\n",
      "         335       0.98      1.00      0.99        41\n",
      "         336       0.86      0.95      0.90        20\n",
      "         337       0.00      0.00      0.00         2\n",
      "         340       1.00      1.00      1.00         2\n",
      "         341       0.64      1.00      0.78         7\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       0.00      0.00      0.00         2\n",
      "         347       0.00      0.00      0.00         1\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       1.00      1.00      1.00        21\n",
      "         350       0.00      0.00      0.00         1\n",
      "         351       0.50      0.71      0.59         7\n",
      "         352       0.00      0.00      0.00         2\n",
      "         353       0.67      0.33      0.44         6\n",
      "         354       0.00      0.00      0.00         7\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       0.00      0.00      0.00         3\n",
      "         358       0.89      1.00      0.94         8\n",
      "         360       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         3\n",
      "         363       1.00      1.00      1.00         3\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         1\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.00      0.00      0.00         1\n",
      "         373       0.00      0.00      0.00         1\n",
      "         374       0.71      1.00      0.83         5\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       1.00      1.00      1.00         1\n",
      "         378       0.75      0.75      0.75         4\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.94      0.94      0.94        33\n",
      "         381       1.00      0.62      0.77        16\n",
      "         382       0.85      1.00      0.92        41\n",
      "         383       1.00      1.00      1.00         7\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       1.00      1.00      1.00         2\n",
      "         388       0.90      1.00      0.95         9\n",
      "         389       1.00      1.00      1.00         1\n",
      "         390       0.00      0.00      0.00         1\n",
      "         392       0.73      0.38      0.50        21\n",
      "         393       1.00      1.00      1.00         6\n",
      "         394       1.00      0.20      0.33         5\n",
      "         395       0.93      1.00      0.96        13\n",
      "         396       1.00      1.00      1.00         6\n",
      "         397       0.58      0.64      0.61        11\n",
      "         398       0.00      0.00      0.00         3\n",
      "         399       1.00      0.25      0.40         4\n",
      "         400       1.00      0.33      0.50         3\n",
      "         402       0.00      0.00      0.00         5\n",
      "         403       1.00      1.00      1.00         3\n",
      "         404       1.00      1.00      1.00        23\n",
      "         405       0.86      1.00      0.92         6\n",
      "         406       1.00      1.00      1.00        25\n",
      "         407       0.62      0.67      0.64        12\n",
      "         409       0.00      0.00      0.00         1\n",
      "         411       0.56      1.00      0.71         5\n",
      "         413       1.00      0.67      0.80         3\n",
      "         415       0.75      0.86      0.80         7\n",
      "         416       0.00      0.00      0.00         1\n",
      "         418       1.00      0.89      0.94         9\n",
      "         421       1.00      1.00      1.00         4\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       1.00      1.00      1.00         2\n",
      "         426       0.00      0.00      0.00         1\n",
      "         427       0.80      1.00      0.89         4\n",
      "         428       0.76      0.44      0.56        36\n",
      "         429       1.00      0.91      0.95        11\n",
      "         431       1.00      1.00      1.00         6\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.60      1.00      0.75         6\n",
      "         435       0.92      1.00      0.96        11\n",
      "         436       0.90      1.00      0.95         9\n",
      "         437       0.93      0.87      0.90        46\n",
      "         438       1.00      1.00      1.00         1\n",
      "         439       0.00      0.00      0.00         2\n",
      "         441       0.58      0.89      0.70        37\n",
      "         443       0.79      1.00      0.88        69\n",
      "         444       0.73      0.92      0.81        12\n",
      "         445       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         1\n",
      "         451       0.00      0.00      0.00         1\n",
      "         454       0.00      0.00      0.00         1\n",
      "         457       0.83      0.83      0.83         6\n",
      "         458       0.75      1.00      0.86         6\n",
      "         462       0.91      0.94      0.93        34\n",
      "         463       1.00      1.00      1.00        17\n",
      "         464       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         3\n",
      "         467       0.00      0.00      0.00         1\n",
      "         468       1.00      1.00      1.00         1\n",
      "         469       0.00      0.00      0.00         2\n",
      "         470       1.00      0.50      0.67         2\n",
      "         471       0.75      1.00      0.86         6\n",
      "         474       0.00      0.00      0.00         3\n",
      "         475       1.00      1.00      1.00        21\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       0.67      0.67      0.67         3\n",
      "         478       1.00      0.78      0.88         9\n",
      "         479       1.00      1.00      1.00         7\n",
      "         480       0.56      1.00      0.72        14\n",
      "         481       0.85      0.94      0.89        65\n",
      "         483       1.00      1.00      1.00         4\n",
      "         484       1.00      1.00      1.00         3\n",
      "         485       1.00      1.00      1.00         5\n",
      "         486       0.00      0.00      0.00         1\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         2\n",
      "         492       0.94      0.96      0.95        91\n",
      "         493       0.00      0.00      0.00         1\n",
      "         494       1.00      0.87      0.93        38\n",
      "         495       0.00      0.00      0.00         4\n",
      "         496       0.67      0.50      0.57         4\n",
      "         498       1.00      1.00      1.00         5\n",
      "         499       0.00      0.00      0.00         2\n",
      "         501       0.47      1.00      0.64         7\n",
      "         502       0.00      0.00      0.00         2\n",
      "         503       0.67      1.00      0.80         4\n",
      "         504       1.00      1.00      1.00         1\n",
      "         505       0.00      0.00      0.00         1\n",
      "         506       1.00      1.00      1.00        47\n",
      "         507       0.99      1.00      1.00       110\n",
      "         509       1.00      0.95      0.97        19\n",
      "         510       0.79      0.92      0.85        12\n",
      "         512       0.76      1.00      0.86        19\n",
      "         513       0.00      0.00      0.00         2\n",
      "         515       1.00      1.00      1.00        19\n",
      "         517       1.00      1.00      1.00         2\n",
      "         519       0.92      1.00      0.96        11\n",
      "         520       0.00      0.00      0.00         1\n",
      "         521       1.00      0.76      0.86        29\n",
      "         522       0.00      0.00      0.00         5\n",
      "         523       0.44      1.00      0.62         4\n",
      "         524       0.00      0.00      0.00         1\n",
      "         525       1.00      0.50      0.67        10\n",
      "         527       0.00      0.00      0.00         1\n",
      "         528       0.00      0.00      0.00         1\n",
      "         531       1.00      1.00      1.00        28\n",
      "         532       0.00      0.00      0.00         1\n",
      "         533       1.00      1.00      1.00         1\n",
      "         536       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         2\n",
      "         540       1.00      0.88      0.93         8\n",
      "         541       0.00      0.00      0.00         2\n",
      "         542       0.62      1.00      0.77        10\n",
      "         543       0.75      1.00      0.86         6\n",
      "         545       1.00      1.00      1.00         9\n",
      "         546       1.00      0.11      0.20         9\n",
      "         547       0.87      0.96      0.91        27\n",
      "         548       0.59      0.87      0.70        15\n",
      "         549       0.00      0.00      0.00         1\n",
      "         550       0.79      1.00      0.88        23\n",
      "         552       0.00      0.00      0.00         1\n",
      "         553       1.00      1.00      1.00         5\n",
      "         554       0.33      1.00      0.50         1\n",
      "         555       0.69      0.93      0.79        29\n",
      "         556       1.00      0.33      0.50         3\n",
      "         557       0.00      0.00      0.00         1\n",
      "         558       0.40      1.00      0.57         2\n",
      "         560       0.00      0.00      0.00         2\n",
      "         561       0.00      0.00      0.00         1\n",
      "         562       0.00      0.00      0.00         4\n",
      "         563       0.00      0.00      0.00         5\n",
      "         564       0.90      0.82      0.86        11\n",
      "         567       0.67      1.00      0.80         2\n",
      "         568       1.00      0.67      0.80         9\n",
      "         570       1.00      1.00      1.00         1\n",
      "         571       1.00      1.00      1.00         2\n",
      "         572       1.00      1.00      1.00        51\n",
      "         573       1.00      1.00      1.00        16\n",
      "         575       0.50      0.50      0.50         2\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       1.00      1.00      1.00        18\n",
      "         578       1.00      1.00      1.00         1\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.80      0.67      0.73        12\n",
      "         582       0.87      1.00      0.93        20\n",
      "         583       1.00      0.60      0.75        10\n",
      "         584       0.71      0.80      0.75        15\n",
      "         585       0.64      1.00      0.78        16\n",
      "         586       0.00      0.00      0.00         1\n",
      "         587       0.50      0.40      0.44         5\n",
      "         588       1.00      0.33      0.50         3\n",
      "         589       0.83      0.83      0.83         6\n",
      "         590       1.00      0.20      0.33        10\n",
      "         591       1.00      1.00      1.00         5\n",
      "         592       0.00      0.00      0.00         2\n",
      "         593       0.67      0.50      0.57         8\n",
      "         594       0.33      0.25      0.29         4\n",
      "         595       0.00      0.00      0.00         1\n",
      "         596       0.00      0.00      0.00         1\n",
      "         597       0.33      0.50      0.40         2\n",
      "         598       1.00      0.75      0.86         4\n",
      "         599       1.00      0.67      0.80        12\n",
      "         601       0.61      0.85      0.71        13\n",
      "         602       0.00      0.00      0.00         1\n",
      "         603       0.00      0.00      0.00         1\n",
      "         604       0.00      0.00      0.00         1\n",
      "         608       1.00      0.25      0.40         4\n",
      "         609       1.00      0.90      0.95        10\n",
      "         610       0.93      0.88      0.90        16\n",
      "         612       0.81      1.00      0.90        13\n",
      "         613       0.50      1.00      0.67         1\n",
      "         614       0.79      1.00      0.88        23\n",
      "         616       0.80      0.92      0.86        36\n",
      "         617       0.92      0.97      0.94        91\n",
      "         619       0.00      0.00      0.00         2\n",
      "         621       1.00      1.00      1.00        11\n",
      "         622       1.00      1.00      1.00        29\n",
      "         623       1.00      0.88      0.93        16\n",
      "         624       0.00      0.00      0.00         3\n",
      "         625       0.76      0.94      0.84        17\n",
      "         626       0.92      0.93      0.93        61\n",
      "         627       0.88      0.64      0.74        11\n",
      "         628       0.00      0.00      0.00         1\n",
      "         629       0.00      0.00      0.00         8\n",
      "         630       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         2\n",
      "         632       0.00      0.00      0.00         1\n",
      "         634       0.80      0.80      0.80         5\n",
      "         635       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         1\n",
      "         638       0.69      0.85      0.76        13\n",
      "         639       0.96      1.00      0.98        22\n",
      "         641       0.88      1.00      0.93        28\n",
      "         642       0.40      1.00      0.57         4\n",
      "         643       0.67      1.00      0.80        10\n",
      "         644       1.00      0.67      0.80         3\n",
      "         645       1.00      1.00      1.00         1\n",
      "         646       0.88      0.78      0.82         9\n",
      "         647       1.00      1.00      1.00         2\n",
      "         648       0.00      0.00      0.00         1\n",
      "         649       0.00      0.00      0.00         1\n",
      "         651       1.00      1.00      1.00        10\n",
      "         652       1.00      0.80      0.89         5\n",
      "         653       1.00      1.00      1.00        34\n",
      "         654       1.00      0.50      0.67         4\n",
      "         655       1.00      1.00      1.00        15\n",
      "         658       0.00      0.00      0.00         1\n",
      "         659       0.00      0.00      0.00         1\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         1\n",
      "         666       0.67      1.00      0.80         2\n",
      "         668       0.90      0.79      0.84        24\n",
      "         669       0.00      0.00      0.00         1\n",
      "         670       0.82      1.00      0.90         9\n",
      "         671       0.00      0.00      0.00         2\n",
      "         673       0.00      0.00      0.00         3\n",
      "         674       1.00      1.00      1.00         5\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.83      1.00      0.90        19\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         1\n",
      "         683       0.25      0.50      0.33         2\n",
      "         684       0.00      0.00      0.00         2\n",
      "         685       0.00      0.00      0.00         1\n",
      "         686       1.00      1.00      1.00         2\n",
      "         688       0.00      0.00      0.00         3\n",
      "         690       0.00      0.00      0.00         1\n",
      "         691       0.62      1.00      0.76         8\n",
      "         692       0.80      1.00      0.89         4\n",
      "         694       0.50      1.00      0.67         3\n",
      "         695       0.00      0.00      0.00         0\n",
      "         696       0.00      0.00      0.00         2\n",
      "         697       0.00      0.00      0.00         1\n",
      "         698       1.00      1.00      1.00         2\n",
      "         699       0.00      0.00      0.00         2\n",
      "         702       0.00      0.00      0.00         3\n",
      "         704       0.00      0.00      0.00         2\n",
      "         707       0.91      0.99      0.95       358\n",
      "         709       0.00      0.00      0.00         3\n",
      "         710       0.00      0.00      0.00         1\n",
      "         711       1.00      0.62      0.76        13\n",
      "         713       0.00      0.00      0.00         1\n",
      "         715       1.00      0.50      0.67         2\n",
      "         716       0.00      0.00      0.00         1\n",
      "         717       0.50      1.00      0.67         2\n",
      "         719       0.97      0.97      0.97      4512\n",
      "\n",
      "    accuracy                           0.91     10009\n",
      "   macro avg       0.50      0.52      0.49     10009\n",
      "weighted avg       0.89      0.91      0.90     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 4\n",
    "print(\"Running Stage 4 model...\")\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_4 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_3.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_4.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_4 = model_4.fit(X_train_combined_3, y_train_3, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_3, y_test_3),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_4, 'model_4_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_4, accuracy_4 = model_4.evaluate(X_test_combined_3, y_test_3)\n",
    "print(\"Accuracy for Stage 4 model:\", accuracy_4)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_4 = model_4.predict(X_test_combined_3)\n",
    "y_pred_4 = np.argmax(y_pred_probabilities_4, axis=1)\n",
    "report_4 = classification_report(y_test_3, y_pred_4)\n",
    "print(\"Classification Report for Stage 4 model:\")\n",
    "print(report_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 5 model...\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.7161 - accuracy: 0.7560 - val_loss: 0.7289 - val_accuracy: 0.8572 - lr: 9.4000e-04\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7660 - accuracy: 0.8524 - val_loss: 0.5045 - val_accuracy: 0.8915 - lr: 8.8360e-04\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6383 - accuracy: 0.8648 - val_loss: 0.3853 - val_accuracy: 0.9058 - lr: 8.3058e-04\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5418 - accuracy: 0.8772 - val_loss: 0.3362 - val_accuracy: 0.9108 - lr: 7.8075e-04\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4812 - accuracy: 0.8842 - val_loss: 0.3017 - val_accuracy: 0.9208 - lr: 7.0267e-04\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4463 - accuracy: 0.8886 - val_loss: 0.2824 - val_accuracy: 0.9253 - lr: 6.3241e-04\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4052 - accuracy: 0.8964 - val_loss: 0.2629 - val_accuracy: 0.9290 - lr: 5.6917e-04\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3815 - accuracy: 0.8986 - val_loss: 0.2508 - val_accuracy: 0.9335 - lr: 5.1225e-04\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3624 - accuracy: 0.9050 - val_loss: 0.2438 - val_accuracy: 0.9304 - lr: 4.0980e-04\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3453 - accuracy: 0.9088 - val_loss: 0.2372 - val_accuracy: 0.9348 - lr: 3.2784e-04\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3300 - accuracy: 0.9103 - val_loss: 0.2282 - val_accuracy: 0.9371 - lr: 2.6227e-04\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3202 - accuracy: 0.9140 - val_loss: 0.2248 - val_accuracy: 0.9372 - lr: 2.0982e-04\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3087 - accuracy: 0.9157 - val_loss: 0.2193 - val_accuracy: 0.9400 - lr: 1.6785e-04\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3029 - accuracy: 0.9183 - val_loss: 0.2159 - val_accuracy: 0.9394 - lr: 1.3428e-04\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2969 - accuracy: 0.9176 - val_loss: 0.2159 - val_accuracy: 0.9400 - lr: 1.0743e-04\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2956 - accuracy: 0.9191 - val_loss: 0.2145 - val_accuracy: 0.9402 - lr: 8.5941e-05\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2939 - accuracy: 0.9179 - val_loss: 0.2140 - val_accuracy: 0.9400 - lr: 6.0159e-05\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2890 - accuracy: 0.9199 - val_loss: 0.2126 - val_accuracy: 0.9410 - lr: 4.2111e-05\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2871 - accuracy: 0.9195 - val_loss: 0.2125 - val_accuracy: 0.9411 - lr: 2.9478e-05\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2884 - accuracy: 0.9198 - val_loss: 0.2122 - val_accuracy: 0.9408 - lr: 1.7687e-05\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2829 - accuracy: 0.9224 - val_loss: 0.2116 - val_accuracy: 0.9409 - lr: 1.0612e-05\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2823 - accuracy: 0.9210 - val_loss: 0.2116 - val_accuracy: 0.9408 - lr: 6.3672e-06\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2786 - accuracy: 0.9220 - val_loss: 0.2118 - val_accuracy: 0.9412 - lr: 3.8203e-06\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2826 - accuracy: 0.9214 - val_loss: 0.2113 - val_accuracy: 0.9403 - lr: 2.2922e-06\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2821 - accuracy: 0.9205 - val_loss: 0.2113 - val_accuracy: 0.9414 - lr: 1.3753e-06\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2767 - accuracy: 0.9245 - val_loss: 0.2119 - val_accuracy: 0.9410 - lr: 8.2519e-07\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2828 - accuracy: 0.9214 - val_loss: 0.2116 - val_accuracy: 0.9407 - lr: 4.9511e-07\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2817 - accuracy: 0.9209 - val_loss: 0.2109 - val_accuracy: 0.9412 - lr: 2.9707e-07\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2833 - accuracy: 0.9205 - val_loss: 0.2116 - val_accuracy: 0.9409 - lr: 1.7824e-07\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2832 - accuracy: 0.9214 - val_loss: 0.2115 - val_accuracy: 0.9409 - lr: 1.0694e-07\n",
      "  1/313 [..............................] - ETA: 6s - loss: 0.1192 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_21692\\3460058163.py:40: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_5, 'model_5_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9409\n",
      "Accuracy for Stage 5 model: 0.9408532381057739\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 5 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           2       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.81      0.94      0.87        18\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       1.00      0.25      0.40         4\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       0.78      1.00      0.88        71\n",
      "          14       1.00      0.80      0.89         5\n",
      "          15       0.95      1.00      0.97        18\n",
      "          16       0.00      0.00      0.00         4\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.50      1.00      0.67         1\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.96      0.72      0.82        32\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         2\n",
      "          31       0.62      0.83      0.71         6\n",
      "          32       0.67      0.67      0.67         9\n",
      "          33       0.00      0.00      0.00         4\n",
      "          34       0.62      1.00      0.77        10\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         1\n",
      "          37       0.68      0.88      0.77        17\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.75      0.91      0.82        65\n",
      "          40       1.00      1.00      1.00        12\n",
      "          41       0.00      0.00      0.00         1\n",
      "          43       0.46      1.00      0.63         6\n",
      "          44       0.67      0.40      0.50         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         1\n",
      "          50       0.92      0.92      0.92        13\n",
      "          51       0.00      0.00      0.00         2\n",
      "          53       0.71      0.71      0.71         7\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.46      0.86      0.60        14\n",
      "          58       0.86      0.99      0.92        88\n",
      "          59       0.50      1.00      0.67         1\n",
      "          60       0.00      0.00      0.00         8\n",
      "          61       1.00      1.00      1.00         3\n",
      "          63       0.50      0.86      0.63         7\n",
      "          64       1.00      0.64      0.78        11\n",
      "          66       0.00      0.00      0.00         5\n",
      "          69       1.00      1.00      1.00        14\n",
      "          70       0.96      0.99      0.97       140\n",
      "          71       0.55      0.85      0.67        13\n",
      "          73       0.84      0.93      0.88        28\n",
      "          74       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          76       0.00      0.00      0.00        13\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.72      1.00      0.84        13\n",
      "          79       1.00      0.50      0.67         4\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       0.43      0.86      0.57         7\n",
      "          83       0.62      1.00      0.77         5\n",
      "          84       0.00      0.00      0.00         3\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         9\n",
      "          87       0.00      0.00      0.00         1\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.80      1.00      0.89        16\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       0.98      0.98      0.98        54\n",
      "          92       0.00      0.00      0.00         6\n",
      "          93       0.67      1.00      0.80         6\n",
      "          94       0.00      0.00      0.00         3\n",
      "          95       0.00      0.00      0.00         1\n",
      "          96       0.00      0.00      0.00         5\n",
      "          99       0.90      1.00      0.95         9\n",
      "         100       0.72      1.00      0.84        13\n",
      "         101       0.00      0.00      0.00         1\n",
      "         103       0.00      0.00      0.00         1\n",
      "         104       0.00      0.00      0.00         5\n",
      "         106       1.00      0.20      0.33         5\n",
      "         107       0.00      0.00      0.00         2\n",
      "         108       0.45      1.00      0.62        19\n",
      "         109       0.80      1.00      0.89         4\n",
      "         111       0.60      0.60      0.60         5\n",
      "         112       0.00      0.00      0.00         2\n",
      "         113       0.39      0.88      0.54         8\n",
      "         114       0.96      1.00      0.98        54\n",
      "         115       0.00      0.00      0.00         3\n",
      "         117       1.00      0.75      0.86        12\n",
      "         118       1.00      0.10      0.18        10\n",
      "         119       0.61      1.00      0.76        11\n",
      "         120       0.00      0.00      0.00         6\n",
      "         121       1.00      0.50      0.67         8\n",
      "         122       0.10      0.50      0.17         2\n",
      "         124       0.00      0.00      0.00         2\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         6\n",
      "         127       0.71      0.83      0.77         6\n",
      "         128       0.70      0.95      0.81        22\n",
      "         130       0.80      0.50      0.62        16\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.70      0.47      0.56        15\n",
      "         134       0.56      1.00      0.71         5\n",
      "         135       0.00      0.00      0.00         7\n",
      "         138       0.00      0.00      0.00         1\n",
      "         139       0.00      0.00      0.00         4\n",
      "         141       1.00      0.78      0.88         9\n",
      "         143       0.94      0.94      0.94        17\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.91      1.00      0.95        10\n",
      "         147       0.86      1.00      0.92         6\n",
      "         148       0.00      0.00      0.00         1\n",
      "         149       0.00      0.00      0.00         1\n",
      "         150       0.36      1.00      0.53         5\n",
      "         151       0.00      0.00      0.00         9\n",
      "         153       0.91      1.00      0.95        30\n",
      "         155       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.82      1.00      0.90         9\n",
      "         159       0.00      0.00      0.00         1\n",
      "         160       0.56      0.82      0.67        11\n",
      "         162       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.75      1.00      0.86        12\n",
      "         166       0.00      0.00      0.00         1\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.50      0.57      0.53         7\n",
      "         169       1.00      1.00      1.00         5\n",
      "         170       1.00      1.00      1.00         2\n",
      "         171       0.00      0.00      0.00         1\n",
      "         173       0.11      0.33      0.17         3\n",
      "         174       0.25      0.50      0.33         2\n",
      "         175       0.87      1.00      0.93        13\n",
      "         177       0.91      1.00      0.95        10\n",
      "         178       0.60      1.00      0.75         9\n",
      "         180       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.00      0.00      0.00         4\n",
      "         186       1.00      1.00      1.00         1\n",
      "         189       1.00      1.00      1.00         2\n",
      "         190       0.00      0.00      0.00         2\n",
      "         191       0.00      0.00      0.00         2\n",
      "         192       0.50      1.00      0.67         2\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       0.86      1.00      0.92         6\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.94      1.00      0.97        15\n",
      "         197       1.00      0.33      0.50         3\n",
      "         198       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         200       0.83      1.00      0.91        10\n",
      "         201       0.38      1.00      0.56        20\n",
      "         202       0.00      0.00      0.00         2\n",
      "         203       1.00      0.38      0.55         8\n",
      "         204       0.83      1.00      0.91        29\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         2\n",
      "         208       0.44      0.67      0.53         6\n",
      "         210       0.81      1.00      0.90        13\n",
      "         211       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00        14\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       0.45      1.00      0.62        15\n",
      "         216       1.00      0.11      0.20        18\n",
      "         217       0.45      1.00      0.62         5\n",
      "         218       1.00      1.00      1.00         1\n",
      "         219       0.86      1.00      0.92        42\n",
      "         220       0.00      0.00      0.00         3\n",
      "         221       1.00      0.50      0.67         2\n",
      "         222       1.00      0.25      0.40         4\n",
      "         223       0.00      0.00      0.00         3\n",
      "         224       0.00      0.00      0.00         2\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       1.00      1.00      1.00         1\n",
      "         227       0.00      0.00      0.00         3\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.80      1.00      0.89         4\n",
      "         231       1.00      1.00      1.00         5\n",
      "         232       1.00      1.00      1.00         6\n",
      "         234       0.00      0.00      0.00         1\n",
      "         236       0.00      0.00      0.00         6\n",
      "         237       0.00      0.00      0.00         1\n",
      "         238       1.00      1.00      1.00         4\n",
      "         239       1.00      1.00      1.00         3\n",
      "         240       0.00      0.00      0.00         2\n",
      "         241       1.00      0.25      0.40         4\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.00      0.00      0.00         2\n",
      "         246       0.67      0.25      0.36         8\n",
      "         247       0.00      0.00      0.00         5\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.42      1.00      0.60        14\n",
      "         251       0.00      0.00      0.00         5\n",
      "         253       0.67      0.67      0.67         3\n",
      "         254       1.00      0.67      0.80         3\n",
      "         255       1.00      1.00      1.00         3\n",
      "         256       0.00      0.00      0.00        10\n",
      "         257       0.00      0.00      0.00         1\n",
      "         258       0.41      0.78      0.54         9\n",
      "         259       0.00      0.00      0.00         4\n",
      "         262       0.00      0.00      0.00         1\n",
      "         264       0.46      0.86      0.60         7\n",
      "         269       0.62      0.68      0.65        19\n",
      "         270       0.85      1.00      0.92        40\n",
      "         271       0.00      0.00      0.00         5\n",
      "         272       0.00      0.00      0.00         1\n",
      "         273       0.00      0.00      0.00         2\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.81      0.93      0.87        14\n",
      "         277       0.00      0.00      0.00         3\n",
      "         279       0.54      1.00      0.70         7\n",
      "         281       0.00      0.00      0.00         4\n",
      "         282       0.00      0.00      0.00         2\n",
      "         283       0.75      1.00      0.86         6\n",
      "         284       0.00      0.00      0.00         1\n",
      "         285       0.50      0.75      0.60         8\n",
      "         286       0.00      0.00      0.00         8\n",
      "         287       1.00      0.33      0.50         6\n",
      "         288       0.00      0.00      0.00         4\n",
      "         289       0.41      1.00      0.58        13\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         3\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.88      0.70      0.78        40\n",
      "         298       1.00      0.69      0.82        26\n",
      "         299       0.99      0.99      0.99      8119\n",
      "\n",
      "    accuracy                           0.94     10009\n",
      "   macro avg       0.39      0.42      0.38     10009\n",
      "weighted avg       0.93      0.94      0.93     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 5\n",
    "print(\"Running Stage 5 model...\")\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "# Create model\n",
    "model_5 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_4.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_5 = model_5.fit(X_train_combined_4, y_train_4, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_4, y_test_4),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_5, 'model_5_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_5, accuracy_5 = model_5.evaluate(X_test_combined_4, y_test_4)\n",
    "print(\"Accuracy for Stage 5 model:\", accuracy_5)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_5 = model_5.predict(X_test_combined_4)\n",
    "y_pred_5 = np.argmax(y_pred_probabilities_5, axis=1)\n",
    "report_5 = classification_report(y_test_4, y_pred_5)\n",
    "print(\"Classification Report for Stage 5 model:\")\n",
    "print(report_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
