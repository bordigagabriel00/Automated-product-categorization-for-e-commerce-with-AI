{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjKToYfc6adY",
    "outputId": "e010d05c-aab8-4947-c72f-255032e61ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\2213573283.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lASrrK4Z6msI",
    "outputId": "a79cb80c-5c32-4ecc-82b2-1689a695c3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50041 entries, 0 to 50040\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   name             50040 non-null  object \n",
      " 1   type             50041 non-null  object \n",
      " 2   price            50041 non-null  float64\n",
      " 3   description      50041 non-null  object \n",
      " 4   manufacturer     49975 non-null  object \n",
      " 5   url              50040 non-null  object \n",
      " 6   parent_category  50041 non-null  object \n",
      " 7   sub_category_1   49294 non-null  object \n",
      " 8   sub_category_2   43948 non-null  object \n",
      " 9   sub_category_3   27955 non-null  object \n",
      " 10  sub_category_4   9788 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\assignment\\alpha2_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(pd.NA)\n",
    "\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OUzBSTQi6p8v"
   },
   "outputs": [],
   "source": [
    "# stemmer, lemmatizer and stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import Optional\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8martjTt6sdd",
    "outputId": "04595e57-b78b-4702-9291-3d7251bcc274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50041, 13)\n"
     ]
    }
   ],
   "source": [
    "normalization = ['name', 'description']\n",
    "for column in normalization:\n",
    "    df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                                      Duracell - AAA Batteries (4-Pack)\n",
      "type                                                               HardGood\n",
      "price                                                                  5.49\n",
      "description               Compatible with select electronic devices; AAA...\n",
      "manufacturer                                                       Duracell\n",
      "url                                           duracell aaa batteries 4 pack\n",
      "parent_category                                 Connected Home & Housewares\n",
      "sub_category_1                                                   Housewares\n",
      "sub_category_2                                          Household Batteries\n",
      "sub_category_3                                           Alkaline Batteries\n",
      "sub_category_4                                                      missing\n",
      "name_normalized                                        duracell aaa battery\n",
      "description_normalized    compatible select electronic device aaa size d...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HYTfY_RArpEX"
   },
   "outputs": [],
   "source": [
    "#df['sub_category_1'].fillna('0', inplace=True)\n",
    "#df['sub_category_2'].fillna('0', inplace=True)\n",
    "#df['sub_category_3'].fillna('0', inplace=True)\n",
    "#df['sub_category_4'].fillna('0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwLSa6uB6upR",
    "outputId": "3b01d80e-984e-4e8f-ffb0-73b48cd31694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape\n",
    "X = df.drop(columns=['parent_category'])\n",
    "y = df['parent_category']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "woBvshjI-gHS"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "X_1 = df.drop(columns=['sub_category_1'])\n",
    "y_1 = df['sub_category_1']\n",
    "y_1.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wYMN3-Aj-f-n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_2 = df.drop(columns=['sub_category_2'])\n",
    "y_2 = df['sub_category_2']\n",
    "y_2.fillna('missing', inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_2_encoded = label_encoder.fit_transform(y_2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sLrnJPS1-f3n"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_3 = df.drop(columns=['sub_category_3'])\n",
    "y_3 = df['sub_category_3']\n",
    "y_3.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_3_encoded = label_encoder.fit_transform(y_3)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MmO58b37-foH"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "X_4 = df.drop(columns=['sub_category_4'])\n",
    "y_4 = df['sub_category_4']\n",
    "y_4.fillna('missing', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_4_encoded = label_encoder.fit_transform(y_4)\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4, y_4_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxJvoJcBsGfp",
    "outputId": "8064db2d-ac1c-4896-9349-64285425f5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \\\n",
      "0     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "1     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "2     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "3     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "4     Housewares   Household Batteries   Alkaline Batteries        missing   \n",
      "\n",
      "                 name_normalized  \\\n",
      "0           duracell aaa battery   \n",
      "1  duracell aa coppertop battery   \n",
      "2            duracell aa battery   \n",
      "3       energizer max battery aa   \n",
      "4             duracell c battery   \n",
      "\n",
      "                              description_normalized  \n",
      "0  compatible select electronic device aaa size d...  \n",
      "1  longlasting energy duralock power preserve tec...  \n",
      "2  compatible select electronic device aa size du...  \n",
      "3         aa alkaline battery battery tester include  \n",
      "4  compatible select electronic device c size dur...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50041, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_ArCp4U6xB_",
    "outputId": "d5cd1931-8036-4eea-de8f-8f3daa08bcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40032, 12)\n",
      "(10009, 12)\n",
      "(40032,)\n",
      "(10009,)\n",
      "9233                            star fox preowned nintendo\n",
      "25631    pioneer networkready ultra hd passthrough av h...\n",
      "19030                     evolve ultimate edition xbox one\n",
      "12044    joby pro series ultraplate quickrelease plate ...\n",
      "18967    aluratek bump w home audio speaker system ipod...\n",
      "                               ...                        \n",
      "11284    samsung class diag lead curved smart ultra hd ...\n",
      "44732    hifonics brutus class mono mosfet subwoofer am...\n",
      "38158    mobile edge premium laptop backpack apple macb...\n",
      "860                                 presonus presonus gray\n",
      "15795      insignia portable bluetooth stereo speaker blue\n",
      "Name: name_normalized, Length: 40032, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train['name_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXktfLf6y1M",
    "outputId": "4b9e8a29-b99b-4ab7-f153-2346eeee9663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55,  5, 55, ..., 16, 43,  5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdo3ct0S6zga"
   },
   "source": [
    "One Hot Encoder y Scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMYODyT5cLA"
   },
   "source": [
    "Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHamdvAY64SL",
    "outputId": "c312cd27-8ae2-4f37-bdde-f90f6b6e38ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded: (40032, 2195)\n",
      "Data type of X_train_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded: float64\n",
      "Shape of X_test_encoded: (10009, 2195)\n",
      "Data type of X_test_encoded: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded: float64\n",
      "Shape of X_train_scaled: (40032, 1)\n",
      "Data type of elements in X_train_scaled: float64\n",
      "Shape of X_test_scaled: (10009, 1)\n",
      "Data type of elements in X_test_scaled: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "numerical_columns = ['price']\n",
    "text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Data type of elements in X_train_scaled:\", X_train_scaled.dtype)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14kj_745e3A"
   },
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rrKOHZ_4_AK",
    "outputId": "bd1b3f38-75f9-494e-dbf6-9bd87d1f6832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_1: (40032, 2253)\n",
      "Data type of X_train_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_1: float64\n",
      "Shape of X_test_encoded_1: (10009, 2253)\n",
      "Data type of X_test_encoded_1: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_1: float64\n",
      "Shape of X_train_scaled_1: (40032, 1)\n",
      "Data type of elements in X_train_scaled_1: float64\n",
      "Shape of X_test_scaled_1: (10009, 1)\n",
      "Data type of elements in X_test_scaled_1: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_1 = ['type', 'manufacturer', 'parent_category']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_1[categorical_columns_1] = X_train_1[categorical_columns_1].fillna('missing')\n",
    "X_test_1[categorical_columns_1] = X_test_1[categorical_columns_1].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_1 = encoder_1.fit_transform(X_train_1[categorical_columns_1])\n",
    "X_test_encoded_1 = encoder_1.transform(X_test_1[categorical_columns_1])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_1:\", X_train_encoded_1.shape)\n",
    "print(\"Data type of X_train_encoded_1:\", type(X_train_encoded_1))\n",
    "print(\"Data type of elements in X_train_encoded_1:\", X_train_encoded_1.dtype)\n",
    "print(\"Shape of X_test_encoded_1:\", X_test_encoded_1.shape)\n",
    "print(\"Data type of X_test_encoded_1:\", type(X_test_encoded_1))\n",
    "print(\"Data type of elements in X_test_encoded_1:\", X_test_encoded_1.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled_1 = scaler_1.fit_transform(X_train_1[numerical_columns])\n",
    "X_test_scaled_1 = scaler_1.transform(X_test_1[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_1:\", X_train_scaled_1.shape)\n",
    "print(\"Data type of elements in X_train_scaled_1:\", X_train_scaled_1.dtype)\n",
    "print(\"Shape of X_test_scaled_1:\", X_test_scaled_1.shape)\n",
    "print(\"Data type of elements in X_test_scaled_1:\", X_test_scaled_1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4uI4css5hqx"
   },
   "source": [
    "Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSlE-ZXJ4-5Q",
    "outputId": "cb5f926f-438b-42f9-ab8b-6c1f9f7ba5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_2: (40032, 2398)\n",
      "Data type of X_train_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_2: float64\n",
      "Shape of X_test_encoded_2: (10009, 2398)\n",
      "Data type of X_test_encoded_2: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_2: float64\n",
      "Shape of X_train_scaled_2: (40032, 1)\n",
      "Data type of elements in X_train_scaled_2: float64\n",
      "Shape of X_test_scaled_2: (10009, 1)\n",
      "Data type of elements in X_test_scaled_2: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_2 = ['type', 'manufacturer', 'parent_category', 'sub_category_1']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_2[categorical_columns_2] = X_train_2[categorical_columns_2].fillna('missing')\n",
    "X_test_2[categorical_columns_2] = X_test_2[categorical_columns_2].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_2 = encoder_2.fit_transform(X_train_2[categorical_columns_2])\n",
    "X_test_encoded_2 = encoder_2.transform(X_test_2[categorical_columns_2])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_2:\", X_train_encoded_2.shape)\n",
    "print(\"Data type of X_train_encoded_2:\", type(X_train_encoded_2))\n",
    "print(\"Data type of elements in X_train_encoded_2:\", X_train_encoded_2.dtype)\n",
    "print(\"Shape of X_test_encoded_2:\", X_test_encoded_2.shape)\n",
    "print(\"Data type of X_test_encoded_2:\", type(X_test_encoded_2))\n",
    "print(\"Data type of elements in X_test_encoded_2:\", X_test_encoded_2.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scaled_2 = scaler_2.fit_transform(X_train_2[numerical_columns])\n",
    "X_test_scaled_2 = scaler_2.transform(X_test_2[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_2:\", X_train_scaled_2.shape)\n",
    "print(\"Data type of elements in X_train_scaled_2:\", X_train_scaled_2.dtype)\n",
    "print(\"Shape of X_test_scaled_2:\", X_test_scaled_2.shape)\n",
    "print(\"Data type of elements in X_test_scaled_2:\", X_test_scaled_2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjEeJaua5jYm"
   },
   "source": [
    "Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UWrFjTJ4-y7",
    "outputId": "14c5078c-21d3-4614-8393-a3daa5372ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_3: (40032, 2943)\n",
      "Data type of X_train_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_3: float64\n",
      "Shape of X_test_encoded_3: (10009, 2943)\n",
      "Data type of X_test_encoded_3: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_3: float64\n",
      "Shape of X_train_scaled_3: (40032, 1)\n",
      "Data type of elements in X_train_scaled_3: float64\n",
      "Shape of X_test_scaled_3: (10009, 1)\n",
      "Data type of elements in X_test_scaled_3: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_3 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_3[categorical_columns_3] = X_train_3[categorical_columns_3].fillna('missing')\n",
    "X_test_3[categorical_columns_3] = X_test_3[categorical_columns_3].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_3 = encoder_3.fit_transform(X_train_3[categorical_columns_3])\n",
    "X_test_encoded_3 = encoder_3.transform(X_test_3[categorical_columns_3])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_3:\", X_train_encoded_3.shape)\n",
    "print(\"Data type of X_train_encoded_3:\", type(X_train_encoded_3))\n",
    "print(\"Data type of elements in X_train_encoded_3:\", X_train_encoded_3.dtype)\n",
    "print(\"Shape of X_test_encoded_3:\", X_test_encoded_3.shape)\n",
    "print(\"Data type of X_test_encoded_3:\", type(X_test_encoded_3))\n",
    "print(\"Data type of elements in X_test_encoded_3:\", X_test_encoded_3.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_3 = StandardScaler()\n",
    "X_train_scaled_3 = scaler_3.fit_transform(X_train_3[numerical_columns])\n",
    "X_test_scaled_3 = scaler_3.transform(X_test_3[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_3:\", X_train_scaled_3.shape)\n",
    "print(\"Data type of elements in X_train_scaled_3:\", X_train_scaled_3.dtype)\n",
    "print(\"Shape of X_test_scaled_3:\", X_test_scaled_3.shape)\n",
    "print(\"Data type of elements in X_test_scaled_3:\", X_test_scaled_3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX9LC3rA5k-T"
   },
   "source": [
    "Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E-BaTiE4-rh",
    "outputId": "0fc98aa5-895c-4d65-e0c7-d19d5a80f750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_4: (40032, 3632)\n",
      "Data type of X_train_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_train_encoded_4: float64\n",
      "Shape of X_test_encoded_4: (10009, 3632)\n",
      "Data type of X_test_encoded_4: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data type of elements in X_test_encoded_4: float64\n",
      "Shape of X_train_scaled_4: (40032, 1)\n",
      "Data type of elements in X_train_scaled_4: float64\n",
      "Shape of X_test_scaled_4: (10009, 1)\n",
      "Data type of elements in X_test_scaled_4: float64\n"
     ]
    }
   ],
   "source": [
    "# Define columns\n",
    "categorical_columns_4 = ['type', 'manufacturer', 'parent_category', 'sub_category_1', 'sub_category_2', 'sub_category_3']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train_4[categorical_columns_4] = X_train_4[categorical_columns_4].fillna('missing')\n",
    "X_test_4[categorical_columns_4] = X_test_4[categorical_columns_4].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded_4 = encoder_4.fit_transform(X_train_4[categorical_columns_4])\n",
    "X_test_encoded_4 = encoder_4.transform(X_test_4[categorical_columns_4])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded_4:\", X_train_encoded_4.shape)\n",
    "print(\"Data type of X_train_encoded_4:\", type(X_train_encoded_4))\n",
    "print(\"Data type of elements in X_train_encoded_4:\", X_train_encoded_4.dtype)\n",
    "print(\"Shape of X_test_encoded_4:\", X_test_encoded_4.shape)\n",
    "print(\"Data type of X_test_encoded_4:\", type(X_test_encoded_4))\n",
    "print(\"Data type of elements in X_test_encoded_4:\", X_test_encoded_4.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler_4 = StandardScaler()\n",
    "X_train_scaled_4 = scaler_4.fit_transform(X_train_4[numerical_columns])\n",
    "X_test_scaled_4 = scaler_4.transform(X_test_4[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_train_scaled_4:\", X_train_scaled_4.shape)\n",
    "print(\"Data type of elements in X_train_scaled_4:\", X_train_scaled_4.dtype)\n",
    "print(\"Shape of X_test_scaled_4:\", X_test_scaled_4.shape)\n",
    "print(\"Data type of elements in X_test_scaled_4:\", X_test_scaled_4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CgYBDhCwHKQA"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Stage 1\n",
    "X_train_processed = hstack([X_train_encoded, X_train_scaled]).astype(np.float32).toarray()\n",
    "X_test_processed = hstack([X_test_encoded, X_test_scaled]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 2\n",
    "X_train_processed_1 = hstack([X_train_encoded_1, X_train_scaled_1]).astype(np.float32).toarray()\n",
    "X_test_processed_1 = hstack([X_test_encoded_1, X_test_scaled_1]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 3\n",
    "X_train_processed_2 = hstack([X_train_encoded_2, X_train_scaled_2]).astype(np.float32).toarray()\n",
    "X_test_processed_2 = hstack([X_test_encoded_2, X_test_scaled_2]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 4\n",
    "X_train_processed_3 = hstack([X_train_encoded_3, X_train_scaled_3]).astype(np.float32).toarray()\n",
    "X_test_processed_3 = hstack([X_test_encoded_3, X_test_scaled_3]).astype(np.float32).toarray()\n",
    "\n",
    "# Stage 5\n",
    "X_train_processed_4 = hstack([X_train_encoded_4, X_train_scaled_4]).astype(np.float32).toarray()\n",
    "X_test_processed_4 = hstack([X_test_encoded_4, X_test_scaled_4]).astype(np.float32).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvYoUCeHHjx_",
    "outputId": "61b6f3c1-d6bf-4b4a-fcd5-cc6f7e54ab90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed: float32\n",
      "Data type of X_test_processed: float32\n",
      "X_train_processed shape: (40032, 2196)\n",
      "X_test_processed shape: (10009, 2196)\n"
     ]
    }
   ],
   "source": [
    "# Dim 1\n",
    "print(\"Data type of X_train_processed:\", X_train_processed.dtype)\n",
    "print(\"Data type of X_test_processed:\", X_test_processed.dtype)\n",
    "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhfSkN9I1vK",
    "outputId": "180da8c5-683c-4361-98ce-f51e50cd01ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_1: float32\n",
      "Data type of X_test_processed_1: float32\n",
      "X_train_processed_1 shape: (40032, 2254)\n",
      "X_test_processed_1 shape: (10009, 2254)\n"
     ]
    }
   ],
   "source": [
    "# Dim 2\n",
    "print(\"Data type of X_train_processed_1:\", X_train_processed_1.dtype)\n",
    "print(\"Data type of X_test_processed_1:\", X_test_processed_1.dtype)\n",
    "print(\"X_train_processed_1 shape:\", X_train_processed_1.shape)\n",
    "print(\"X_test_processed_1 shape:\", X_test_processed_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Az3OeCXI1rI",
    "outputId": "3b70f69e-056d-4351-9485-bd9313c99b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_2: float32\n",
      "Data type of X_test_processed_2: float32\n",
      "X_train_processed_2 shape: (40032, 2399)\n",
      "X_test_processed_2 shape: (10009, 2399)\n"
     ]
    }
   ],
   "source": [
    "# Dim 3\n",
    "print(\"Data type of X_train_processed_2:\", X_train_processed_2.dtype)\n",
    "print(\"Data type of X_test_processed_2:\", X_test_processed_2.dtype)\n",
    "print(\"X_train_processed_2 shape:\", X_train_processed_2.shape)\n",
    "print(\"X_test_processed_2 shape:\", X_test_processed_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBSG8gbeI1ng",
    "outputId": "902e994e-05fa-4a26-997f-66ff75d4eef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_3: float32\n",
      "Data type of X_test_processed_3: float32\n",
      "X_train_processed_3 shape: (40032, 2944)\n",
      "X_test_processed_3 shape: (10009, 2944)\n"
     ]
    }
   ],
   "source": [
    "# Dim 4\n",
    "print(\"Data type of X_train_processed_3:\", X_train_processed_3.dtype)\n",
    "print(\"Data type of X_test_processed_3:\",X_test_processed_3.dtype)\n",
    "print(\"X_train_processed_3 shape:\", X_train_processed_3.shape)\n",
    "print(\"X_test_processed_3 shape:\", X_test_processed_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIkzAILmI5Iv",
    "outputId": "945746cf-2be2-42d4-f3d9-49007732e383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train_processed_4: float32\n",
      "Data type of X_test_processed_4: float32\n",
      "X_train_processed_4 shape: (40032, 3633)\n",
      "X_test_processed_4 shape: (10009, 3633)\n"
     ]
    }
   ],
   "source": [
    "# Dim 5\n",
    "print(\"Data type of X_train_processed_4:\", X_train_processed_4.dtype)\n",
    "print(\"Data type of X_test_processed_4:\", X_test_processed_4.dtype)\n",
    "print(\"X_train_processed_4 shape:\", X_train_processed_4.shape)\n",
    "print(\"X_test_processed_4 shape:\", X_test_processed_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define file paths to save the arrays\n",
    "file_paths = {\n",
    "    'X_train_processed.npy': X_train_processed,\n",
    "    'X_test_processed.npy': X_test_processed,\n",
    "    'X_train_processed_1.npy': X_train_processed_1,\n",
    "    'X_test_processed_1.npy': X_test_processed_1,\n",
    "    'X_train_processed_2.npy': X_train_processed_2,\n",
    "    'X_test_processed_2.npy': X_test_processed_2,\n",
    "    'X_train_processed_3.npy': X_train_processed_3,\n",
    "    'X_test_processed_3.npy': X_test_processed_3,\n",
    "    'X_train_processed_4.npy': X_train_processed_4,\n",
    "    'X_test_processed_4.npy': X_test_processed_4\n",
    "}\n",
    "\n",
    "# Save each array\n",
    "for file_name, array in file_paths.items():\n",
    "    np.save(file_name, array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EVfWlKtNKCOR"
   },
   "outputs": [],
   "source": [
    "X_train_name_embeddings_loaded = np.load('X_train_name_embeddings.npy', allow_pickle= True)\n",
    "X_train_description_embeddings_loaded = np.load('X_train_description_embeddings.npy',allow_pickle= True)\n",
    "X_test_name_embeddings_loaded = np.load('X_test_name_embeddings.npy', allow_pickle= True)\n",
    "X_test_description_embeddings_loaded = np.load('X_test_description_embeddings.npy', allow_pickle= True)\n",
    "\n",
    "# Load the saved NumPy arrays\n",
    "X_train_processed_loaded = np.load('X_train_processed.npy', allow_pickle=True)\n",
    "X_test_processed_loaded = np.load('X_test_processed.npy', allow_pickle=True)\n",
    "X_train_processed_1_loaded = np.load('X_train_processed_1.npy', allow_pickle=True)\n",
    "X_test_processed_1_loaded = np.load('X_test_processed_1.npy', allow_pickle=True)\n",
    "X_train_processed_2_loaded = np.load('X_train_processed_2.npy', allow_pickle=True)\n",
    "X_test_processed_2_loaded = np.load('X_test_processed_2.npy', allow_pickle=True)\n",
    "X_train_processed_3_loaded = np.load('X_train_processed_3.npy', allow_pickle=True)\n",
    "X_test_processed_3_loaded = np.load('X_test_processed_3.npy', allow_pickle=True)\n",
    "X_train_processed_4_loaded = np.load('X_train_processed_4.npy', allow_pickle=True)\n",
    "X_test_processed_4_loaded = np.load('X_test_processed_4.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name_last_hidden = np.array([x[0][-1] for x in X_train_name_embeddings_loaded])\n",
    "X_train_description_last_hidden = np.array([x[0][-1] for x in X_train_description_embeddings_loaded])\n",
    "X_test_name_last_hidden = np.array([x[0][-1] for x in X_test_name_embeddings_loaded])\n",
    "X_test_description_last_hidden = np.array([x[0][-1] for x in X_test_description_embeddings_loaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of X_train_name_last_hidden: [ 6.20537519e-01 -1.57028392e-01 -4.39105332e-01  5.87245941e-01\n",
      " -4.92567480e-01 -6.75922573e-01  3.66373003e-01 -7.91122556e-01\n",
      "  6.63730741e-01  4.76211309e-02  4.97472771e-02 -3.77824754e-01\n",
      " -5.29451743e-02 -3.52970883e-03 -7.33473897e-01 -2.74465412e-01\n",
      " -8.19092095e-02 -1.52533606e-01  7.26244077e-02 -4.86062840e-02\n",
      "  3.07591200e-01 -1.32735014e-01  8.47112298e-01  2.39423364e-01\n",
      "  1.34860486e-01  3.51377934e-01 -4.78440404e-01 -1.80202276e-02\n",
      " -2.26639926e-01 -3.52599382e-01 -6.21600211e-01 -2.67526209e-01\n",
      " -1.92636084e-02  3.99675339e-01  1.55551210e-01 -8.61436129e-02\n",
      "  3.39647025e-01 -4.60110493e-02 -5.71235001e-01 -4.14561182e-01\n",
      " -3.34018916e-01 -1.61173679e-02 -1.93653673e-01  6.32311583e-01\n",
      "  5.47741950e-02 -6.00057304e-01  6.22461319e-01  3.42181236e-01\n",
      " -2.25496083e-01  4.89310294e-01  2.93806463e-01  1.60992384e-01\n",
      " -7.31535181e-02  8.67539346e-02  5.76151237e-02  1.21880889e-01\n",
      "  2.94119507e-01 -4.34998453e-01  1.54902250e-01  2.93051004e-01\n",
      " -5.07322550e-02  6.77793384e-01 -2.17716694e-01 -1.94208890e-01\n",
      "  6.51268899e-01 -1.67188928e-01  2.17404403e-02 -3.18291545e-01\n",
      " -6.07384741e-01 -3.05744082e-01 -2.39384830e-01 -1.02852356e+00\n",
      "  6.55281067e-01  3.35646331e-01  3.62706840e-01  2.41505504e-01\n",
      " -3.31891775e-01  7.45274603e-01  1.18391871e-01  3.59835505e-01\n",
      "  3.76000077e-01 -3.06956947e-01 -1.38037503e-01  7.29502961e-02\n",
      "  5.02546012e-01  1.56718288e-02 -2.42989823e-01  4.20575216e-02\n",
      " -4.59762454e-01 -1.26091674e-01  3.46461594e-01  2.71966606e-01\n",
      "  3.90934289e-01 -3.59424323e-01 -1.19082555e-01  3.55039179e-01\n",
      " -6.93194717e-02 -9.02826637e-02 -2.01892227e-01 -1.14928253e-01\n",
      " -3.77589911e-01 -1.72685519e-01 -5.20075373e-02  8.87612224e-01\n",
      " -1.62285715e-01 -2.95471847e-01  4.46627766e-01  5.57538629e-01\n",
      "  2.56792873e-01  1.03553367e+00  5.43226182e-01  2.22238839e-01\n",
      "  1.73543707e-01 -6.79688826e-02 -4.89283442e-01 -5.52646935e-01\n",
      "  2.85861969e-01  2.83273697e-01  3.80976677e-01  2.35821024e-01\n",
      " -6.67967856e-01 -4.66061831e-01  5.99375069e-01  1.08154857e+00\n",
      " -1.25421882e-01 -3.18098664e-02  2.50165947e-02 -7.18430936e-01\n",
      "  9.47480053e-02 -7.66403854e-01 -3.98136169e-01  6.48566902e-01\n",
      "  2.57072091e-01  8.71164203e-01 -1.05473615e-01  2.04653174e-01\n",
      " -3.08098674e-01  5.59956357e-02 -6.77700162e-01  1.01367123e-01\n",
      " -3.72281134e-01  9.31154370e-01  6.59990549e-01 -1.15582418e+00\n",
      "  5.36541790e-02  6.25010610e-01  5.59506238e-01 -3.16040702e-02\n",
      "  4.84125316e-02 -3.06834847e-01  7.54427016e-01 -1.69089168e-01\n",
      " -2.54145682e-01 -2.30913237e-02 -5.35159945e-01  2.63796747e-01\n",
      " -1.52081802e-01  1.89791620e-02  4.29030150e-01  7.86521673e-01\n",
      "  2.92653799e-01  3.41052055e-01  3.65956217e-01  5.37699759e-01\n",
      " -5.17349124e-01  2.24996656e-01 -1.17838478e+00 -1.76467597e-01\n",
      "  1.01287454e-01  4.32819426e-01 -4.23639059e-01 -4.54228640e-01\n",
      "  7.73141310e-02  3.79181325e-01 -3.45093966e-01  3.52966428e-01\n",
      " -1.62603110e-01  8.23184326e-02 -2.84529865e-01 -6.81628287e-01\n",
      " -8.46690559e+00 -2.35476375e-01 -9.76541191e-02  2.22685993e-01\n",
      " -3.64512168e-02 -3.91305238e-01 -8.25014532e-01 -1.73412472e-01\n",
      "  3.74747545e-01 -8.75730038e-01 -1.22479826e-01 -1.77380100e-01\n",
      " -9.63426471e-01  8.22258413e-01  3.41257453e-01 -2.38749608e-01\n",
      "  2.57653482e-02  2.47886866e-01 -3.30268562e-01  3.26861292e-01\n",
      " -1.97851792e-01 -3.42524916e-01  9.84942913e-02  6.40032470e-01\n",
      " -2.42812037e-01 -1.43791389e+00  2.46120483e-01 -2.29564041e-01\n",
      "  5.11401772e-01  5.85837178e-02 -1.31213963e+00 -1.12423360e-01\n",
      " -3.62519175e-03 -7.86731243e-01  1.56666100e-01 -5.34660220e-01\n",
      "  4.07156013e-02 -5.64395070e-01 -9.06348109e-01 -2.95303822e-01\n",
      " -5.59946835e-01 -3.20167810e-01 -1.84398927e-02  2.15938147e-02\n",
      "  5.29996753e-01 -1.46843731e+00  3.76851022e-01  7.21755743e-01\n",
      "  6.36393428e-01  2.52907544e-01 -8.59751627e-02 -1.71806812e-01\n",
      "  7.80056894e-01  1.51630104e-01  2.96293616e-01  1.60506576e-01\n",
      " -6.33846670e-02 -5.69900796e-02 -5.88919036e-02 -6.84572160e-01\n",
      " -7.04938546e-04  2.46744156e-01  2.25677311e-01 -8.23454112e-02\n",
      " -5.27979791e-01 -5.08032322e-01 -1.84435844e-01  9.23074484e-01\n",
      "  7.35809445e-01 -4.10479635e-01 -2.21967623e-01 -5.50180316e-01\n",
      "  3.03729922e-01 -4.96013671e-01  5.50130188e-01  7.58742869e-01\n",
      "  5.94226606e-02 -4.39812601e-01  1.16967094e+00 -5.56134701e-01\n",
      "  4.20879245e-01  7.08146334e-01  3.12095106e-01  1.29798785e-01\n",
      " -6.85978770e-01  6.66718185e-01  4.30409670e-01  2.85361290e-01\n",
      " -3.43734175e-01  3.40557098e-01  4.58756328e-01 -1.30768120e-01\n",
      " -4.67268795e-01  5.87800920e-01 -1.30464152e-01 -1.07199299e+00\n",
      "  6.74340010e-01 -8.66399184e-02  3.83873284e-01 -1.83765680e-01\n",
      " -3.93801302e-01  2.69577444e-01 -5.39098233e-02  1.67211309e-01\n",
      "  2.02100113e-01 -4.57387537e-01 -8.52275074e-01 -1.42785877e-01\n",
      "  3.35738182e-01 -6.06438577e-01 -1.52843416e-01 -3.03091884e-01\n",
      " -8.47115964e-02  6.67190492e-01 -6.68784678e-02  6.33220226e-02\n",
      "  5.23253739e-01 -1.75359398e-02 -4.24083918e-01 -2.04071775e-01\n",
      "  2.14214072e-01 -7.02822745e-01  1.12920888e-01 -3.13348323e-02\n",
      " -2.11466819e-01 -1.50750086e-01  4.50119257e-01  1.24140903e-01\n",
      "  7.55189002e-01 -1.10170841e-01  2.85554498e-01 -6.85934961e-01\n",
      "  2.90729553e-01 -1.57349512e-01 -2.48254389e-02  7.73136169e-02\n",
      " -1.95251510e-01 -1.05724163e-01  1.81615889e-01 -4.38701361e-01\n",
      "  2.71253109e-01  2.69789636e-01  1.97617477e-03  4.88260210e-01\n",
      " -6.08754009e-02 -6.35233000e-02 -8.48654211e-01 -4.67859924e-01\n",
      " -4.04063538e-02  3.92924100e-01  4.71455455e-01 -1.16113812e-01\n",
      "  6.34941638e-01 -2.62468249e-01 -4.95653987e-01 -3.08744982e-02\n",
      "  3.34599704e-01 -8.20729434e-02 -3.69854458e-02 -4.42685336e-01\n",
      " -5.49810231e-01  7.30598047e-02  2.70330966e-01 -1.09698489e-01\n",
      " -2.68085241e-01  5.26564658e-01  2.56138742e-01 -1.54329166e-01\n",
      "  2.68264174e-01  6.50385201e-01  1.24321252e-01 -8.18036258e-01\n",
      " -7.96757817e-01 -3.77051294e-01 -2.34010071e-01 -4.08915311e-01\n",
      " -4.76937480e-02  3.31464559e-01  7.78878927e-01  2.71527432e-02\n",
      " -3.22321892e-01  6.98770434e-02 -4.85345066e-01 -7.42784023e-01\n",
      " -4.95838150e-02  2.48317659e-01 -6.83598459e-01 -2.63965130e-01\n",
      " -3.53393927e-02 -4.28863764e-01  3.14770162e-01  1.37096956e-01\n",
      " -3.14997852e-01  8.23536664e-02  2.95320004e-02  5.10599613e-01\n",
      " -3.71191561e-01 -7.13964030e-02  2.93738581e-02 -3.19731176e-01\n",
      " -5.60358286e-01 -6.47917867e-01 -4.73039091e-01  1.92232206e-01\n",
      " -4.33800578e-01  1.69615537e-01  2.53361315e-01  1.28410399e-01\n",
      "  2.39444315e-01 -5.11149056e-02  2.00972959e-01  4.52268481e-01\n",
      " -9.15191919e-02  7.91754797e-02  6.82296529e-02 -2.93338329e-01\n",
      "  4.06531304e-01 -4.03168797e-01  3.09270024e-01  6.50663003e-02\n",
      "  1.05046615e-01 -8.64279717e-02 -1.53797373e-01  4.02077079e-01\n",
      "  5.03367066e-01  7.63851777e-02  2.09626377e-01  1.53182715e-01\n",
      " -5.65243840e-01 -1.32839739e-01  6.27765238e-01 -3.82222414e-01\n",
      " -3.88237000e-01  4.48667586e-01 -4.94298071e-01 -4.10451323e-01\n",
      " -4.89565134e-01 -4.12086457e-01  5.01163781e-01 -6.34864271e-01\n",
      "  4.93994743e-01  3.40991318e-02 -7.70612210e-02 -5.23390770e-01\n",
      " -3.90536785e-01  9.76225793e-01 -2.31955290e-01  4.92922813e-01\n",
      "  2.07122847e-01  3.43242586e-01  6.75997019e-01  3.43689382e-01\n",
      " -7.34328479e-03 -7.62901306e-02 -1.36262581e-01 -3.49554360e-01\n",
      "  2.33103514e-01 -2.97866374e-01  1.42095387e-01  2.46145166e-02\n",
      "  6.91457465e-02 -7.26145744e-01 -2.68725991e-01  4.27473038e-01\n",
      "  8.14948857e-01  1.12877059e+00  5.72656631e-01  6.83157027e-01\n",
      "  2.91611552e-01  4.32616830e-01 -8.02971900e-01 -2.75246471e-01\n",
      "  1.94451630e-01  6.13069296e-01 -5.72932422e-01  2.11954072e-01\n",
      "  1.04959095e+00 -7.37529248e-02  1.31617635e-01  5.35595417e-01\n",
      "  5.16681373e-01 -9.72960353e-01  2.33713865e-01 -6.80972874e-01\n",
      " -4.43347394e-02  6.06121868e-02 -8.95191729e-01  6.56643152e-01\n",
      " -4.67941612e-01  1.72935054e-01  3.06211114e-01 -2.09628448e-01\n",
      " -4.73560929e-01 -7.77069032e-01  5.25709808e-01 -5.47120214e-01\n",
      " -4.76924509e-01  3.44251424e-01  5.33317804e-01 -1.14928357e-01\n",
      "  8.92056108e-01 -1.09298661e-01 -1.78937271e-01  4.77421999e-01\n",
      "  1.20972060e-02 -6.01830721e-01  1.15526125e-01  1.84282243e-01\n",
      "  1.20737918e-01 -8.41489851e-01  2.74324179e-01  6.53214380e-02\n",
      "  3.10614675e-01  1.87517211e-01 -3.63761693e-01 -4.86926019e-01\n",
      "  2.67216444e-01  7.08758384e-02  4.93382275e-01 -6.62216306e-01\n",
      " -8.03277671e-01  5.06674275e-02 -4.52651903e-02  8.58727932e-01\n",
      "  3.86713773e-01 -2.26233333e-01  2.94005901e-01  8.16828609e-02\n",
      "  4.67826068e-01  9.88701731e-02 -5.52253127e-01 -6.25416636e-02\n",
      " -4.36638117e-01 -1.23846389e-01  3.10184360e-01  1.10362805e-01\n",
      " -3.76352072e-01 -6.95727170e-01 -2.82260239e-01 -2.98604459e-01\n",
      " -5.82896113e-01 -7.70874918e-02  6.86300695e-01  4.38593328e-01\n",
      "  2.09501594e-01  1.93385899e-01  1.10943151e+00 -6.18437052e-01\n",
      " -5.46705842e-01 -3.53303879e-01 -4.70864028e-03  2.02219620e-01\n",
      "  1.96173102e-01 -8.64403367e-01 -6.46913230e-01  1.45509869e-01\n",
      " -6.57370865e-01  2.44000092e-01  6.11473203e-01  5.38556337e-01\n",
      " -6.71112657e-01 -4.79236394e-02  1.10620749e+00  2.27204785e-01\n",
      " -2.44950041e-01 -4.12875004e-02 -4.92067188e-02 -3.21430787e-02\n",
      " -3.99663895e-01 -2.86540627e-01 -4.12346452e-01  3.79928648e-02\n",
      "  3.89240086e-01 -1.03023134e-01  3.54121447e-01  1.34739459e+00\n",
      " -1.63724944e-02 -6.84470415e-01 -2.10255329e-02 -1.12153515e-01\n",
      "  4.70662117e-01  5.75038865e-02 -6.25386477e-01 -3.15227062e-01\n",
      " -4.69694585e-01 -4.81898487e-02 -1.43374532e-01 -3.73101383e-01\n",
      "  1.92334324e-01 -1.23024061e-01  1.10399842e+00 -8.03215504e-01\n",
      "  3.67378086e-01  8.49291235e-02 -2.87107050e-01 -3.32370818e-01\n",
      " -2.46271253e-01  4.15957689e-01  1.03018686e-01  4.71279770e-01\n",
      " -1.91447139e-01  3.02230835e-01 -6.40176773e-01  4.34333146e-01\n",
      "  4.13303167e-01 -2.25464195e-01 -2.43023872e-01 -2.54389524e-01\n",
      "  1.05514765e-01  4.62910652e-01 -4.52698588e-01  4.98102456e-02\n",
      " -2.12586150e-01 -5.03674865e-01  1.03526384e-01  2.60310173e-01\n",
      " -1.91697702e-01 -4.90235597e-01  9.73624110e-01  1.67644136e-02\n",
      " -3.09523135e-01 -5.45606792e-01 -3.38727415e-01  1.95590585e-01\n",
      " -3.95997971e-01  4.34039265e-01 -4.79470164e-01 -1.79774463e-01\n",
      " -1.33216277e-01  2.92425573e-01  1.17567241e-01 -2.01351941e-04\n",
      " -2.28282869e-01 -2.98369490e-03  1.14583872e-01  1.87827110e-01\n",
      " -6.16419092e-02 -2.99429864e-01 -5.49328566e-01  1.95448071e-01\n",
      "  3.30738798e-02  4.45143133e-01 -1.45918047e+00  2.76990116e-01\n",
      "  1.06867522e-01 -6.36705995e-01  1.49721503e-02  6.21623933e-01\n",
      "  4.35345471e-01  3.67273450e-01  2.21252292e-01 -1.68745726e-01\n",
      "  6.34661615e-01  3.05576414e-01 -6.60773218e-01 -3.48220825e-01\n",
      " -6.01494908e-01  1.62416726e-01  8.51277590e-01  5.64038873e-01\n",
      " -1.32888541e-01  5.78130960e-01  4.37329769e-01 -2.50419378e-01\n",
      "  1.27389640e-01  3.04841608e-01  1.69228345e-01 -6.80671453e-01\n",
      " -7.91647136e-02 -3.90768163e-02 -5.89853674e-02  3.26032192e-01\n",
      "  2.79810995e-01  2.01112419e-01  2.79959261e-01 -3.94225419e-01\n",
      " -5.61534055e-02 -2.80866593e-01  4.21973437e-01 -1.25217527e-01\n",
      " -2.44032800e-01  2.56264925e-01 -5.77546299e-01 -1.79337002e-02\n",
      "  3.75045419e-01  3.90127748e-01  3.08235317e-01  4.32461239e-02\n",
      " -3.19815397e-01 -7.31928587e-01 -8.57445300e-01 -4.56196636e-01\n",
      "  1.91792414e-01  1.12777472e+00  1.14098623e-01 -3.93427730e-01\n",
      " -5.31382799e-01  3.73633385e-01  6.49822295e-01  6.61252618e-01\n",
      " -5.59039831e-01 -5.07438898e-01  1.88692242e-01  4.30638194e-01\n",
      " -4.43620563e-01  4.72782195e-01 -6.59415543e-01  6.56473190e-02\n",
      "  5.67307770e-01 -2.75379837e-01 -4.96693611e-01  7.40167964e-03\n",
      "  3.16577435e-01 -4.18295413e-01 -4.22885656e-01 -5.52101433e-01\n",
      "  1.22607075e-01  9.61042792e-02  3.23055595e-01 -8.41065586e-01\n",
      " -7.24421814e-02 -1.82764068e-01  6.32212877e-01  1.67815357e-01\n",
      "  9.77995574e-01 -5.84435463e-03  5.92191696e-01  3.41564238e-01\n",
      "  6.90299749e-01 -1.18450627e-01 -7.60542631e-01  3.70549768e-01\n",
      "  7.42236316e-01 -1.17789373e-01  1.43134284e+00 -5.23150861e-01\n",
      "  7.84933329e-01 -1.76844656e-01 -7.12502420e-01 -9.65571776e-02\n",
      " -3.18498445e+00  3.49936277e-01  1.60431802e-01 -4.65894155e-02\n",
      "  1.10750936e-01  5.58575392e-01  3.64335775e-01 -2.92843252e-01\n",
      " -2.93849111e-01 -2.34929547e-01  4.15985495e-01 -2.82904059e-01\n",
      " -2.30949983e-01  4.11721736e-01  1.61260486e-01  7.83595920e-01\n",
      "  9.95904356e-02 -6.22513071e-02  6.16447330e-01  1.61838517e-01\n",
      "  1.10445112e-01  9.79023241e-03  1.38397485e-01 -5.57122171e-01\n",
      " -3.70276511e-01  4.98058796e-02 -4.47070837e-01 -5.60444772e-01\n",
      " -3.11797470e-01 -1.47280797e-01  5.75177312e-01  6.68729022e-02\n",
      "  5.60362518e-01 -1.32139444e-01  8.84481013e-01 -8.28564614e-02\n",
      "  2.30413064e-01 -3.49385709e-01  4.17937599e-02 -9.30054039e-02\n",
      " -3.41964692e-01 -6.47252619e-01  1.84758946e-01 -8.61653328e-01\n",
      " -2.63148900e-02  2.52711296e-01 -6.25010908e-01  6.25658184e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"First element of X_train_name_last_hidden:\", X_train_name_last_hidden[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_name_last_hidden: (40032, 768)\n",
      "Data type of X_train_name_last_hidden: float32\n",
      "Shape of X_train_description_last_hidden: (40032, 768)\n",
      "Data type of X_train_description_last_hidden: float32\n",
      "Shape of X_test_name_last_hidden: (10009, 768)\n",
      "Data type of X_test_name_last_hidden: float32\n",
      "Shape of X_test_description_last_hidden: (10009, 768)\n",
      "Data type of X_test_description_last_hidden: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_name_last_hidden:\", X_train_name_last_hidden.shape)\n",
    "print(\"Data type of X_train_name_last_hidden:\", X_train_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_train_description_last_hidden:\", X_train_description_last_hidden.shape)\n",
    "print(\"Data type of X_train_description_last_hidden:\", X_train_description_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_name_last_hidden:\", X_test_name_last_hidden.shape)\n",
    "print(\"Data type of X_test_name_last_hidden:\", X_test_name_last_hidden.dtype)\n",
    "\n",
    "print(\"Shape of X_test_description_last_hidden:\", X_test_description_last_hidden.shape)\n",
    "print(\"Data type of X_test_description_last_hidden:\", X_test_description_last_hidden.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concatenated = np.concatenate((X_train_name_last_hidden, X_train_description_last_hidden), axis=1)\n",
    "X_test_concatenated = np.concatenate((X_test_name_last_hidden, X_test_description_last_hidden), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_concatenated: (40032, 1536)\n",
      "Data type of X_train_concatenated: float32\n",
      "Shape of X_test_concatenated: (10009, 1536)\n",
      "Data type of X_test_concatenated: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_concatenated:\", X_train_concatenated.shape)\n",
    "print(\"Data type of X_train_concatenated:\", X_train_concatenated.dtype)\n",
    "\n",
    "print(\"Shape of X_test_concatenated:\", X_test_concatenated.shape)\n",
    "print(\"Data type of X_test_concatenated:\", X_test_concatenated.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "X_train_combined = np.concatenate((X_train_processed_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_processed_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 2\n",
    "X_train_combined_1 = np.concatenate((X_train_processed_1_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_1 = np.concatenate((X_test_processed_1_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 3\n",
    "X_train_combined_2 = np.concatenate((X_train_processed_2_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_2 = np.concatenate((X_test_processed_2_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 4\n",
    "X_train_combined_3 = np.concatenate((X_train_processed_3_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_3 = np.concatenate((X_test_processed_3_loaded, X_test_concatenated), axis=1)\n",
    "\n",
    "# Stage 5\n",
    "X_train_combined_4 = np.concatenate((X_train_processed_4_loaded, X_train_concatenated), axis=1)\n",
    "X_test_combined_4 = np.concatenate((X_test_processed_4_loaded, X_test_concatenated), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define num_clases\n",
    "\n",
    "num_classes = len(df['parent_category'].unique())\n",
    "num_classes_1 = len(df['sub_category_1'].unique())\n",
    "num_classes_2 = len(df['sub_category_2'].unique())\n",
    "num_classes_3 = len(df['sub_category_3'].unique())\n",
    "num_classes_4 = len(df['sub_category_4'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "149\n",
      "559\n",
      "720\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)\n",
    "print(num_classes_1)\n",
    "print(num_classes_2)\n",
    "print(num_classes_3)\n",
    "print(num_classes_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1251/1251 [==============================] - 6s 3ms/step - loss: 1.0607 - accuracy: 0.7399 - val_loss: 0.4510 - val_accuracy: 0.8724 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5946 - accuracy: 0.8413 - val_loss: 0.3856 - val_accuracy: 0.8925 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4888 - accuracy: 0.8704 - val_loss: 0.3253 - val_accuracy: 0.9148 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4241 - accuracy: 0.8895 - val_loss: 0.2971 - val_accuracy: 0.9225 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3778 - accuracy: 0.8997 - val_loss: 0.2922 - val_accuracy: 0.9201 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3557 - accuracy: 0.9064 - val_loss: 0.2394 - val_accuracy: 0.9420 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3225 - accuracy: 0.9161 - val_loss: 0.2185 - val_accuracy: 0.9460 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2998 - accuracy: 0.9214 - val_loss: 0.2284 - val_accuracy: 0.9433 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2802 - accuracy: 0.9275 - val_loss: 0.2207 - val_accuracy: 0.9445 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2631 - accuracy: 0.9331 - val_loss: 0.2096 - val_accuracy: 0.9497 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2492 - accuracy: 0.9359 - val_loss: 0.1995 - val_accuracy: 0.9517 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2357 - accuracy: 0.9387 - val_loss: 0.1961 - val_accuracy: 0.9535 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2242 - accuracy: 0.9423 - val_loss: 0.1935 - val_accuracy: 0.9542 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2199 - accuracy: 0.9449 - val_loss: 0.1917 - val_accuracy: 0.9548 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2153 - accuracy: 0.9448 - val_loss: 0.1923 - val_accuracy: 0.9547 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 3s 2ms/step - loss: 0.2111 - accuracy: 0.9462 - val_loss: 0.1935 - val_accuracy: 0.9553 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2075 - accuracy: 0.9483 - val_loss: 0.1925 - val_accuracy: 0.9546 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1989 - accuracy: 0.9482 - val_loss: 0.1914 - val_accuracy: 0.9554 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1984 - accuracy: 0.9491 - val_loss: 0.1920 - val_accuracy: 0.9557 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1940 - accuracy: 0.9502 - val_loss: 0.1907 - val_accuracy: 0.9558 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1944 - accuracy: 0.9492 - val_loss: 0.1909 - val_accuracy: 0.9557 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1916 - accuracy: 0.9503 - val_loss: 0.1905 - val_accuracy: 0.9555 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1924 - accuracy: 0.9506 - val_loss: 0.1911 - val_accuracy: 0.9559 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1952 - accuracy: 0.9492 - val_loss: 0.1904 - val_accuracy: 0.9556 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1953 - accuracy: 0.9503 - val_loss: 0.1905 - val_accuracy: 0.9554 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1955 - accuracy: 0.9493 - val_loss: 0.1914 - val_accuracy: 0.9559 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1890 - accuracy: 0.9508 - val_loss: 0.1911 - val_accuracy: 0.9560 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1952 - accuracy: 0.9504 - val_loss: 0.1916 - val_accuracy: 0.9556 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.1946 - accuracy: 0.9505 - val_loss: 0.1912 - val_accuracy: 0.9559 - lr: 1.7824e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\2724651961.py:46: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, 'model_1_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9556\n",
      "Accuracy: 0.9556398987770081\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           4       0.98      0.98      0.98      1712\n",
      "           5       0.92      0.96      0.94       715\n",
      "           6       0.98      0.98      0.98        91\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       1.00      1.00      1.00         2\n",
      "          11       0.94      0.97      0.96       649\n",
      "          12       0.96      0.97      0.96       505\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.97      0.97      0.97      1392\n",
      "          16       0.94      0.95      0.95      1247\n",
      "          17       0.93      0.93      0.93       890\n",
      "          18       0.00      0.00      0.00         6\n",
      "          25       0.00      0.00      0.00         2\n",
      "          27       0.83      0.94      0.88        16\n",
      "          28       0.00      0.00      0.00         3\n",
      "          31       0.00      0.00      0.00         3\n",
      "          33       0.94      0.92      0.93       245\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         1\n",
      "          39       1.00      1.00      1.00        15\n",
      "          40       0.00      0.00      0.00         2\n",
      "          41       0.93      0.52      0.67        27\n",
      "          42       1.00      0.90      0.95        10\n",
      "          43       0.95      0.96      0.95       624\n",
      "          44       0.89      0.44      0.59        36\n",
      "          46       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.92      0.92      0.92       417\n",
      "          54       0.95      0.89      0.92       132\n",
      "          55       0.99      0.98      0.99      1194\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       1.00      0.86      0.92        57\n",
      "          61       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.96     10009\n",
      "   macro avg       0.51      0.49      0.50     10009\n",
      "weighted avg       0.95      0.96      0.95     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage1\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Compile the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model fit\n",
    "run_1 = model.fit(X_train_combined, y_train, epochs=60, batch_size=32,\n",
    "                  validation_data=(X_test_combined, y_test),\n",
    "                  callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "save_model(model, 'model_1_preberttune.h5')\n",
    "\n",
    "y_pred_probabilities = model.predict(X_test_combined)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.7915 - accuracy: 0.6070 - val_loss: 0.7050 - val_accuracy: 0.8294 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.8417 - accuracy: 0.7730 - val_loss: 0.4212 - val_accuracy: 0.8756 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6612 - accuracy: 0.8105 - val_loss: 0.4042 - val_accuracy: 0.8757 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5898 - accuracy: 0.8317 - val_loss: 0.3148 - val_accuracy: 0.9042 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.5257 - accuracy: 0.8436 - val_loss: 0.2736 - val_accuracy: 0.9223 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4870 - accuracy: 0.8571 - val_loss: 0.2528 - val_accuracy: 0.9247 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4563 - accuracy: 0.8636 - val_loss: 0.2502 - val_accuracy: 0.9270 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4327 - accuracy: 0.8704 - val_loss: 0.2413 - val_accuracy: 0.9255 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.4071 - accuracy: 0.8780 - val_loss: 0.2237 - val_accuracy: 0.9345 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3907 - accuracy: 0.8826 - val_loss: 0.2475 - val_accuracy: 0.9243 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3734 - accuracy: 0.8872 - val_loss: 0.2044 - val_accuracy: 0.9424 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3486 - accuracy: 0.8962 - val_loss: 0.1976 - val_accuracy: 0.9440 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3442 - accuracy: 0.8961 - val_loss: 0.2015 - val_accuracy: 0.9406 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3307 - accuracy: 0.9017 - val_loss: 0.1952 - val_accuracy: 0.9450 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3220 - accuracy: 0.9037 - val_loss: 0.1914 - val_accuracy: 0.9455 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3199 - accuracy: 0.9032 - val_loss: 0.1912 - val_accuracy: 0.9455 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3174 - accuracy: 0.9046 - val_loss: 0.1905 - val_accuracy: 0.9455 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3071 - accuracy: 0.9095 - val_loss: 0.1895 - val_accuracy: 0.9452 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3109 - accuracy: 0.9072 - val_loss: 0.1883 - val_accuracy: 0.9460 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3094 - accuracy: 0.9066 - val_loss: 0.1875 - val_accuracy: 0.9463 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3041 - accuracy: 0.9081 - val_loss: 0.1878 - val_accuracy: 0.9464 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3054 - accuracy: 0.9080 - val_loss: 0.1876 - val_accuracy: 0.9471 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3043 - accuracy: 0.9105 - val_loss: 0.1873 - val_accuracy: 0.9467 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3007 - accuracy: 0.9093 - val_loss: 0.1873 - val_accuracy: 0.9462 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3011 - accuracy: 0.9102 - val_loss: 0.1867 - val_accuracy: 0.9466 - lr: 1.3753e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3043 - accuracy: 0.9089 - val_loss: 0.1876 - val_accuracy: 0.9469 - lr: 8.2519e-07\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3042 - accuracy: 0.9090 - val_loss: 0.1873 - val_accuracy: 0.9468 - lr: 4.9511e-07\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.2970 - accuracy: 0.9100 - val_loss: 0.1873 - val_accuracy: 0.9468 - lr: 2.9707e-07\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3009 - accuracy: 0.9097 - val_loss: 0.1871 - val_accuracy: 0.9471 - lr: 1.7824e-07\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.3008 - accuracy: 0.9091 - val_loss: 0.1873 - val_accuracy: 0.9468 - lr: 1.0694e-07\n",
      " 33/313 [==>...........................] - ETA: 0s - loss: 0.1828 - accuracy: 0.9479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\4019872855.py:41: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_2, 'model_2_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9466\n",
      "Accuracy for Stage 2 model: 0.9466480016708374\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 2 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.94      0.93        35\n",
      "           2       0.88      0.61      0.72        23\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       0.95      1.00      0.97        54\n",
      "           5       0.93      0.92      0.93       128\n",
      "           6       0.93      1.00      0.96        13\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       1.00      0.96      0.98        79\n",
      "          10       1.00      1.00      1.00         9\n",
      "          11       0.95      0.92      0.94       156\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.83      0.63      0.72        71\n",
      "          14       1.00      0.73      0.84        11\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       0.33      1.00      0.50         1\n",
      "          18       1.00      0.67      0.80         3\n",
      "          19       0.87      0.96      0.91       128\n",
      "          20       0.86      1.00      0.92         6\n",
      "          21       0.96      0.96      0.96       278\n",
      "          22       0.90      0.75      0.82        12\n",
      "          23       1.00      0.58      0.74        12\n",
      "          24       0.78      0.70      0.74        10\n",
      "          25       0.00      0.00      0.00         3\n",
      "          26       0.99      1.00      1.00      1242\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.67      0.50      0.57         4\n",
      "          29       0.95      0.98      0.97       464\n",
      "          30       0.99      0.89      0.94        74\n",
      "          31       0.94      0.98      0.96       125\n",
      "          32       0.96      0.64      0.77        39\n",
      "          33       1.00      0.99      0.99        83\n",
      "          34       0.91      0.97      0.94       372\n",
      "          35       0.95      0.93      0.94        88\n",
      "          36       1.00      0.96      0.98        57\n",
      "          37       1.00      0.25      0.40         8\n",
      "          38       0.87      0.71      0.78        28\n",
      "          39       0.00      0.00      0.00         7\n",
      "          40       1.00      1.00      1.00        14\n",
      "          42       1.00      1.00      1.00       245\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       0.98      0.95      0.96        43\n",
      "          45       0.92      0.92      0.92       105\n",
      "          46       0.94      0.94      0.94        32\n",
      "          47       1.00      0.75      0.86         4\n",
      "          48       1.00      1.00      1.00         6\n",
      "          49       1.00      1.00      1.00       132\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          54       0.88      1.00      0.93         7\n",
      "          55       0.85      0.96      0.90        24\n",
      "          56       0.99      1.00      0.99       207\n",
      "          57       0.96      0.98      0.97       200\n",
      "          59       0.91      0.97      0.94       275\n",
      "          60       0.96      0.79      0.87        34\n",
      "          61       0.00      0.00      0.00         1\n",
      "          62       0.95      0.84      0.89        43\n",
      "          63       0.91      0.98      0.94       134\n",
      "          64       0.67      1.00      0.80         2\n",
      "          65       1.00      0.84      0.91        19\n",
      "          66       1.00      0.71      0.83         7\n",
      "          67       0.94      0.68      0.79        25\n",
      "          70       0.99      0.99      0.99       113\n",
      "          72       1.00      1.00      1.00        27\n",
      "          73       0.76      0.70      0.73        27\n",
      "          74       1.00      1.00      1.00        27\n",
      "          75       0.67      0.86      0.75       100\n",
      "          77       0.98      1.00      0.99        58\n",
      "          79       0.98      1.00      0.99        52\n",
      "          80       1.00      1.00      1.00         6\n",
      "          81       1.00      1.00      1.00         4\n",
      "          83       0.67      0.81      0.73        99\n",
      "          84       0.98      0.93      0.96        58\n",
      "          85       0.00      0.00      0.00         3\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.72      0.70      0.71        30\n",
      "          88       0.89      0.97      0.93        32\n",
      "          89       0.92      0.75      0.83        16\n",
      "          90       0.97      0.98      0.97       217\n",
      "          91       0.92      0.87      0.89        53\n",
      "          92       0.96      0.93      0.94       108\n",
      "          93       1.00      0.78      0.88        32\n",
      "          94       1.00      0.10      0.18        10\n",
      "          95       0.00      0.00      0.00         2\n",
      "          96       1.00      0.95      0.98        22\n",
      "          97       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00        13\n",
      "          99       0.35      0.19      0.25        37\n",
      "         100       0.55      0.88      0.67        68\n",
      "         102       0.00      0.00      0.00         1\n",
      "         103       0.99      1.00      1.00       722\n",
      "         104       1.00      0.94      0.97        31\n",
      "         105       1.00      1.00      1.00       236\n",
      "         107       0.71      0.61      0.66        57\n",
      "         108       0.98      0.98      0.98       197\n",
      "         109       1.00      1.00      1.00         9\n",
      "         110       0.00      0.00      0.00         8\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       1.00      0.78      0.88         9\n",
      "         114       0.98      0.98      0.98       181\n",
      "         115       0.98      0.99      0.99       614\n",
      "         116       0.99      0.99      0.99        84\n",
      "         119       1.00      0.38      0.55         8\n",
      "         120       0.93      0.96      0.94       117\n",
      "         121       0.50      0.80      0.62         5\n",
      "         122       0.96      0.98      0.97       191\n",
      "         123       0.97      0.97      0.97        61\n",
      "         124       0.97      1.00      0.99        68\n",
      "         125       0.97      1.00      0.99        72\n",
      "         126       0.90      1.00      0.95        55\n",
      "         127       1.00      1.00      1.00         1\n",
      "         128       0.00      0.00      0.00         1\n",
      "         130       1.00      1.00      1.00        64\n",
      "         131       0.95      0.97      0.96        62\n",
      "         132       0.73      0.76      0.74        21\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         1\n",
      "         136       1.00      0.99      0.99        96\n",
      "         138       0.78      0.67      0.72        21\n",
      "         139       0.77      0.67      0.71        15\n",
      "         140       0.84      0.80      0.82        65\n",
      "         141       0.85      0.96      0.90        54\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.95      0.97      0.96       243\n",
      "         144       0.95      1.00      0.97        18\n",
      "         145       0.95      0.83      0.88        23\n",
      "         146       1.00      1.00      1.00        12\n",
      "         148       0.96      0.89      0.92       154\n",
      "\n",
      "    accuracy                           0.95     10009\n",
      "   macro avg       0.78      0.74      0.75     10009\n",
      "weighted avg       0.94      0.95      0.94     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Stage2\n",
    "# Define learning rate function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_2 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_1.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_2 = model_2.fit(X_train_combined_1, y_train_1, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_1, y_test_1),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_2, 'model_2_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_2, accuracy_2 = model_2.evaluate(X_test_combined_1, y_test_1)\n",
    "print(\"Accuracy for Stage 2 model:\", accuracy_2)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_2 = model_2.predict(X_test_combined_1)\n",
    "y_pred_2 = np.argmax(y_pred_probabilities_2, axis=1)\n",
    "report_2 = classification_report(y_test_1, y_pred_2)\n",
    "print(\"Classification Report for Stage 2 model:\")\n",
    "print(report_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 3.0072 - accuracy: 0.4531 - val_loss: 1.4295 - val_accuracy: 0.6903 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 1.6287 - accuracy: 0.6349 - val_loss: 0.9727 - val_accuracy: 0.7575 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 1.2960 - accuracy: 0.6858 - val_loss: 0.7394 - val_accuracy: 0.8075 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 1.1583 - accuracy: 0.7081 - val_loss: 0.6114 - val_accuracy: 0.8315 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.0522 - accuracy: 0.7279 - val_loss: 0.5647 - val_accuracy: 0.8462 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.9664 - accuracy: 0.7457 - val_loss: 0.5470 - val_accuracy: 0.8505 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.9126 - accuracy: 0.7560 - val_loss: 0.4993 - val_accuracy: 0.8682 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.8621 - accuracy: 0.7671 - val_loss: 0.4839 - val_accuracy: 0.8661 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.8264 - accuracy: 0.7732 - val_loss: 0.4623 - val_accuracy: 0.8755 - lr: 4.3541e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.7871 - accuracy: 0.7834 - val_loss: 0.4524 - val_accuracy: 0.8784 - lr: 3.7010e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.7613 - accuracy: 0.7924 - val_loss: 0.4384 - val_accuracy: 0.8794 - lr: 3.1459e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7408 - accuracy: 0.7951 - val_loss: 0.4270 - val_accuracy: 0.8855 - lr: 2.6740e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7120 - accuracy: 0.8001 - val_loss: 0.4118 - val_accuracy: 0.8905 - lr: 2.2729e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6907 - accuracy: 0.8070 - val_loss: 0.4187 - val_accuracy: 0.8874 - lr: 1.9319e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6822 - accuracy: 0.8085 - val_loss: 0.4150 - val_accuracy: 0.8875 - lr: 1.6422e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6613 - accuracy: 0.8148 - val_loss: 0.4105 - val_accuracy: 0.8899 - lr: 1.3958e-04\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6616 - accuracy: 0.8160 - val_loss: 0.4032 - val_accuracy: 0.8936 - lr: 1.0469e-04\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 4s 3ms/step - loss: 0.6441 - accuracy: 0.8208 - val_loss: 0.3965 - val_accuracy: 0.8948 - lr: 7.8515e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6407 - accuracy: 0.8218 - val_loss: 0.3956 - val_accuracy: 0.8950 - lr: 5.8887e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6313 - accuracy: 0.8232 - val_loss: 0.3976 - val_accuracy: 0.8948 - lr: 4.1221e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6270 - accuracy: 0.8247 - val_loss: 0.3951 - val_accuracy: 0.8959 - lr: 2.8854e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6283 - accuracy: 0.8233 - val_loss: 0.3949 - val_accuracy: 0.8951 - lr: 2.0198e-05\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6241 - accuracy: 0.8241 - val_loss: 0.3943 - val_accuracy: 0.8947 - lr: 1.4139e-05\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6187 - accuracy: 0.8266 - val_loss: 0.3935 - val_accuracy: 0.8958 - lr: 9.8971e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6171 - accuracy: 0.8278 - val_loss: 0.3941 - val_accuracy: 0.8956 - lr: 6.9280e-06\n",
      "Epoch 26/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6211 - accuracy: 0.8241 - val_loss: 0.3940 - val_accuracy: 0.8951 - lr: 4.8496e-06\n",
      "Epoch 27/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6160 - accuracy: 0.8259 - val_loss: 0.3941 - val_accuracy: 0.8957 - lr: 3.3947e-06\n",
      "Epoch 28/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6217 - accuracy: 0.8252 - val_loss: 0.3930 - val_accuracy: 0.8953 - lr: 2.3763e-06\n",
      "Epoch 29/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6248 - accuracy: 0.8248 - val_loss: 0.3938 - val_accuracy: 0.8962 - lr: 1.6634e-06\n",
      "Epoch 30/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6280 - accuracy: 0.8222 - val_loss: 0.3929 - val_accuracy: 0.8952 - lr: 1.1644e-06\n",
      "Epoch 31/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6169 - accuracy: 0.8250 - val_loss: 0.3926 - val_accuracy: 0.8955 - lr: 8.1507e-07\n",
      "Epoch 32/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6132 - accuracy: 0.8271 - val_loss: 0.3933 - val_accuracy: 0.8956 - lr: 5.7055e-07\n",
      "Epoch 33/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6179 - accuracy: 0.8256 - val_loss: 0.3928 - val_accuracy: 0.8950 - lr: 3.9938e-07\n",
      "Epoch 34/60\n",
      "1251/1251 [==============================] - 4s 4ms/step - loss: 0.6247 - accuracy: 0.8262 - val_loss: 0.3926 - val_accuracy: 0.8958 - lr: 2.7957e-07\n",
      "Epoch 35/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6166 - accuracy: 0.8258 - val_loss: 0.3932 - val_accuracy: 0.8954 - lr: 1.9570e-07\n",
      "Epoch 36/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6185 - accuracy: 0.8274 - val_loss: 0.3941 - val_accuracy: 0.8958 - lr: 1.3699e-07\n",
      " 30/313 [=>............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8917"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\3792186953.py:41: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_3, 'model_3_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8955\n",
      "Accuracy for Stage 3 model: 0.8954940438270569\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "Classification Report for Stage 3 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00        18\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.78      1.00      0.88        70\n",
      "           6       1.00      0.33      0.50         3\n",
      "           8       0.75      0.86      0.80         7\n",
      "          10       0.67      1.00      0.80         6\n",
      "          12       0.80      1.00      0.89         8\n",
      "          13       1.00      1.00      1.00        10\n",
      "          14       0.83      0.97      0.89        39\n",
      "          15       1.00      0.50      0.67         6\n",
      "          16       1.00      1.00      1.00         3\n",
      "          17       0.89      0.91      0.90        35\n",
      "          18       0.89      0.92      0.91       101\n",
      "          19       0.92      0.92      0.92        25\n",
      "          20       0.00      0.00      0.00         3\n",
      "          22       0.91      0.95      0.93        21\n",
      "          23       1.00      1.00      1.00        83\n",
      "          24       0.63      1.00      0.77        34\n",
      "          25       0.86      0.29      0.43        21\n",
      "          26       1.00      1.00      1.00       113\n",
      "          27       0.93      1.00      0.96        25\n",
      "          28       0.95      1.00      0.97        55\n",
      "          29       0.00      0.00      0.00        18\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       1.00      1.00      1.00        16\n",
      "          32       0.69      1.00      0.81       135\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.91      0.98      0.95        53\n",
      "          35       0.98      1.00      0.99        63\n",
      "          37       1.00      0.70      0.82        10\n",
      "          38       1.00      1.00      1.00         9\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.73      1.00      0.84         8\n",
      "          42       1.00      1.00      1.00        43\n",
      "          43       0.00      0.00      0.00         3\n",
      "          44       1.00      0.50      0.67        12\n",
      "          46       1.00      0.67      0.80         6\n",
      "          47       0.50      0.67      0.57         6\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       1.00      0.90      0.95        10\n",
      "          50       1.00      1.00      1.00         9\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         2\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         6\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00         3\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       1.00      1.00      1.00        13\n",
      "          64       0.92      0.98      0.95        61\n",
      "          65       1.00      1.00      1.00         4\n",
      "          66       1.00      0.22      0.36         9\n",
      "          68       0.00      0.00      0.00         4\n",
      "          69       0.00      0.00      0.00         2\n",
      "          70       1.00      1.00      1.00         4\n",
      "          71       1.00      1.00      1.00         1\n",
      "          73       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         4\n",
      "          76       1.00      1.00      1.00         3\n",
      "          77       0.91      1.00      0.95        50\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.64      0.64      0.64        11\n",
      "          81       0.00      0.00      0.00         1\n",
      "          83       0.96      0.81      0.88        27\n",
      "          84       0.88      1.00      0.93         7\n",
      "          85       1.00      0.56      0.71         9\n",
      "          86       0.76      0.95      0.85        41\n",
      "          87       0.50      1.00      0.67         3\n",
      "          89       0.00      0.00      0.00         4\n",
      "          90       0.95      1.00      0.97        18\n",
      "          91       1.00      1.00      1.00         1\n",
      "          93       0.00      0.00      0.00         5\n",
      "          94       0.85      0.92      0.88        36\n",
      "          95       0.95      0.97      0.96        76\n",
      "          96       0.95      0.99      0.97        84\n",
      "          99       0.33      1.00      0.50         1\n",
      "         100       0.33      0.09      0.14        11\n",
      "         101       1.00      1.00      1.00         3\n",
      "         102       0.92      0.88      0.90        25\n",
      "         103       0.93      1.00      0.96       243\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       0.75      1.00      0.86         6\n",
      "         107       0.92      0.92      0.92        26\n",
      "         108       0.00      0.00      0.00         8\n",
      "         109       0.91      1.00      0.95        29\n",
      "         110       0.86      0.90      0.88        42\n",
      "         112       1.00      1.00      1.00         3\n",
      "         113       0.00      0.00      0.00         3\n",
      "         114       0.83      1.00      0.91       184\n",
      "         115       1.00      1.00      1.00         6\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.00      0.00      0.00         4\n",
      "         118       0.95      0.99      0.97       159\n",
      "         119       0.95      0.93      0.94       395\n",
      "         120       0.83      1.00      0.91         5\n",
      "         121       1.00      0.40      0.57         5\n",
      "         123       1.00      0.69      0.81        16\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       0.00      0.00      0.00         4\n",
      "         127       1.00      0.50      0.67         2\n",
      "         128       1.00      1.00      1.00         6\n",
      "         129       0.88      0.76      0.81        29\n",
      "         130       0.75      0.55      0.63        11\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.00      0.00      0.00        11\n",
      "         133       1.00      1.00      1.00         4\n",
      "         134       1.00      0.62      0.77         8\n",
      "         136       0.83      1.00      0.91         5\n",
      "         137       1.00      1.00      1.00       236\n",
      "         138       0.89      0.96      0.92       131\n",
      "         139       1.00      1.00      1.00       200\n",
      "         140       1.00      1.00      1.00         1\n",
      "         141       0.60      0.50      0.55         6\n",
      "         142       0.85      0.97      0.90        29\n",
      "         143       0.00      0.00      0.00         1\n",
      "         144       0.00      0.00      0.00         3\n",
      "         145       0.00      0.00      0.00         2\n",
      "         147       0.00      0.00      0.00         1\n",
      "         148       1.00      1.00      1.00         5\n",
      "         151       0.67      0.60      0.63        10\n",
      "         152       0.70      1.00      0.82        21\n",
      "         153       1.00      0.50      0.67        10\n",
      "         154       1.00      0.75      0.86         4\n",
      "         155       0.00      0.00      0.00         6\n",
      "         156       0.00      0.00      0.00         1\n",
      "         158       1.00      0.50      0.67        10\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       1.00      1.00      1.00         3\n",
      "         161       0.00      0.00      0.00         2\n",
      "         162       0.70      0.93      0.80        28\n",
      "         163       1.00      0.25      0.40         4\n",
      "         164       1.00      0.33      0.50         3\n",
      "         165       0.00      0.00      0.00         7\n",
      "         166       0.80      0.92      0.86        13\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       1.00      0.80      0.89         5\n",
      "         169       0.00      0.00      0.00         2\n",
      "         171       1.00      0.33      0.50         6\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.74      0.97      0.84        29\n",
      "         175       0.89      0.57      0.70        14\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       1.00      1.00      1.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         179       0.98      1.00      0.99        58\n",
      "         180       0.50      0.33      0.40         3\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.76      0.86      0.80        69\n",
      "         183       1.00      0.95      0.97        19\n",
      "         184       0.85      1.00      0.92        17\n",
      "         186       0.55      0.84      0.67        19\n",
      "         187       0.33      0.25      0.29         4\n",
      "         188       0.54      1.00      0.70         7\n",
      "         189       1.00      1.00      1.00         2\n",
      "         190       0.83      0.56      0.67         9\n",
      "         191       0.00      0.00      0.00         2\n",
      "         192       1.00      0.83      0.91        12\n",
      "         194       0.00      0.00      0.00         2\n",
      "         195       1.00      0.50      0.67         4\n",
      "         196       1.00      1.00      1.00         9\n",
      "         197       0.80      0.67      0.73         6\n",
      "         198       0.95      0.90      0.93        63\n",
      "         199       1.00      0.82      0.90        11\n",
      "         200       1.00      1.00      1.00        10\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.86      0.94      0.90        32\n",
      "         206       0.68      0.81      0.74        21\n",
      "         207       1.00      0.33      0.50         3\n",
      "         208       0.92      0.65      0.76        17\n",
      "         209       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         7\n",
      "         211       0.86      1.00      0.92         6\n",
      "         212       0.00      0.00      0.00         5\n",
      "         213       1.00      0.79      0.88        19\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.00      0.00      0.00         3\n",
      "         217       1.00      1.00      1.00         7\n",
      "         218       0.89      0.92      0.91        26\n",
      "         220       1.00      0.40      0.57         5\n",
      "         221       0.00      0.00      0.00         2\n",
      "         223       1.00      1.00      1.00         2\n",
      "         224       0.82      1.00      0.90         9\n",
      "         225       0.00      0.00      0.00         2\n",
      "         228       0.86      1.00      0.92         6\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00         1\n",
      "         231       1.00      0.17      0.29         6\n",
      "         232       0.70      1.00      0.82        35\n",
      "         233       0.67      1.00      0.80        18\n",
      "         234       1.00      0.25      0.40         8\n",
      "         236       1.00      0.99      0.99        71\n",
      "         238       0.00      0.00      0.00         5\n",
      "         239       0.00      0.00      0.00         2\n",
      "         240       0.71      0.71      0.71         7\n",
      "         241       0.93      1.00      0.96        37\n",
      "         242       1.00      0.92      0.96        12\n",
      "         243       0.00      0.00      0.00         2\n",
      "         244       1.00      0.17      0.29        18\n",
      "         245       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         1\n",
      "         248       0.92      0.92      0.92        13\n",
      "         249       0.00      0.00      0.00        10\n",
      "         250       0.79      0.73      0.76        26\n",
      "         251       1.00      1.00      1.00        32\n",
      "         252       0.00      0.00      0.00         1\n",
      "         253       0.00      0.00      0.00         2\n",
      "         254       0.00      0.00      0.00         1\n",
      "         256       1.00      1.00      1.00        19\n",
      "         257       0.00      0.00      0.00         3\n",
      "         258       0.73      0.57      0.64        28\n",
      "         259       1.00      0.33      0.50         3\n",
      "         260       0.00      0.00      0.00         2\n",
      "         262       1.00      1.00      1.00         7\n",
      "         263       1.00      0.25      0.40         4\n",
      "         266       1.00      0.25      0.40         4\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         1\n",
      "         273       0.82      0.90      0.86       112\n",
      "         274       0.00      0.00      0.00         4\n",
      "         275       0.54      1.00      0.70        28\n",
      "         278       1.00      0.75      0.86         4\n",
      "         280       0.00      0.00      0.00         3\n",
      "         282       0.93      0.95      0.94       146\n",
      "         283       0.84      1.00      0.91        26\n",
      "         284       0.50      0.60      0.55         5\n",
      "         285       0.79      0.73      0.76        15\n",
      "         288       0.88      0.58      0.70        12\n",
      "         289       0.00      0.00      0.00         8\n",
      "         290       0.72      0.76      0.74        17\n",
      "         291       0.80      0.92      0.86        26\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.89      1.00      0.94        33\n",
      "         295       1.00      1.00      1.00         7\n",
      "         297       1.00      1.00      1.00        23\n",
      "         299       0.00      0.00      0.00         2\n",
      "         300       0.60      0.86      0.71         7\n",
      "         301       0.00      0.00      0.00         1\n",
      "         302       1.00      0.50      0.67         2\n",
      "         303       0.77      1.00      0.87        17\n",
      "         306       0.82      0.98      0.89        60\n",
      "         307       0.00      0.00      0.00         1\n",
      "         308       1.00      1.00      1.00         5\n",
      "         309       0.74      1.00      0.85        59\n",
      "         311       0.85      0.92      0.88        12\n",
      "         313       0.86      0.86      0.86         7\n",
      "         314       0.92      0.88      0.90        26\n",
      "         315       1.00      0.89      0.94        35\n",
      "         317       1.00      0.50      0.67         2\n",
      "         318       1.00      1.00      1.00         1\n",
      "         321       0.93      0.67      0.78        21\n",
      "         322       1.00      0.68      0.81        25\n",
      "         323       1.00      1.00      1.00         5\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      1.00      1.00       191\n",
      "         327       0.00      0.00      0.00         1\n",
      "         329       0.00      0.00      0.00         4\n",
      "         330       0.64      1.00      0.78        21\n",
      "         333       1.00      1.00      1.00        16\n",
      "         334       0.00      0.00      0.00         1\n",
      "         335       0.00      0.00      0.00         1\n",
      "         336       0.00      0.00      0.00         4\n",
      "         337       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         2\n",
      "         340       0.80      1.00      0.89         4\n",
      "         341       0.00      0.00      0.00         1\n",
      "         342       1.00      1.00      1.00        25\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.97      1.00      0.98        31\n",
      "         345       1.00      1.00      1.00         2\n",
      "         347       0.50      1.00      0.67         2\n",
      "         349       0.98      0.96      0.97        52\n",
      "         350       0.00      0.00      0.00         2\n",
      "         351       1.00      0.33      0.50         3\n",
      "         352       0.75      1.00      0.86         6\n",
      "         353       0.00      0.00      0.00         1\n",
      "         355       0.94      1.00      0.97        88\n",
      "         356       1.00      0.88      0.93         8\n",
      "         357       0.73      0.96      0.83        80\n",
      "         358       0.33      1.00      0.50         1\n",
      "         359       1.00      0.33      0.50         6\n",
      "         360       0.75      1.00      0.86         3\n",
      "         361       1.00      0.95      0.98        21\n",
      "         362       0.40      0.50      0.44         4\n",
      "         364       0.25      0.18      0.21        11\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       1.00      1.00      1.00         2\n",
      "         368       1.00      1.00      1.00         8\n",
      "         369       1.00      1.00      1.00        13\n",
      "         370       0.67      1.00      0.80         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.97      1.00      0.99        33\n",
      "         373       0.93      1.00      0.96        13\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       1.00      1.00      1.00        54\n",
      "         378       1.00      1.00      1.00         2\n",
      "         379       0.78      1.00      0.88         7\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.92      0.98      0.95       102\n",
      "         382       1.00      0.62      0.76        13\n",
      "         383       0.82      1.00      0.90        14\n",
      "         384       0.97      0.86      0.91        35\n",
      "         385       0.00      0.00      0.00         0\n",
      "         387       0.96      1.00      0.98        47\n",
      "         389       1.00      0.29      0.44         7\n",
      "         390       0.80      1.00      0.89         4\n",
      "         391       0.00      0.00      0.00         2\n",
      "         392       0.52      0.92      0.67        13\n",
      "         393       0.62      1.00      0.77        10\n",
      "         394       0.00      0.00      0.00         1\n",
      "         395       0.00      0.00      0.00         8\n",
      "         396       0.86      1.00      0.92         6\n",
      "         397       0.00      0.00      0.00         1\n",
      "         398       0.60      0.55      0.57        11\n",
      "         399       0.67      1.00      0.80         4\n",
      "         400       0.00      0.00      0.00         1\n",
      "         401       1.00      0.86      0.92         7\n",
      "         402       1.00      1.00      1.00         3\n",
      "         403       1.00      1.00      1.00        15\n",
      "         404       0.98      1.00      0.99       213\n",
      "         405       0.95      1.00      0.97        19\n",
      "         408       1.00      1.00      1.00        31\n",
      "         409       1.00      1.00      1.00         3\n",
      "         410       1.00      1.00      1.00         2\n",
      "         411       1.00      1.00      1.00         8\n",
      "         412       1.00      0.25      0.40         8\n",
      "         413       0.67      1.00      0.80         6\n",
      "         415       0.77      0.97      0.86        31\n",
      "         416       0.96      0.92      0.94        25\n",
      "         417       0.88      1.00      0.94        15\n",
      "         419       0.67      1.00      0.80         2\n",
      "         421       0.91      0.62      0.74        16\n",
      "         422       0.93      0.93      0.93        15\n",
      "         425       0.00      0.00      0.00         2\n",
      "         426       0.00      0.00      0.00         2\n",
      "         427       1.00      1.00      1.00         2\n",
      "         429       1.00      0.78      0.88         9\n",
      "         430       1.00      1.00      1.00         9\n",
      "         431       0.00      0.00      0.00         1\n",
      "         432       0.49      0.95      0.65        21\n",
      "         433       1.00      1.00      1.00         1\n",
      "         434       1.00      1.00      1.00        12\n",
      "         435       1.00      0.50      0.67         8\n",
      "         437       1.00      0.40      0.57         5\n",
      "         439       0.87      0.67      0.75        39\n",
      "         440       0.70      1.00      0.82        53\n",
      "         442       0.84      0.93      0.89       134\n",
      "         443       0.81      1.00      0.90        13\n",
      "         444       0.00      0.00      0.00         7\n",
      "         445       0.60      0.75      0.67         8\n",
      "         446       1.00      0.33      0.50         3\n",
      "         447       0.00      0.00      0.00         6\n",
      "         448       0.90      0.75      0.82        12\n",
      "         449       0.77      0.88      0.82        26\n",
      "         450       0.82      1.00      0.90         9\n",
      "         451       0.90      1.00      0.95        18\n",
      "         452       1.00      0.67      0.80         3\n",
      "         453       1.00      1.00      1.00         2\n",
      "         454       0.50      1.00      0.67         6\n",
      "         455       0.95      0.94      0.95        67\n",
      "         456       1.00      1.00      1.00         2\n",
      "         458       0.00      0.00      0.00         1\n",
      "         459       0.71      1.00      0.83         5\n",
      "         461       1.00      1.00      1.00        34\n",
      "         462       0.92      0.96      0.94       170\n",
      "         465       0.82      0.89      0.85        65\n",
      "         466       0.73      1.00      0.84         8\n",
      "         467       1.00      0.57      0.73        14\n",
      "         470       0.67      1.00      0.80         2\n",
      "         471       1.00      0.33      0.50         3\n",
      "         472       0.00      0.00      0.00         8\n",
      "         473       1.00      0.62      0.77         8\n",
      "         474       0.88      0.75      0.81        28\n",
      "         475       1.00      1.00      1.00        19\n",
      "         478       0.33      0.40      0.36         5\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       0.00      0.00      0.00         1\n",
      "         482       1.00      0.60      0.75        10\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         1\n",
      "         485       1.00      0.87      0.93        15\n",
      "         486       0.93      0.96      0.95        27\n",
      "         488       0.00      0.00      0.00         1\n",
      "         489       0.73      0.88      0.80        25\n",
      "         490       1.00      1.00      1.00         4\n",
      "         492       0.50      1.00      0.67         1\n",
      "         493       0.94      0.88      0.91        17\n",
      "         494       0.00      0.00      0.00         1\n",
      "         495       1.00      0.96      0.98        24\n",
      "         496       1.00      1.00      1.00        79\n",
      "         498       1.00      0.64      0.78        22\n",
      "         499       0.89      1.00      0.94        33\n",
      "         500       0.75      0.75      0.75         4\n",
      "         501       0.00      0.00      0.00        12\n",
      "         502       0.00      0.00      0.00         1\n",
      "         503       1.00      1.00      1.00         1\n",
      "         504       0.38      1.00      0.55         3\n",
      "         506       1.00      1.00      1.00         3\n",
      "         507       0.74      0.96      0.83        26\n",
      "         508       1.00      0.62      0.77         8\n",
      "         509       0.98      0.95      0.96        55\n",
      "         510       0.00      0.00      0.00         2\n",
      "         511       1.00      0.80      0.89         5\n",
      "         512       0.62      1.00      0.76         8\n",
      "         513       0.79      0.85      0.81        13\n",
      "         514       0.81      1.00      0.89        38\n",
      "         516       0.00      0.00      0.00         1\n",
      "         517       0.91      1.00      0.95        10\n",
      "         519       1.00      0.60      0.75         5\n",
      "         520       1.00      1.00      1.00         3\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       1.00      0.30      0.46        10\n",
      "         523       1.00      1.00      1.00        28\n",
      "         524       0.00      0.00      0.00         2\n",
      "         525       0.00      0.00      0.00         1\n",
      "         526       1.00      0.96      0.98        25\n",
      "         527       1.00      1.00      1.00         2\n",
      "         528       1.00      0.50      0.67         2\n",
      "         529       0.00      0.00      0.00         1\n",
      "         531       1.00      0.67      0.80         3\n",
      "         532       0.95      1.00      0.97        18\n",
      "         533       1.00      1.00      1.00         3\n",
      "         535       1.00      1.00      1.00        10\n",
      "         536       0.00      0.00      0.00        11\n",
      "         537       1.00      0.31      0.48        16\n",
      "         538       0.00      0.00      0.00         9\n",
      "         539       0.75      1.00      0.86         3\n",
      "         540       0.71      1.00      0.83         5\n",
      "         541       0.86      1.00      0.92         6\n",
      "         543       0.96      1.00      0.98        55\n",
      "         544       1.00      0.67      0.80         3\n",
      "         545       0.89      1.00      0.94        16\n",
      "         546       0.00      0.00      0.00         1\n",
      "         547       1.00      0.97      0.99        37\n",
      "         548       0.64      0.88      0.74         8\n",
      "         549       0.00      0.00      0.00         9\n",
      "         550       0.91      0.97      0.94       403\n",
      "         551       1.00      1.00      1.00         7\n",
      "         552       0.00      0.00      0.00         1\n",
      "         554       1.00      1.00      1.00         5\n",
      "         558       0.96      0.94      0.95      1254\n",
      "\n",
      "    accuracy                           0.90     10009\n",
      "   macro avg       0.62      0.60      0.59     10009\n",
      "weighted avg       0.87      0.90      0.88     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 3\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.85\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.75\n",
    "    else:\n",
    "        return lr * 0.7\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "model_3 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_2.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_3 = model_3.fit(X_train_combined_2, y_train_2, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_2, y_test_2),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_3, 'model_3_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_3, accuracy_3 = model_3.evaluate(X_test_combined_2, y_test_2)\n",
    "print(\"Accuracy for Stage 3 model:\", accuracy_3)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_3 = model_3.predict(X_test_combined_2)\n",
    "y_pred_3 = np.argmax(y_pred_probabilities_3, axis=1)\n",
    "report_3 = classification_report(y_test_2, y_pred_3)\n",
    "print(\"Classification Report for Stage 3 model:\")\n",
    "print(report_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 4 model...\n",
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 2.7823 - accuracy: 0.5589 - val_loss: 1.3688 - val_accuracy: 0.7346 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.5030 - accuracy: 0.6941 - val_loss: 0.9212 - val_accuracy: 0.8097 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.1854 - accuracy: 0.7391 - val_loss: 0.6993 - val_accuracy: 0.8415 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 1.0099 - accuracy: 0.7662 - val_loss: 0.6046 - val_accuracy: 0.8573 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.9004 - accuracy: 0.7837 - val_loss: 0.6445 - val_accuracy: 0.8472 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.8263 - accuracy: 0.7974 - val_loss: 0.5138 - val_accuracy: 0.8726 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7621 - accuracy: 0.8098 - val_loss: 0.4827 - val_accuracy: 0.8848 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7137 - accuracy: 0.8177 - val_loss: 0.4535 - val_accuracy: 0.8909 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6765 - accuracy: 0.8272 - val_loss: 0.4525 - val_accuracy: 0.8939 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6395 - accuracy: 0.8351 - val_loss: 0.4427 - val_accuracy: 0.8988 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6167 - accuracy: 0.8392 - val_loss: 0.4259 - val_accuracy: 0.9057 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5868 - accuracy: 0.8442 - val_loss: 0.4173 - val_accuracy: 0.9035 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5729 - accuracy: 0.8483 - val_loss: 0.4158 - val_accuracy: 0.9061 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5580 - accuracy: 0.8529 - val_loss: 0.4069 - val_accuracy: 0.9074 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5467 - accuracy: 0.8559 - val_loss: 0.4012 - val_accuracy: 0.9099 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5372 - accuracy: 0.8562 - val_loss: 0.3980 - val_accuracy: 0.9116 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5268 - accuracy: 0.8584 - val_loss: 0.3965 - val_accuracy: 0.9106 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5221 - accuracy: 0.8597 - val_loss: 0.3946 - val_accuracy: 0.9116 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5158 - accuracy: 0.8603 - val_loss: 0.3968 - val_accuracy: 0.9113 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5161 - accuracy: 0.8615 - val_loss: 0.3968 - val_accuracy: 0.9109 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5094 - accuracy: 0.8643 - val_loss: 0.3968 - val_accuracy: 0.9114 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5121 - accuracy: 0.8627 - val_loss: 0.3972 - val_accuracy: 0.9115 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5110 - accuracy: 0.8614 - val_loss: 0.3974 - val_accuracy: 0.9116 - lr: 3.8203e-06\n",
      "  1/313 [..............................] - ETA: 6s - loss: 0.9719 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\1033576444.py:42: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_4, 'model_4_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3946 - accuracy: 0.9116\n",
      "Accuracy for Stage 4 model: 0.9115795493125916\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Classification Report for Stage 4 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "           6       1.00      1.00      1.00        56\n",
      "           8       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.31      1.00      0.47         8\n",
      "          19       0.67      0.33      0.44         6\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.50      0.50      0.50         4\n",
      "          23       0.67      1.00      0.80         2\n",
      "          24       0.79      1.00      0.88        15\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.66      1.00      0.79        23\n",
      "          31       1.00      1.00      1.00        22\n",
      "          33       0.94      0.97      0.96        33\n",
      "          34       0.95      0.95      0.95        22\n",
      "          35       0.75      0.82      0.78        11\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       0.50      0.11      0.18         9\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       1.00      0.62      0.77         8\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       0.92      1.00      0.96        24\n",
      "          46       0.70      1.00      0.82        28\n",
      "          47       1.00      1.00      1.00         3\n",
      "          48       1.00      1.00      1.00         1\n",
      "          49       0.00      0.00      0.00         3\n",
      "          51       1.00      0.78      0.88         9\n",
      "          52       0.92      1.00      0.96        12\n",
      "          54       0.87      1.00      0.93        26\n",
      "          55       0.92      1.00      0.96        11\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       1.00      0.33      0.50         3\n",
      "          60       1.00      0.67      0.80         3\n",
      "          61       1.00      1.00      1.00         4\n",
      "          63       0.00      0.00      0.00         3\n",
      "          64       0.00      0.00      0.00         4\n",
      "          65       0.00      0.00      0.00         1\n",
      "          66       1.00      1.00      1.00        33\n",
      "          67       1.00      1.00      1.00       131\n",
      "          68       0.75      1.00      0.86         6\n",
      "          69       1.00      0.90      0.95        10\n",
      "          70       0.00      0.00      0.00         3\n",
      "          71       0.83      0.62      0.71         8\n",
      "          72       1.00      0.50      0.67         2\n",
      "          76       1.00      1.00      1.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.80      1.00      0.89        39\n",
      "          81       0.98      0.96      0.97        47\n",
      "          82       0.00      0.00      0.00         1\n",
      "          83       0.70      1.00      0.82         7\n",
      "          85       0.67      0.67      0.67         3\n",
      "          87       0.60      1.00      0.75         3\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         1\n",
      "          91       0.74      0.65      0.69        31\n",
      "          92       0.00      0.00      0.00         2\n",
      "          93       0.00      0.00      0.00         3\n",
      "          95       1.00      0.67      0.80         3\n",
      "          97       0.33      0.50      0.40         2\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       1.00      0.92      0.96        13\n",
      "         100       0.00      0.00      0.00         4\n",
      "         101       1.00      0.50      0.67         2\n",
      "         102       0.38      1.00      0.56         5\n",
      "         103       1.00      1.00      1.00         6\n",
      "         105       1.00      0.67      0.80         3\n",
      "         106       1.00      1.00      1.00       101\n",
      "         109       0.00      0.00      0.00         5\n",
      "         110       0.87      0.94      0.90        48\n",
      "         112       0.72      0.87      0.79        15\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       0.50      1.00      0.67         3\n",
      "         115       0.00      0.00      0.00         2\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.00      0.00      0.00         2\n",
      "         118       0.76      1.00      0.86        25\n",
      "         120       0.00      0.00      0.00         4\n",
      "         122       0.00      0.00      0.00         2\n",
      "         123       1.00      1.00      1.00        36\n",
      "         125       1.00      1.00      1.00         5\n",
      "         126       0.00      0.00      0.00         2\n",
      "         127       0.86      1.00      0.92         6\n",
      "         128       0.00      0.00      0.00         1\n",
      "         129       0.00      0.00      0.00         1\n",
      "         130       1.00      0.60      0.75         5\n",
      "         132       0.00      0.00      0.00         4\n",
      "         133       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       1.00      0.11      0.20         9\n",
      "         136       0.75      1.00      0.86         3\n",
      "         138       0.91      0.77      0.83        13\n",
      "         139       0.00      0.00      0.00         1\n",
      "         143       0.00      0.00      0.00         2\n",
      "         144       0.00      0.00      0.00         5\n",
      "         146       1.00      0.33      0.50         3\n",
      "         147       0.92      0.85      0.88        13\n",
      "         148       1.00      1.00      1.00         1\n",
      "         149       0.00      0.00      0.00         5\n",
      "         150       1.00      1.00      1.00         7\n",
      "         151       1.00      1.00      1.00        18\n",
      "         154       0.00      0.00      0.00         1\n",
      "         155       0.98      1.00      0.99        41\n",
      "         156       0.00      0.00      0.00         2\n",
      "         157       0.00      0.00      0.00         1\n",
      "         158       0.76      1.00      0.87        13\n",
      "         159       1.00      1.00      1.00         7\n",
      "         160       0.00      0.00      0.00         2\n",
      "         162       1.00      1.00      1.00       184\n",
      "         164       0.20      1.00      0.33         1\n",
      "         165       1.00      1.00      1.00        26\n",
      "         167       0.46      1.00      0.63         6\n",
      "         168       0.00      0.00      0.00         6\n",
      "         169       0.78      1.00      0.88        54\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.00      0.00      0.00         6\n",
      "         173       1.00      1.00      1.00        14\n",
      "         174       0.56      1.00      0.71         5\n",
      "         175       0.91      1.00      0.96       204\n",
      "         176       1.00      1.00      1.00         6\n",
      "         177       1.00      0.88      0.93         8\n",
      "         178       0.76      1.00      0.87        13\n",
      "         180       1.00      0.90      0.95        10\n",
      "         182       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         1\n",
      "         186       0.69      0.92      0.79        12\n",
      "         187       0.94      0.94      0.94        18\n",
      "         188       0.00      0.00      0.00         1\n",
      "         189       0.00      0.00      0.00         1\n",
      "         191       0.57      1.00      0.73         8\n",
      "         192       0.00      0.00      0.00         1\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       0.67      1.00      0.80         2\n",
      "         197       0.00      0.00      0.00         2\n",
      "         199       0.71      0.96      0.82        26\n",
      "         200       0.00      0.00      0.00         2\n",
      "         203       1.00      0.50      0.67         2\n",
      "         204       0.50      0.75      0.60         4\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         3\n",
      "         207       0.00      0.00      0.00         5\n",
      "         210       0.65      0.96      0.77        23\n",
      "         211       0.98      0.95      0.97        44\n",
      "         212       0.00      0.00      0.00         1\n",
      "         214       0.60      1.00      0.75         3\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       1.00      1.00      1.00        31\n",
      "         220       0.00      0.00      0.00         1\n",
      "         221       1.00      1.00      1.00         3\n",
      "         224       0.81      0.76      0.79        17\n",
      "         228       0.75      0.98      0.85        61\n",
      "         229       0.82      1.00      0.90        14\n",
      "         230       1.00      0.62      0.77         8\n",
      "         231       0.68      1.00      0.81        13\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       0.00      0.00      0.00         1\n",
      "         237       0.33      1.00      0.50         2\n",
      "         239       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         244       0.96      0.69      0.80        32\n",
      "         248       0.00      0.00      0.00         1\n",
      "         254       0.54      1.00      0.70         7\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.50      1.00      0.67         2\n",
      "         258       1.00      0.50      0.67         4\n",
      "         259       0.94      1.00      0.97        30\n",
      "         261       0.50      1.00      0.67         2\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.80      1.00      0.89         8\n",
      "         266       0.50      0.50      0.50         2\n",
      "         267       1.00      1.00      1.00        14\n",
      "         268       1.00      0.67      0.80         3\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.79      1.00      0.88        15\n",
      "         271       0.81      0.83      0.82        36\n",
      "         272       0.87      1.00      0.93        13\n",
      "         273       1.00      0.67      0.80         3\n",
      "         275       1.00      0.40      0.57         5\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       1.00      0.67      0.80         6\n",
      "         283       0.85      0.97      0.90        29\n",
      "         284       1.00      0.50      0.67         4\n",
      "         285       0.00      0.00      0.00         1\n",
      "         286       0.00      0.00      0.00         1\n",
      "         288       0.56      0.82      0.67        11\n",
      "         289       0.00      0.00      0.00         1\n",
      "         291       0.93      1.00      0.96        13\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.90      1.00      0.95        26\n",
      "         295       0.55      0.75      0.63         8\n",
      "         297       0.00      0.00      0.00         2\n",
      "         298       0.37      1.00      0.54        10\n",
      "         299       0.00      0.00      0.00         4\n",
      "         300       0.00      0.00      0.00         2\n",
      "         301       0.64      0.95      0.77        40\n",
      "         302       1.00      0.10      0.18        10\n",
      "         304       1.00      1.00      1.00        19\n",
      "         306       1.00      0.83      0.91         6\n",
      "         307       1.00      1.00      1.00        11\n",
      "         308       0.00      0.00      0.00         2\n",
      "         309       0.91      0.90      0.91        48\n",
      "         312       0.47      0.64      0.55        14\n",
      "         313       0.86      0.75      0.80         8\n",
      "         314       0.00      0.00      0.00         1\n",
      "         315       1.00      0.92      0.96        13\n",
      "         317       0.00      0.00      0.00         1\n",
      "         320       0.00      0.00      0.00         3\n",
      "         324       0.00      0.00      0.00         1\n",
      "         328       1.00      1.00      1.00         1\n",
      "         329       0.50      1.00      0.67         2\n",
      "         330       0.00      0.00      0.00         1\n",
      "         332       0.00      0.00      0.00         3\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.95      1.00      0.98        80\n",
      "         335       1.00      1.00      1.00        41\n",
      "         336       0.95      1.00      0.98        20\n",
      "         337       0.00      0.00      0.00         2\n",
      "         340       1.00      1.00      1.00         2\n",
      "         341       0.50      1.00      0.67         7\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       0.00      0.00      0.00         2\n",
      "         347       0.00      0.00      0.00         1\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       1.00      1.00      1.00        21\n",
      "         350       0.00      0.00      0.00         1\n",
      "         351       0.67      0.86      0.75         7\n",
      "         352       0.00      0.00      0.00         2\n",
      "         353       1.00      0.50      0.67         6\n",
      "         354       0.00      0.00      0.00         7\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       0.00      0.00      0.00         3\n",
      "         358       0.89      1.00      0.94         8\n",
      "         360       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         3\n",
      "         363       1.00      1.00      1.00         3\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         1\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.00      0.00      0.00         1\n",
      "         373       0.00      0.00      0.00         1\n",
      "         374       0.71      1.00      0.83         5\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         1\n",
      "         378       0.67      1.00      0.80         4\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.92      1.00      0.96        33\n",
      "         381       1.00      0.44      0.61        16\n",
      "         382       0.79      1.00      0.88        41\n",
      "         383       1.00      0.86      0.92         7\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       1.00      1.00      1.00         2\n",
      "         388       0.90      1.00      0.95         9\n",
      "         389       1.00      1.00      1.00         1\n",
      "         390       0.00      0.00      0.00         1\n",
      "         392       0.79      0.52      0.63        21\n",
      "         393       0.75      1.00      0.86         6\n",
      "         394       1.00      0.60      0.75         5\n",
      "         395       1.00      1.00      1.00        13\n",
      "         396       0.86      1.00      0.92         6\n",
      "         397       0.71      0.91      0.80        11\n",
      "         398       0.00      0.00      0.00         3\n",
      "         399       0.00      0.00      0.00         4\n",
      "         400       0.50      0.33      0.40         3\n",
      "         402       0.00      0.00      0.00         5\n",
      "         403       1.00      1.00      1.00         3\n",
      "         404       1.00      1.00      1.00        23\n",
      "         405       1.00      1.00      1.00         6\n",
      "         406       1.00      1.00      1.00        25\n",
      "         407       0.82      0.75      0.78        12\n",
      "         409       0.00      0.00      0.00         1\n",
      "         411       0.44      0.80      0.57         5\n",
      "         413       0.67      0.67      0.67         3\n",
      "         415       0.75      0.86      0.80         7\n",
      "         416       0.00      0.00      0.00         1\n",
      "         418       1.00      0.89      0.94         9\n",
      "         421       1.00      1.00      1.00         4\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       1.00      1.00      1.00         2\n",
      "         426       1.00      1.00      1.00         1\n",
      "         427       1.00      1.00      1.00         4\n",
      "         428       0.86      0.50      0.63        36\n",
      "         429       1.00      0.91      0.95        11\n",
      "         431       0.86      1.00      0.92         6\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.75      1.00      0.86         6\n",
      "         435       0.92      1.00      0.96        11\n",
      "         436       0.90      1.00      0.95         9\n",
      "         437       0.83      0.83      0.83        46\n",
      "         438       1.00      1.00      1.00         1\n",
      "         439       0.00      0.00      0.00         2\n",
      "         441       0.61      0.92      0.73        37\n",
      "         443       0.80      1.00      0.89        69\n",
      "         444       0.73      0.92      0.81        12\n",
      "         445       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         1\n",
      "         451       0.00      0.00      0.00         1\n",
      "         454       0.00      0.00      0.00         1\n",
      "         457       0.83      0.83      0.83         6\n",
      "         458       0.67      1.00      0.80         6\n",
      "         462       1.00      0.94      0.97        34\n",
      "         463       1.00      1.00      1.00        17\n",
      "         464       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         3\n",
      "         467       0.00      0.00      0.00         1\n",
      "         468       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         2\n",
      "         470       1.00      0.50      0.67         2\n",
      "         471       0.86      1.00      0.92         6\n",
      "         474       0.00      0.00      0.00         3\n",
      "         475       1.00      1.00      1.00        21\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       1.00      0.67      0.80         3\n",
      "         478       1.00      0.78      0.88         9\n",
      "         479       1.00      1.00      1.00         7\n",
      "         480       0.52      0.79      0.63        14\n",
      "         481       0.86      0.94      0.90        65\n",
      "         483       0.80      1.00      0.89         4\n",
      "         484       1.00      1.00      1.00         3\n",
      "         485       1.00      1.00      1.00         5\n",
      "         486       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         2\n",
      "         492       0.94      0.96      0.95        91\n",
      "         493       0.00      0.00      0.00         1\n",
      "         494       0.92      0.92      0.92        38\n",
      "         495       0.00      0.00      0.00         4\n",
      "         496       0.67      0.50      0.57         4\n",
      "         498       1.00      1.00      1.00         5\n",
      "         499       0.00      0.00      0.00         2\n",
      "         501       0.43      0.86      0.57         7\n",
      "         502       0.00      0.00      0.00         2\n",
      "         503       0.75      0.75      0.75         4\n",
      "         504       0.50      1.00      0.67         1\n",
      "         505       0.00      0.00      0.00         1\n",
      "         506       0.98      1.00      0.99        47\n",
      "         507       0.99      1.00      1.00       110\n",
      "         509       1.00      0.95      0.97        19\n",
      "         510       0.77      0.83      0.80        12\n",
      "         512       0.76      1.00      0.86        19\n",
      "         513       0.00      0.00      0.00         2\n",
      "         515       1.00      1.00      1.00        19\n",
      "         517       0.40      1.00      0.57         2\n",
      "         519       1.00      1.00      1.00        11\n",
      "         520       0.00      0.00      0.00         1\n",
      "         521       1.00      0.79      0.88        29\n",
      "         522       0.00      0.00      0.00         5\n",
      "         523       0.80      1.00      0.89         4\n",
      "         524       0.00      0.00      0.00         1\n",
      "         525       1.00      1.00      1.00        10\n",
      "         527       0.00      0.00      0.00         1\n",
      "         528       0.00      0.00      0.00         1\n",
      "         531       1.00      1.00      1.00        28\n",
      "         532       0.00      0.00      0.00         1\n",
      "         533       0.00      0.00      0.00         1\n",
      "         536       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         2\n",
      "         540       1.00      0.88      0.93         8\n",
      "         541       0.00      0.00      0.00         2\n",
      "         542       0.67      1.00      0.80        10\n",
      "         543       0.75      1.00      0.86         6\n",
      "         545       1.00      1.00      1.00         9\n",
      "         546       0.00      0.00      0.00         9\n",
      "         547       0.87      0.96      0.91        27\n",
      "         548       0.48      0.73      0.58        15\n",
      "         549       0.00      0.00      0.00         1\n",
      "         550       0.82      1.00      0.90        23\n",
      "         552       0.00      0.00      0.00         1\n",
      "         553       1.00      1.00      1.00         5\n",
      "         554       0.33      1.00      0.50         1\n",
      "         555       0.79      0.93      0.86        29\n",
      "         556       1.00      1.00      1.00         3\n",
      "         557       0.00      0.00      0.00         1\n",
      "         558       0.33      0.50      0.40         2\n",
      "         560       0.00      0.00      0.00         2\n",
      "         561       0.00      0.00      0.00         1\n",
      "         562       0.00      0.00      0.00         4\n",
      "         563       0.00      0.00      0.00         5\n",
      "         564       0.89      0.73      0.80        11\n",
      "         567       1.00      1.00      1.00         2\n",
      "         568       0.88      0.78      0.82         9\n",
      "         570       0.00      0.00      0.00         1\n",
      "         571       1.00      1.00      1.00         2\n",
      "         572       0.98      1.00      0.99        51\n",
      "         573       1.00      0.94      0.97        16\n",
      "         575       0.25      0.50      0.33         2\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       1.00      1.00      1.00        18\n",
      "         578       1.00      1.00      1.00         1\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       1.00      0.42      0.59        12\n",
      "         582       0.80      1.00      0.89        20\n",
      "         583       1.00      0.50      0.67        10\n",
      "         584       0.61      0.93      0.74        15\n",
      "         585       0.67      1.00      0.80        16\n",
      "         586       0.00      0.00      0.00         1\n",
      "         587       0.60      0.60      0.60         5\n",
      "         588       0.67      0.67      0.67         3\n",
      "         589       1.00      1.00      1.00         6\n",
      "         590       0.00      0.00      0.00        10\n",
      "         591       0.83      1.00      0.91         5\n",
      "         592       0.00      0.00      0.00         2\n",
      "         593       0.71      0.62      0.67         8\n",
      "         594       0.00      0.00      0.00         4\n",
      "         595       0.00      0.00      0.00         1\n",
      "         596       0.00      0.00      0.00         1\n",
      "         597       0.33      0.50      0.40         2\n",
      "         598       1.00      0.25      0.40         4\n",
      "         599       1.00      0.75      0.86        12\n",
      "         601       0.77      0.77      0.77        13\n",
      "         602       0.00      0.00      0.00         1\n",
      "         603       0.00      0.00      0.00         1\n",
      "         604       0.00      0.00      0.00         1\n",
      "         608       0.00      0.00      0.00         4\n",
      "         609       1.00      0.90      0.95        10\n",
      "         610       0.88      0.88      0.88        16\n",
      "         612       0.87      1.00      0.93        13\n",
      "         613       1.00      1.00      1.00         1\n",
      "         614       0.79      1.00      0.88        23\n",
      "         616       0.86      0.89      0.88        36\n",
      "         617       0.93      0.97      0.95        91\n",
      "         619       0.00      0.00      0.00         2\n",
      "         621       1.00      1.00      1.00        11\n",
      "         622       1.00      1.00      1.00        29\n",
      "         623       1.00      0.94      0.97        16\n",
      "         624       0.00      0.00      0.00         3\n",
      "         625       0.76      0.94      0.84        17\n",
      "         626       0.92      0.93      0.93        61\n",
      "         627       1.00      0.55      0.71        11\n",
      "         628       0.00      0.00      0.00         1\n",
      "         629       0.00      0.00      0.00         8\n",
      "         630       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         2\n",
      "         632       0.00      0.00      0.00         1\n",
      "         634       0.80      0.80      0.80         5\n",
      "         635       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         1\n",
      "         638       0.67      0.77      0.71        13\n",
      "         639       0.92      1.00      0.96        22\n",
      "         641       0.90      1.00      0.95        28\n",
      "         642       0.44      1.00      0.62         4\n",
      "         643       0.77      1.00      0.87        10\n",
      "         644       1.00      0.33      0.50         3\n",
      "         645       1.00      1.00      1.00         1\n",
      "         646       0.82      1.00      0.90         9\n",
      "         647       1.00      1.00      1.00         2\n",
      "         648       0.00      0.00      0.00         1\n",
      "         649       0.00      0.00      0.00         1\n",
      "         651       1.00      1.00      1.00        10\n",
      "         652       0.67      0.80      0.73         5\n",
      "         653       1.00      1.00      1.00        34\n",
      "         654       1.00      0.25      0.40         4\n",
      "         655       1.00      1.00      1.00        15\n",
      "         658       0.00      0.00      0.00         1\n",
      "         659       0.00      0.00      0.00         1\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         1\n",
      "         666       0.50      1.00      0.67         2\n",
      "         668       0.90      0.75      0.82        24\n",
      "         669       0.00      0.00      0.00         1\n",
      "         670       0.80      0.89      0.84         9\n",
      "         671       0.00      0.00      0.00         2\n",
      "         673       0.00      0.00      0.00         3\n",
      "         674       0.83      1.00      0.91         5\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.83      1.00      0.90        19\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         1\n",
      "         683       0.25      0.50      0.33         2\n",
      "         684       0.00      0.00      0.00         2\n",
      "         685       0.00      0.00      0.00         1\n",
      "         686       0.00      0.00      0.00         2\n",
      "         688       0.00      0.00      0.00         3\n",
      "         690       0.00      0.00      0.00         1\n",
      "         691       0.62      1.00      0.76         8\n",
      "         692       0.80      1.00      0.89         4\n",
      "         694       0.50      1.00      0.67         3\n",
      "         695       0.00      0.00      0.00         0\n",
      "         696       0.00      0.00      0.00         2\n",
      "         697       0.00      0.00      0.00         1\n",
      "         698       1.00      1.00      1.00         2\n",
      "         699       0.00      0.00      0.00         2\n",
      "         702       0.00      0.00      0.00         3\n",
      "         704       0.00      0.00      0.00         2\n",
      "         707       0.92      0.99      0.96       358\n",
      "         709       0.00      0.00      0.00         3\n",
      "         710       0.00      0.00      0.00         1\n",
      "         711       1.00      0.77      0.87        13\n",
      "         713       0.00      0.00      0.00         1\n",
      "         715       1.00      0.50      0.67         2\n",
      "         716       0.00      0.00      0.00         1\n",
      "         717       0.50      1.00      0.67         2\n",
      "         719       0.97      0.97      0.97      4512\n",
      "\n",
      "    accuracy                           0.91     10009\n",
      "   macro avg       0.48      0.50      0.48     10009\n",
      "weighted avg       0.89      0.91      0.89     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 4\n",
    "print(\"Running Stage 4 model...\")\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Create model\n",
    "model_4 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_3.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_4.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_4 = model_4.fit(X_train_combined_3, y_train_3, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_3, y_test_3),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_4, 'model_4_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_4, accuracy_4 = model_4.evaluate(X_test_combined_3, y_test_3)\n",
    "print(\"Accuracy for Stage 4 model:\", accuracy_4)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_4 = model_4.predict(X_test_combined_3)\n",
    "y_pred_4 = np.argmax(y_pred_probabilities_4, axis=1)\n",
    "report_4 = classification_report(y_test_3, y_pred_4)\n",
    "print(\"Classification Report for Stage 4 model:\")\n",
    "print(report_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 5 model...\n",
      "Epoch 1/60\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.7319 - accuracy: 0.7524 - val_loss: 0.7192 - val_accuracy: 0.8691 - lr: 9.4000e-04\n",
      "Epoch 2/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.7697 - accuracy: 0.8532 - val_loss: 0.4820 - val_accuracy: 0.8931 - lr: 8.8360e-04\n",
      "Epoch 3/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.6136 - accuracy: 0.8689 - val_loss: 0.4140 - val_accuracy: 0.9024 - lr: 8.3058e-04\n",
      "Epoch 4/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5200 - accuracy: 0.8816 - val_loss: 0.3204 - val_accuracy: 0.9213 - lr: 7.8075e-04\n",
      "Epoch 5/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4661 - accuracy: 0.8878 - val_loss: 0.2995 - val_accuracy: 0.9212 - lr: 7.0267e-04\n",
      "Epoch 6/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.4197 - accuracy: 0.8960 - val_loss: 0.2757 - val_accuracy: 0.9268 - lr: 6.3241e-04\n",
      "Epoch 7/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3896 - accuracy: 0.9016 - val_loss: 0.2655 - val_accuracy: 0.9276 - lr: 5.6917e-04\n",
      "Epoch 8/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3632 - accuracy: 0.9047 - val_loss: 0.2460 - val_accuracy: 0.9326 - lr: 5.1225e-04\n",
      "Epoch 9/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3453 - accuracy: 0.9079 - val_loss: 0.2346 - val_accuracy: 0.9361 - lr: 4.0980e-04\n",
      "Epoch 10/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3307 - accuracy: 0.9105 - val_loss: 0.2339 - val_accuracy: 0.9367 - lr: 3.2784e-04\n",
      "Epoch 11/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3090 - accuracy: 0.9158 - val_loss: 0.2198 - val_accuracy: 0.9407 - lr: 2.6227e-04\n",
      "Epoch 12/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.3019 - accuracy: 0.9168 - val_loss: 0.2165 - val_accuracy: 0.9403 - lr: 2.0982e-04\n",
      "Epoch 13/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2949 - accuracy: 0.9205 - val_loss: 0.2114 - val_accuracy: 0.9423 - lr: 1.6785e-04\n",
      "Epoch 14/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2864 - accuracy: 0.9223 - val_loss: 0.2110 - val_accuracy: 0.9418 - lr: 1.3428e-04\n",
      "Epoch 15/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2829 - accuracy: 0.9215 - val_loss: 0.2086 - val_accuracy: 0.9433 - lr: 1.0743e-04\n",
      "Epoch 16/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2763 - accuracy: 0.9226 - val_loss: 0.2073 - val_accuracy: 0.9434 - lr: 8.5941e-05\n",
      "Epoch 17/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2743 - accuracy: 0.9226 - val_loss: 0.2055 - val_accuracy: 0.9432 - lr: 6.0159e-05\n",
      "Epoch 18/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2699 - accuracy: 0.9238 - val_loss: 0.2055 - val_accuracy: 0.9438 - lr: 4.2111e-05\n",
      "Epoch 19/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2670 - accuracy: 0.9255 - val_loss: 0.2057 - val_accuracy: 0.9441 - lr: 2.9478e-05\n",
      "Epoch 20/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2704 - accuracy: 0.9238 - val_loss: 0.2043 - val_accuracy: 0.9436 - lr: 1.7687e-05\n",
      "Epoch 21/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2668 - accuracy: 0.9265 - val_loss: 0.2053 - val_accuracy: 0.9438 - lr: 1.0612e-05\n",
      "Epoch 22/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2656 - accuracy: 0.9260 - val_loss: 0.2048 - val_accuracy: 0.9443 - lr: 6.3672e-06\n",
      "Epoch 23/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2647 - accuracy: 0.9259 - val_loss: 0.2046 - val_accuracy: 0.9444 - lr: 3.8203e-06\n",
      "Epoch 24/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2647 - accuracy: 0.9253 - val_loss: 0.2046 - val_accuracy: 0.9441 - lr: 2.2922e-06\n",
      "Epoch 25/60\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.2644 - accuracy: 0.9244 - val_loss: 0.2047 - val_accuracy: 0.9443 - lr: 1.3753e-06\n",
      "  1/313 [..............................] - ETA: 7s - loss: 0.0726 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_24136\\646761526.py:40: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model_5, 'model_5_preberttune.h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9436\n",
      "Accuracy for Stage 5 model: 0.9435508251190186\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "Classification Report for Stage 5 model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           2       0.00      0.00      0.00         1\n",
      "           4       0.33      0.50      0.40         2\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.81      0.94      0.87        18\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       1.00      0.25      0.40         4\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       0.78      1.00      0.88        71\n",
      "          14       1.00      0.80      0.89         5\n",
      "          15       0.95      1.00      0.97        18\n",
      "          16       0.00      0.00      0.00         4\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.92      0.72      0.81        32\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         2\n",
      "          31       0.50      1.00      0.67         6\n",
      "          32       0.80      0.44      0.57         9\n",
      "          33       0.00      0.00      0.00         4\n",
      "          34       0.60      0.90      0.72        10\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         1\n",
      "          37       0.58      0.65      0.61        17\n",
      "          38       0.67      0.67      0.67         3\n",
      "          39       0.76      0.91      0.83        65\n",
      "          40       1.00      1.00      1.00        12\n",
      "          41       0.00      0.00      0.00         1\n",
      "          43       0.50      1.00      0.67         6\n",
      "          44       0.43      0.60      0.50         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         1\n",
      "          50       1.00      0.69      0.82        13\n",
      "          51       1.00      0.50      0.67         2\n",
      "          53       0.86      0.86      0.86         7\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.57      0.93      0.70        14\n",
      "          58       0.88      0.98      0.92        88\n",
      "          59       1.00      1.00      1.00         1\n",
      "          60       0.00      0.00      0.00         8\n",
      "          61       1.00      1.00      1.00         3\n",
      "          63       0.62      0.71      0.67         7\n",
      "          64       1.00      0.73      0.84        11\n",
      "          66       0.00      0.00      0.00         5\n",
      "          69       1.00      1.00      1.00        14\n",
      "          70       0.97      0.99      0.98       140\n",
      "          71       0.57      0.92      0.71        13\n",
      "          73       0.90      0.93      0.91        28\n",
      "          74       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          76       0.00      0.00      0.00        13\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.72      1.00      0.84        13\n",
      "          79       1.00      0.50      0.67         4\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       0.50      0.86      0.63         7\n",
      "          83       0.62      1.00      0.77         5\n",
      "          84       0.00      0.00      0.00         3\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         9\n",
      "          87       0.00      0.00      0.00         1\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.80      1.00      0.89        16\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       1.00      0.98      0.99        54\n",
      "          92       0.00      0.00      0.00         6\n",
      "          93       0.75      1.00      0.86         6\n",
      "          94       0.00      0.00      0.00         3\n",
      "          95       0.00      0.00      0.00         1\n",
      "          96       0.00      0.00      0.00         5\n",
      "          99       0.90      1.00      0.95         9\n",
      "         100       0.81      1.00      0.90        13\n",
      "         101       0.00      0.00      0.00         1\n",
      "         103       0.00      0.00      0.00         1\n",
      "         104       0.00      0.00      0.00         5\n",
      "         106       1.00      0.60      0.75         5\n",
      "         107       0.33      0.50      0.40         2\n",
      "         108       0.49      1.00      0.66        19\n",
      "         109       0.75      0.75      0.75         4\n",
      "         111       0.60      0.60      0.60         5\n",
      "         112       0.00      0.00      0.00         2\n",
      "         113       0.62      1.00      0.76         8\n",
      "         114       0.95      1.00      0.97        54\n",
      "         115       1.00      0.33      0.50         3\n",
      "         117       1.00      0.83      0.91        12\n",
      "         118       1.00      0.50      0.67        10\n",
      "         119       0.55      1.00      0.71        11\n",
      "         120       0.50      0.17      0.25         6\n",
      "         121       1.00      0.62      0.77         8\n",
      "         122       0.20      1.00      0.33         2\n",
      "         124       0.00      0.00      0.00         2\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         6\n",
      "         127       0.75      1.00      0.86         6\n",
      "         128       0.64      0.82      0.72        22\n",
      "         130       0.58      0.44      0.50        16\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.79      0.73      0.76        15\n",
      "         134       0.56      1.00      0.71         5\n",
      "         135       0.00      0.00      0.00         7\n",
      "         138       0.00      0.00      0.00         1\n",
      "         139       0.00      0.00      0.00         4\n",
      "         141       0.88      0.78      0.82         9\n",
      "         143       0.94      0.94      0.94        17\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.91      1.00      0.95        10\n",
      "         147       1.00      1.00      1.00         6\n",
      "         148       0.00      0.00      0.00         1\n",
      "         149       0.00      0.00      0.00         1\n",
      "         150       0.45      1.00      0.62         5\n",
      "         151       0.00      0.00      0.00         9\n",
      "         153       0.91      1.00      0.95        30\n",
      "         155       0.00      0.00      0.00         1\n",
      "         157       1.00      0.33      0.50         3\n",
      "         158       0.82      1.00      0.90         9\n",
      "         159       0.00      0.00      0.00         1\n",
      "         160       0.53      0.82      0.64        11\n",
      "         162       1.00      1.00      1.00         1\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.65      0.92      0.76        12\n",
      "         166       0.00      0.00      0.00         1\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.57      0.57      0.57         7\n",
      "         169       0.71      1.00      0.83         5\n",
      "         170       1.00      1.00      1.00         2\n",
      "         171       0.00      0.00      0.00         1\n",
      "         173       0.12      0.33      0.18         3\n",
      "         174       0.33      0.50      0.40         2\n",
      "         175       0.87      1.00      0.93        13\n",
      "         177       0.83      1.00      0.91        10\n",
      "         178       0.64      1.00      0.78         9\n",
      "         180       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       1.00      0.25      0.40         4\n",
      "         186       1.00      1.00      1.00         1\n",
      "         189       1.00      1.00      1.00         2\n",
      "         190       0.00      0.00      0.00         2\n",
      "         191       0.00      0.00      0.00         2\n",
      "         192       0.50      1.00      0.67         2\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       0.86      1.00      0.92         6\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.88      0.93      0.90        15\n",
      "         197       1.00      0.67      0.80         3\n",
      "         198       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         200       0.91      1.00      0.95        10\n",
      "         201       0.39      1.00      0.56        20\n",
      "         202       0.00      0.00      0.00         2\n",
      "         203       0.86      0.75      0.80         8\n",
      "         204       0.94      1.00      0.97        29\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         2\n",
      "         208       0.45      0.83      0.59         6\n",
      "         210       0.76      1.00      0.87        13\n",
      "         211       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00        14\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       0.47      1.00      0.64        15\n",
      "         216       1.00      0.17      0.29        18\n",
      "         217       0.45      1.00      0.62         5\n",
      "         218       0.50      1.00      0.67         1\n",
      "         219       0.84      1.00      0.91        42\n",
      "         220       0.00      0.00      0.00         3\n",
      "         221       1.00      0.50      0.67         2\n",
      "         222       0.00      0.00      0.00         4\n",
      "         223       0.00      0.00      0.00         3\n",
      "         224       0.00      0.00      0.00         2\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       1.00      1.00      1.00         1\n",
      "         227       1.00      0.67      0.80         3\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.67      1.00      0.80         4\n",
      "         231       1.00      1.00      1.00         5\n",
      "         232       1.00      1.00      1.00         6\n",
      "         234       0.00      0.00      0.00         1\n",
      "         236       0.00      0.00      0.00         6\n",
      "         237       0.00      0.00      0.00         1\n",
      "         238       1.00      0.50      0.67         4\n",
      "         239       1.00      1.00      1.00         3\n",
      "         240       0.00      0.00      0.00         2\n",
      "         241       0.50      0.25      0.33         4\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.00      0.00      0.00         2\n",
      "         246       0.75      0.38      0.50         8\n",
      "         247       0.00      0.00      0.00         5\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.47      1.00      0.64        14\n",
      "         251       0.00      0.00      0.00         5\n",
      "         253       1.00      0.33      0.50         3\n",
      "         254       1.00      1.00      1.00         3\n",
      "         255       1.00      1.00      1.00         3\n",
      "         256       0.50      0.10      0.17        10\n",
      "         257       0.00      0.00      0.00         1\n",
      "         258       0.54      0.78      0.64         9\n",
      "         259       1.00      0.25      0.40         4\n",
      "         262       0.00      0.00      0.00         1\n",
      "         264       0.50      1.00      0.67         7\n",
      "         267       0.00      0.00      0.00         0\n",
      "         269       0.65      0.68      0.67        19\n",
      "         270       0.85      1.00      0.92        40\n",
      "         271       0.00      0.00      0.00         5\n",
      "         272       0.00      0.00      0.00         1\n",
      "         273       0.00      0.00      0.00         2\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.75      0.86      0.80        14\n",
      "         277       0.67      0.67      0.67         3\n",
      "         279       0.86      0.86      0.86         7\n",
      "         281       0.00      0.00      0.00         4\n",
      "         282       1.00      1.00      1.00         2\n",
      "         283       0.55      1.00      0.71         6\n",
      "         284       0.00      0.00      0.00         1\n",
      "         285       0.50      0.75      0.60         8\n",
      "         286       0.00      0.00      0.00         8\n",
      "         287       1.00      0.67      0.80         6\n",
      "         288       0.00      0.00      0.00         4\n",
      "         289       0.43      1.00      0.60        13\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         3\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.85      0.70      0.77        40\n",
      "         298       0.89      0.65      0.76        26\n",
      "         299       0.99      0.99      0.99      8119\n",
      "\n",
      "    accuracy                           0.94     10009\n",
      "   macro avg       0.43      0.45      0.42     10009\n",
      "weighted avg       0.93      0.94      0.93     10009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stage 5\n",
    "print(\"Running Stage 5 model...\")\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr * 0.94\n",
    "    elif epoch < 8:\n",
    "        return lr * 0.9\n",
    "    elif epoch < 16: \n",
    "        return lr * 0.80\n",
    "    elif epoch < 19:\n",
    "        return lr * 0.70\n",
    "    else:\n",
    "        return lr * 0.6\n",
    "\n",
    "# Define common callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "# Create model\n",
    "model_5 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_combined_4.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes_4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history_5 = model_5.fit(X_train_combined_4, y_train_4, epochs=60, batch_size=32,\n",
    "                        validation_data=(X_test_combined_4, y_test_4),\n",
    "                        callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Save model\n",
    "save_model(model_5, 'model_5_preberttune.h5')\n",
    "\n",
    "# Evaluate model\n",
    "loss_5, accuracy_5 = model_5.evaluate(X_test_combined_4, y_test_4)\n",
    "print(\"Accuracy for Stage 5 model:\", accuracy_5)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred_probabilities_5 = model_5.predict(X_test_combined_4)\n",
    "y_pred_5 = np.argmax(y_pred_probabilities_5, axis=1)\n",
    "report_5 = classification_report(y_test_4, y_pred_5)\n",
    "print(\"Classification Report for Stage 5 model:\")\n",
    "print(report_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "[[3.59479699e-08 5.56247635e-08 1.98854408e-08 ... 7.86569210e-09\n",
      "  2.36865683e-09 1.35191874e-06]\n",
      " [8.45505781e-08 9.05322040e-08 1.28690345e-07 ... 3.19916126e-07\n",
      "  5.32627553e-07 9.99980688e-01]\n",
      " [6.37860467e-07 5.78791287e-07 1.91184284e-07 ... 2.91431381e-04\n",
      "  3.00299707e-05 9.97268736e-01]\n",
      " [5.23321617e-08 6.82980215e-08 4.20930846e-08 ... 1.56526113e-07\n",
      "  7.98947575e-09 9.99991298e-01]\n",
      " [3.65704341e-06 3.15597049e-06 5.47987156e-06 ... 1.52147495e-05\n",
      "  1.42752060e-06 9.99792397e-01]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load all the models\n",
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "model_2 = load_model('model_2_preberttune.h5')\n",
    "model_3 = load_model('model_3_preberttune.h5')\n",
    "model_4 = load_model('model_4_preberttune.h5')\n",
    "model_5 = load_model('model_5_preberttune.h5')\n",
    "\n",
    "#Predictions\n",
    "y_pred_probabilities_1 = model_1.predict(X_test_combined)\n",
    "y_pred_probabilities_2 = model_2.predict(X_test_combined_1)\n",
    "y_pred_probabilities_3 = model_3.predict(X_test_combined_2)\n",
    "y_pred_probabilities_4 = model_4.predict(X_test_combined_3)\n",
    "y_pred_probabilities_5 = model_5.predict(X_test_combined_4)\n",
    "\n",
    "# Concatenated predictions\n",
    "concatenated_predictions = np.concatenate((y_pred_probabilities_1, y_pred_probabilities_2, y_pred_probabilities_3, y_pred_probabilities_4, y_pred_probabilities_5), axis=1)\n",
    "\n",
    "print(concatenated_predictions[:5]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "model_2 = load_model('model_2_preberttune.h5')\n",
    "\n",
    "\n",
    "user_input = np.array([[name, description, price, type, manufacturer]]) \n",
    "#Se le tiene que aplicar las funciones de normalizacion al input del usuario texto(name,desccription), usar scale para precio y one hot para categoricas.\n",
    "#usar label encoder para target category, usar bert para embeddings de las columnas de texto\n",
    "target_1 = [parent_category]\n",
    "\n",
    "#Preprocessed user input\n",
    "#                                                          Usar las categorias directo del df?\n",
    "#Main category pred\n",
    "main_category_pred_probs = model_1.predict(user_input)\n",
    "\n",
    "# get labels for main category\n",
    "main_category_pred_labels = np.argmax(main_category_pred_probs, axis=1) #Houseware [0.9,0.8,0.5,0.4]----> Houseware,0\n",
    "\n",
    "#Concat user input with label  and preprocess to enter second model\n",
    "#se tiene que aplicar one hot al output del primer modelo y usarlo como feature en el segundo, \n",
    "#se puede usar el output preprocesado del primero y solo unir con el one hot\n",
    "user_input_with_pred = np.concatenate((user_input, main_category_pred_labels.reshape(-1, 1)), axis=1) \n",
    "\n",
    "#Predict subcategory using model_2 with the concatenated input\n",
    "subcategory_pred_probs = model_2.predict(user_input_with_pred)\n",
    "\n",
    "# Getting index\n",
    "subcategory_pred_labels = np.argmax(subcategory_pred_probs, axis=1)\n",
    "\n",
    "# Print the subcategory prediction labels\n",
    "print(subcategory_pred_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
