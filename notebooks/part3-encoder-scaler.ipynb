{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional\n",
    "print(\"Importing...\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:53:06.350030Z",
     "start_time": "2024-03-18T22:53:06.344279Z"
    }
   },
   "id": "7cbe1ccd8938576a",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rcastillo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rcastillo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rcastillo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/rcastillo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "EDA dataset...\n",
      "                                              name      type  price  \\\n",
      "0                Duracell - AAA Batteries (4-Pack)  HardGood   5.49   \n",
      "1  Duracell - AA 1.5V CopperTop Batteries (4-Pack)  HardGood   5.49   \n",
      "2                 Duracell - AA Batteries (8-Pack)  HardGood   7.49   \n",
      "3            Energizer - MAX Batteries AA (4-Pack)  HardGood   4.99   \n",
      "4                  Duracell - C Batteries (4-Pack)  HardGood   8.99   \n",
      "\n",
      "                                         description manufacturer  \\\n",
      "0  Compatible with select electronic devices; AAA...     Duracell   \n",
      "1  Long-lasting energy; DURALOCK Power Preserve t...     Duracell   \n",
      "2  Compatible with select electronic devices; AA ...     Duracell   \n",
      "3  4-pack AA alkaline batteries; battery tester i...    Energizer   \n",
      "4  Compatible with select electronic devices; C s...     Duracell   \n",
      "\n",
      "                                           url              parent_category  \\\n",
      "0                duracell aaa batteries 4 pack  Connected Home & Housewares   \n",
      "1  duracell aa 1 5v coppertop batteries 4 pack  Connected Home & Housewares   \n",
      "2                 duracell aa batteries 8 pack  Connected Home & Housewares   \n",
      "3            energizer max batteries aa 4 pack  Connected Home & Housewares   \n",
      "4                  duracell c batteries 4 pack  Connected Home & Housewares   \n",
      "\n",
      "  sub_category_1        sub_category_2       sub_category_3 sub_category_4  \n",
      "0     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "1     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "2     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "3     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "4     Housewares   Household Batteries   Alkaline Batteries           <NA>  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50041 entries, 0 to 50040\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   name             50040 non-null  object \n",
      " 1   type             50041 non-null  object \n",
      " 2   price            50041 non-null  float64\n",
      " 3   description      50041 non-null  object \n",
      " 4   manufacturer     49975 non-null  object \n",
      " 5   url              50040 non-null  object \n",
      " 6   parent_category  50041 non-null  object \n",
      " 7   sub_category_1   49294 non-null  object \n",
      " 8   sub_category_2   43948 non-null  object \n",
      " 9   sub_category_3   27955 non-null  object \n",
      " 10  sub_category_4   9788 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 4.2+ MB\n",
      "X_train shape: (40032, 12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "file_name = \"alpha2_dataset.csv\"\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, '..', file_name)\n",
    "file_path = os.path.normpath(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(pd.NA)\n",
    "\n",
    "print(\"EDA dataset...\")\n",
    "print(df.head())\n",
    "df.info()\n",
    "\n",
    "# stemmer, lemmatizer and stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "normalization = ['name', 'description']\n",
    "for column in normalization:\n",
    "    df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "# print(df.shape)\n",
    "# print(df.iloc[0])\n",
    "# print(df.head())\n",
    "\n",
    "X = df.drop(columns=['parent_category'])\n",
    "y = df['parent_category']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "print(f'X_train shape: {X_train.shape}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:26:54.104136Z",
     "start_time": "2024-03-18T23:25:35.768151Z"
    }
   },
   "id": "637c433fbe3ef4d2",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "numerical_columns = ['price']\n",
    "text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# Fill missing values\n",
    "X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Print information about X_train_encoded and X_test_encoded\n",
    "print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Print information about X_train_scaled and X_test_scaled\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb441e66e0467671",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "print(\"Shape of X_test_scaled2:\", X_test_scaled.shape)\n",
    "print(\"Shape of X_test_encoded2:\", X_test_encoded.shape)\n",
    "\n",
    "print(\"saving\")\n",
    "dump(X_test_encoded, 'onehot_encoder_model_v1.joblib')\n",
    "dump(X_test_scaled, 'scaler_model_v1.joblib')\n",
    "\n",
    "print(\"loading\")\n",
    "X_test_encoded2 = load('onehot_encoder_model_v1.joblib')\n",
    "X_test_scaled2 = load('scaler_model_v1.joblib')\n",
    "\n",
    "print(\"Shape of X_test_scaled2:\", X_test_scaled2.shape)\n",
    "print(\"Data type of elements in X_test_scaled2:\", X_test_scaled2.dtype)\n",
    "\n",
    "print(\"Shape of X_test_encoded2:\", X_test_encoded2.shape)\n",
    "print(\"Data type of elements in X_test_encoded2:\", X_test_encoded2.dtype)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "493db116aa97b010",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  name description price product_type  \\\n",
      "0  6bd0fb8f-d850-4fb9-b8a7-5d8d3ad21be7  name  descripton   100     Software   \n",
      "\n",
      "  manufacturer  \n",
      "0     EnerPlex  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# JSON\n",
    "json_data = {\n",
    "    \"id\": \"6bd0fb8f-d850-4fb9-b8a7-5d8d3ad21be7\",\n",
    "    \"payload\": {\n",
    "        \"name\": \"name\",\n",
    "        \"description\": \"descripton\",\n",
    "        \"price\": \"100\",\n",
    "        \"product_type\": \"Software\",\n",
    "        \"manufacturer\": \"EnerPlex\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertir JSON a DataFrame\n",
    "df = pd.json_normalize(json_data)\n",
    "df.columns = df.columns.str.replace('payload.', '')\n",
    "# Imprimir el DataFrame\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:53:41.164420Z",
     "start_time": "2024-03-18T22:53:41.153734Z"
    }
   },
   "id": "6894438ff931ab94",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcastillo/.cache/pypoetry/virtualenvs/automated-product-categorization-for-e-com-ltcjCYoG-py3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\"\"\"\n",
    "\"sku\": 1003136,\n",
    "  \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "  \"type\": \"Software\",\n",
    "  \"price\": 29.99,\n",
    "  \"upc\": \"884088157449\",\n",
    "  \"category\": [\n",
    "    {\n",
    "      \"id\": \"abcat0207000\",\n",
    "      \"name\": \"Musical Instruments\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"pcmcat152100050020\",\n",
    "      \"name\": \"Recording Equipment\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"pcmcat152100050026\",\n",
    "      \"name\": \"Sound Recording Software\"\n",
    "    }\n",
    "  ],\n",
    "  \"shipping\": 5.49,\n",
    "  \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops for each instrument\",\n",
    "  \"manufacturer\": \"Hal Leonard\",\n",
    "  \"model\": \"631386\",\n",
    "\n",
    "\"\"\"\n",
    "user_input = {\n",
    "    \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "    \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops \",\n",
    "    \"price\": 29.99,\n",
    "    \"type\": \"Software\",\n",
    "    \"manufacturer\": \"Hal Leonard\",\n",
    "}\n",
    "\n",
    "\n",
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:23.859665Z",
     "start_time": "2024-03-18T23:32:01.566314Z"
    }
   },
   "id": "fed8458c82aa984e",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "def extract_last_hidden_state(embeddings):\n",
    "    return embeddings[:, -1, :]\n",
    "\n",
    "def tokenize_and_get_embeddings(column):\n",
    "    # Tokenize text\n",
    "    if isinstance(column, str):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    elif isinstance(column, list) and all(isinstance(item, str) for item in column):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input format.\")\n",
    "\n",
    "    outputs = model(tokenized_text)\n",
    "    embeddings = outputs.last_hidden_state.numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def prepare_input(user_input, scaler, encoder, categorical_columns):\n",
    "\n",
    "    user_input['name'] = normalize_text(user_input['name'])\n",
    "    user_input['description'] = normalize_text(user_input['description'])\n",
    "\n",
    "    name_embeddings = tokenize_and_get_embeddings(user_input['name'])\n",
    "    description_embeddings = tokenize_and_get_embeddings(user_input['description'])\n",
    "    extracted_name_hidden = extract_last_hidden_state(name_embeddings)\n",
    "    extracted_description_hidden = extract_last_hidden_state(description_embeddings)\n",
    "    scaled_price = scaler.transform([[user_input['price']]])[0, 0]\n",
    "\n",
    "    encoded_user_input = [[user_input[column]] for column in categorical_columns]\n",
    "    encoded_user_input = np.array(encoded_user_input).reshape(1, -1)\n",
    "    encoded_categorical_features = encoder.transform(encoded_user_input)\n",
    "    print(encoded_categorical_features.shape)\n",
    "\n",
    "    combined_features = hstack([encoded_categorical_features, np.array([[scaled_price]])])\n",
    "    print(combined_features.shape)\n",
    "    num_cat_input_array = combined_features.toarray().astype(np.float32)\n",
    "    final_input_array = np.concatenate((num_cat_input_array,extracted_name_hidden,extracted_description_hidden), axis=1)\n",
    "\n",
    "    return name_embeddings, description_embeddings, final_input_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:30.709406Z",
     "start_time": "2024-03-18T23:33:30.702890Z"
    }
   },
   "id": "6cdcd68fde14962e",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2195)\n",
      "(1, 2196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcastillo/.cache/pypoetry/virtualenvs/automated-product-categorization-for-e-com-ltcjCYoG-py3.11/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/rcastillo/.cache/pypoetry/virtualenvs/automated-product-categorization-for-e-com-ltcjCYoG-py3.11/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "name_embeddings, description_embeddings, final_input_array = prepare_input(user_input, scaler, encoder, categorical_columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:39.994697Z",
     "start_time": "2024-03-18T23:33:39.125943Z"
    }
   },
   "id": "2655cdeccfa2028b",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "[[5.9872679e-07 4.8258988e-07 2.9880678e-07 1.6442855e-06 1.5122211e-07\n",
      "  1.8521788e-04 1.9810813e-07 3.2663820e-07 8.2353949e-08 3.8066194e-06\n",
      "  4.4644270e-07 2.1809466e-05 1.0719603e-05 4.8484844e-07 7.7668361e-08\n",
      "  1.5808581e-04 1.5995936e-02 1.4492360e-06 2.0094559e-07 4.1292154e-07\n",
      "  4.3375762e-06 7.1777839e-07 2.0001869e-07 1.1779769e-06 3.9620249e-07\n",
      "  1.1999542e-06 5.8523278e-07 1.7006349e-07 7.8968235e-08 1.8490706e-06\n",
      "  1.3293675e-06 4.5076538e-07 6.5526649e-07 3.5007156e-07 6.3509577e-07\n",
      "  5.8573228e-06 3.4087401e-07 3.8726887e-07 9.5410940e-07 1.4988204e-06\n",
      "  3.2991964e-06 1.5742604e-05 1.7435389e-04 9.8315400e-01 1.7742989e-04\n",
      "  8.2149569e-07 6.5203687e-07 1.0753408e-06 3.0232496e-07 7.6895084e-08\n",
      "  5.1515372e-07 1.7579384e-07 1.4305623e-06 3.7233913e-05 2.7063729e-06\n",
      "  1.3869836e-06 2.6501255e-07 2.0228770e-05 1.2311910e-06 2.0472783e-07\n",
      "  6.6331438e-07 5.5620706e-07]]\n",
      "[43]\n",
      "[43]\n",
      "['Analog Audio Cables']\n"
     ]
    }
   ],
   "source": [
    "categories_dict = {\n",
    "    'parent_category': {category: index for index, category in enumerate(df['parent_category'].unique())},\n",
    "    'sub_category_1': {category: index for index, category in enumerate(df['sub_category_1'].unique())},\n",
    "    'sub_category_2': {category: index for index, category in enumerate(df['sub_category_2'].unique())},\n",
    "    'sub_category_3': {category: index for index, category in enumerate(df['sub_category_3'].unique())},\n",
    "    'sub_category_4': {category: index for index, category in enumerate(df['sub_category_4'].unique())},\n",
    "}\n",
    "user_input = {\n",
    "    \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "    \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops \",\n",
    "    \"price\": 29.99,\n",
    "    \"type\": \"Software\",\n",
    "    \"manufacturer\": \"Hal Leonard\",\n",
    "}\n",
    "\n",
    "\n",
    "def predict_model_1(final_input_array, model_1):\n",
    "    predictions = model_1.predict(final_input_array)  # creo que este es el modelo correspondiente // acá cargo el h5 vector\n",
    "    subcategory_pred_labels = np.argmax(predictions, axis=1)\n",
    "    print(predictions)\n",
    "    print(subcategory_pred_labels)\n",
    "    return subcategory_pred_labels\n",
    "\n",
    "def compare_predictions(subcategory_pred_labels, categories_dict, category_type):\n",
    "    mapping = {v: k for k, v in categories_dict[category_type].items()}\n",
    "    predicted_labels = [mapping[idx] for idx in subcategory_pred_labels]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "predicted_labels = predict_model_1(final_input_array, model_1)\n",
    "predicted_labels_2 = compare_predictions(predicted_labels, categories_dict, 'parent_category')\n",
    "\n",
    "print(predicted_labels)\n",
    "print(predicted_labels_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:36:05.565932Z",
     "start_time": "2024-03-18T23:36:05.469205Z"
    }
   },
   "id": "61ccb220ea9e7a08",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d650a4bf8ef760c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
