This notebook illustrates the application of the BERT model for text classification, utilizing advanced natural language processing and machine learning techniques. Below are the sections detailed in the order they appear in the file:

Library Installation (FT1):
The project begins with the installation of essential libraries such as transformers, torch, and accelerate, which are fundamental for using BERT and PyTorch.

Import and Configuration of BERT (FT2):
BertTokenizer and BertForSequenceClassification from the transformers library are imported, and the BERT tokenizer is loaded with the bert-base-uncased configuration.

Creation of the Dataset Class for BERT (FT3):
A custom Dataset class is defined to adapt data to the format required by BERT, facilitating the handling of text encodings and corresponding labels.

Combining Text Columns (FT4):
The 'name' and 'description' columns in X_train and X_test are combined into a new 'combined_text' column, using fillna('') to handle null values. This provides a more comprehensive context for the BERT model.

Tokenization and Creation of Training and Test Datasets (FT5):
The training and test sets are tokenized using the BERT tokenizer, preparing them for use in the classification model.

Configuration and Training of the BERT Model (FT6):
A pre-trained BERT model is configured for classification, training arguments are set up, and compute_metrics is defined to calculate metrics. The Trainer class is used for training.

Model Saving (FT7):
After training, the adjusted model is saved to a specified path.

Evaluation and Prediction (FT8):
The performance of the model is evaluated, and predictions are made on new texts.
Data Preparation

Between sections FT2 and FT3, the file handles data preparation and cleaning using pandas and nltk, including tokenization and lemmatization.

Additional Processes:

The notebook includes additional training and prediction processes for different categories, following a similar structure of data preparation, tokenization, model configuration, training, and prediction.

