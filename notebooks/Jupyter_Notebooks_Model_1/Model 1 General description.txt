1.-Data Loading and Preprocessing:

Loads a dataset from a CSV file (alpha2_dataset_cleaned.csv) using pandas.
Performs basic data cleaning by filling missing values with pd.NA.
Normalizes text data by removing extra new lines, extra whitespace, special characters, and applying lemmatization using NLTK.
Splits the dataset into features (X) and target labels (y).

Encoding Categorical Features:
Encodes categorical features using one-hot encoding and numerical features using standard scaling.
Saves the encoders and scalers using pickle.

Label encoding:
Label encodes target column for each model.

2.-Model Training:

Trains multiple neural network models using TensorFlow/Keras.
The models are trained in stages, each time incorporating additional features from the dataset.
Each stage includes compiling the model, defining callbacks (early stopping and learning rate scheduling), fitting the model on the training data, and evaluating its performance on the test data.
The models are saved after training.
3.-Evaluation and Reporting:

After training each model, it evaluates the model's performance using accuracy and generates a classification report.

4.-.-File Saving:

Saves the processed feature arrays and models for later use.
5.-BERT Embeddings (Optional):

There's a  section for generating BERT embeddings, overall, as the same words were present in each dataset, we used the same embeddings for each of the models for efficiency.