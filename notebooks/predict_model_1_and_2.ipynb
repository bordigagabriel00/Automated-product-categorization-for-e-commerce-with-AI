{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cbe1ccd8938576a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:53:06.350030Z",
     "start_time": "2024-03-18T22:53:06.344279Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional\n",
    "print(\"Importing...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637c433fbe3ef4d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:26:54.104136Z",
     "start_time": "2024-03-18T23:25:35.768151Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# file_name = \"alpha2_dataset_cleaned.csv\"\n",
    "\n",
    "# print(\"Downloading dataset...\")\n",
    "\n",
    "\n",
    "# file_path = r'C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\assignment\\alpha2_dataset_cleaned.csv'\n",
    "\n",
    "# df = pd.read_csv(file_path)\n",
    "# df = df.fillna(pd.NA)\n",
    "\n",
    "# print(\"EDA dataset...\")\n",
    "# print(df.head())\n",
    "# df.info()\n",
    "\n",
    "# X = df.drop(columns=['parent_category'])\n",
    "# y = df['parent_category']\n",
    "# stemmer, lemmatizer and stopwords\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun if not recognized\n",
    "\n",
    "\n",
    "def remove_extra_new_lines(text):\n",
    "    if pd.isnull(text):  # check if text is nan\n",
    "        return ''  # replace with an empty string\n",
    "\n",
    "    clean_text = [i for i in str(text).splitlines() if i.strip()]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    spaceless_text = re.sub(r'\\s+', ' ', text)\n",
    "    return spaceless_text\n",
    "\n",
    "\n",
    "def remove_special_chars(text: str, remove_digits: Optional[bool] = False) -> str:\n",
    "    if remove_digits:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_extra_new_lines(text)\n",
    "\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    text = remove_special_chars(text, remove_digits=False)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "# normalization = ['name', 'description']\n",
    "# for column in normalization:\n",
    "#     df[column + '_normalized'] = df[column].apply(normalize_text)\n",
    "\n",
    "# print(df.shape)\n",
    "# print(df.iloc[0])\n",
    "# print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb441e66e0467671",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define columns\n",
    "categorical_columns = ['type', 'manufacturer']\n",
    "# numerical_columns = ['price']\n",
    "# text_columns = ['name_normalized', 'description_normalized']\n",
    "\n",
    "# # Fill missing values\n",
    "# X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')\n",
    "# X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')\n",
    "\n",
    "# # One-hot encode categorical features\n",
    "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "# X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# # Print information about X_train_encoded and X_test_encoded\n",
    "# print(\"Shape of X_train_encoded:\", X_train_encoded.shape)\n",
    "# print(\"Data type of X_train_encoded:\", type(X_train_encoded))\n",
    "# print(\"Data type of elements in X_train_encoded:\", X_train_encoded.dtype)\n",
    "# print(\"Shape of X_test_encoded:\", X_test_encoded.shape)\n",
    "# print(\"Data type of X_test_encoded:\", type(X_test_encoded))\n",
    "# print(\"Data type of elements in X_test_encoded:\", X_test_encoded.dtype)\n",
    "\n",
    "# # Scale numerical features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "# X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# # Print information about X_train_scaled and X_test_scaled\n",
    "# print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "# print(\"Data type of elements in X_test_scaled:\", X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "493db116aa97b010",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "\n",
    "# print(\"Shape of X_test_scaled2:\", X_test_scaled.shape)\n",
    "# print(\"Shape of X_test_encoded2:\", X_test_encoded.shape)\n",
    "\n",
    "# print(\"saving\")\n",
    "# dump(X_test_encoded, 'onehot_encoder_model_v1.joblib')\n",
    "# dump(X_test_scaled, 'scaler_model_v1.joblib')\n",
    "\n",
    "# print(\"loading\")\n",
    "# X_test_encoded2 = load('onehot_encoder_model_v1.joblib')\n",
    "# X_test_scaled2 = load('scaler_model_v1.joblib')\n",
    "\n",
    "# print(\"Shape of X_test_scaled2:\", X_test_scaled2.shape)\n",
    "# print(\"Data type of elements in X_test_scaled2:\", X_test_scaled2.dtype)\n",
    "\n",
    "# print(\"Shape of X_test_encoded2:\", X_test_encoded2.shape)\n",
    "# print(\"Data type of elements in X_test_encoded2:\", X_test_encoded2.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6894438ff931ab94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:53:41.164420Z",
     "start_time": "2024-03-18T22:53:41.153734Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  name description price product_type  \\\n",
      "0  6bd0fb8f-d850-4fb9-b8a7-5d8d3ad21be7  name  descripton   100     Software   \n",
      "\n",
      "  manufacturer  \n",
      "0     EnerPlex  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# JSON\n",
    "json_data = {\n",
    "    \"id\": \"6bd0fb8f-d850-4fb9-b8a7-5d8d3ad21be7\",\n",
    "    \"payload\": {\n",
    "        \"name\": \"name\",\n",
    "        \"description\": \"descripton\",\n",
    "        \"price\": \"100\",\n",
    "        \"product_type\": \"Software\",\n",
    "        \"manufacturer\": \"EnerPlex\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertir JSON a DataFrame\n",
    "df = pd.json_normalize(json_data)\n",
    "df.columns = df.columns.str.replace('payload.', '')\n",
    "# Imprimir el DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fed8458c82aa984e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:23.859665Z",
     "start_time": "2024-03-18T23:32:01.566314Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "\"sku\": 1003136,\n",
    "  \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "  \"type\": \"Software\",\n",
    "  \"price\": 29.99,\n",
    "  \"upc\": \"884088157449\",\n",
    "  \"category\": [\n",
    "    {\n",
    "      \"id\": \"abcat0207000\",\n",
    "      \"name\": \"Musical Instruments\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"pcmcat152100050020\",\n",
    "      \"name\": \"Recording Equipment\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"pcmcat152100050026\",\n",
    "      \"name\": \"Sound Recording Software\"\n",
    "    }\n",
    "  ],\n",
    "  \"shipping\": 5.49,\n",
    "  \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops for each instrument\",\n",
    "  \"manufacturer\": \"Hal Leonard\",\n",
    "  \"model\": \"631386\",\n",
    "\n",
    "\"\"\"\n",
    "user_input = {\n",
    "    \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "    \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops \",\n",
    "    \"price\": 29.99,\n",
    "    \"type\": \"Software\",\n",
    "    \"manufacturer\": \"Hal Leonard\",\n",
    "}\n",
    "\n",
    "with open('encoder.pkl', 'rb') as file:\n",
    "    encoder = pickle.load(file)\n",
    "    \n",
    "with open('scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "    \n",
    "with h5py.File('label_encoder.h5', 'r') as hf:\n",
    "    label_encoder_classes = hf['label_encoder'][:]\n",
    "\n",
    "\n",
    "model_1 = load_model('model_1_preberttune.h5')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cdcd68fde14962e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:30.709406Z",
     "start_time": "2024-03-18T23:33:30.702890Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "def extract_last_hidden_state(embeddings):\n",
    "    return embeddings[:, -1, :]\n",
    "\n",
    "def tokenize_and_get_embeddings(column):\n",
    "    # Tokenize text\n",
    "    if isinstance(column, str):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    elif isinstance(column, list) and all(isinstance(item, str) for item in column):\n",
    "        tokenized_text = tokenizer(column, padding=True, truncation=True, return_tensors='tf')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input format.\")\n",
    "\n",
    "    outputs = model(tokenized_text)\n",
    "    embeddings = outputs.last_hidden_state.numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def prepare_input(user_input, scaler, encoder, categorical_columns):\n",
    "\n",
    "    user_input['name'] = normalize_text(user_input['name'])\n",
    "    user_input['description'] = normalize_text(user_input['description'])\n",
    "\n",
    "    name_embeddings = tokenize_and_get_embeddings(user_input['name'])\n",
    "    description_embeddings = tokenize_and_get_embeddings(user_input['description'])\n",
    "    extracted_name_hidden = extract_last_hidden_state(name_embeddings)\n",
    "    extracted_description_hidden = extract_last_hidden_state(description_embeddings)\n",
    "    scaled_price = scaler.transform([[user_input['price']]])[0, 0]\n",
    "\n",
    "    encoded_user_input = [[user_input[column]] for column in categorical_columns]\n",
    "    encoded_user_input = np.array(encoded_user_input).reshape(1, -1)\n",
    "    encoded_categorical_features = encoder.transform(encoded_user_input)\n",
    "    print(encoded_categorical_features.shape)\n",
    "\n",
    "    combined_features = hstack([encoded_categorical_features, np.array([[scaled_price]])])\n",
    "    print(combined_features.shape)\n",
    "    num_cat_input_array = combined_features.toarray().astype(np.float32)\n",
    "    final_input_array = np.concatenate((num_cat_input_array,extracted_name_hidden,extracted_description_hidden), axis=1)\n",
    "\n",
    "    return extracted_name_hidden, extracted_description_hidden, final_input_array, scaled_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2655cdeccfa2028b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:33:39.994697Z",
     "start_time": "2024-03-18T23:33:39.125943Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2195)\n",
      "(1, 2196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "extracted_name_hidden, extracted_description_hidden, final_input_array, scaled_price = prepare_input(user_input, scaler, encoder, categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61ccb220ea9e7a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:36:05.565932Z",
     "start_time": "2024-03-18T23:36:05.469205Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# categories_dict = {\n",
    "#     'parent_category': {category: index for index, category in enumerate(df['parent_category'].unique())},\n",
    "#     'sub_category_1': {category: index for index, category in enumerate(df['sub_category_1'].unique())},\n",
    "#     'sub_category_2': {category: index for index, category in enumerate(df['sub_category_2'].unique())},\n",
    "#     'sub_category_3': {category: index for index, category in enumerate(df['sub_category_3'].unique())},\n",
    "#     'sub_category_4': {category: index for index, category in enumerate(df['sub_category_4'].unique())},\n",
    "# }\n",
    "# user_input = {\n",
    "#     \"name\": \"1970s Rock TrackPak - Mac\",\n",
    "#     \"description\": \"HAL LEONARD 1970s Rock TrackPak: Features 12 classic rock songs; compatible with GarageBand; includes loops \",\n",
    "#     \"price\": 29.99,\n",
    "#     \"type\": \"Software\",\n",
    "#     \"manufacturer\": \"Hal Leonard\",\n",
    "# }\n",
    "\n",
    "\n",
    "def predict_model_1(final_input_array, model_1):\n",
    "    predictions = model_1.predict(final_input_array)  # creo que este es el modelo correspondiente // acá cargo el h5 vector\n",
    "    subcategory_pred_labels = np.argmax(predictions, axis=1)\n",
    "    print(predictions)\n",
    "    print(subcategory_pred_labels)\n",
    "    return subcategory_pred_labels\n",
    "\n",
    "def compare_predictions(subcategory_pred_labels, label_encoder_classes):\n",
    "    predicted_labels = []\n",
    "    for idx in subcategory_pred_labels:\n",
    "        if idx < len(label_encoder_classes):\n",
    "            predicted_labels.append(label_encoder_classes[idx])\n",
    "        else:\n",
    "            predicted_labels.append('unknown')  \n",
    "    return predicted_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d650a4bf8ef760c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[2.0316011e-06 3.6132272e-04 2.6377302e-07 8.9121377e-06 2.1533744e-05\n",
      "  4.8780334e-05 2.0462597e-02 6.9033308e-06 7.0857214e-07 3.8897153e-07\n",
      "  1.6465702e-06 2.8329794e-07 2.2558066e-05 4.6699761e-06 2.2832181e-05\n",
      "  1.6142009e-04 9.7844636e-01 2.9636937e-04 3.5501547e-05 2.2440856e-06\n",
      "  8.4397135e-07 1.9531635e-05 7.2272531e-05]]\n",
      "[16]\n",
      "[16]\n",
      "[b'Musical Instruments']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = predict_model_1(final_input_array, model_1)\n",
    "predicted_labels_2 = compare_predictions(predicted_labels, label_encoder_classes)\n",
    "\n",
    "print(predicted_labels)\n",
    "print(predicted_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44adad6e-a24a-40f1-a68f-fea868e3487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Musical Instruments']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f65aa163-c233-4e54-abe8-e06305030fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = load_model('model_2_preberttune.h5')\n",
    "\n",
    "decoded_labels = [label.decode() for label in predicted_labels_2]\n",
    "\n",
    "with open('encoder_1.pkl', 'rb') as file:\n",
    "    encoder_1 = pickle.load(file)\n",
    "\n",
    "with open('scaler_1.pkl', 'rb') as file:\n",
    "    scaler_1 = pickle.load(file)\n",
    "    \n",
    "with h5py.File('label_encoder_1.h5', 'r') as hf:\n",
    "    label_encoder_classes_1 = hf['label_encoder_1'][:]\n",
    "\n",
    "def prepare_input_2(user_input, encoder_1, decoded_labels, scaler_1):\n",
    "    \n",
    "\n",
    "    user_input['name'] = normalize_text(user_input['name'])\n",
    "    user_input['description'] = normalize_text(user_input['description'])\n",
    "    name_embeddings = tokenize_and_get_embeddings(user_input['name'])\n",
    "    description_embeddings = tokenize_and_get_embeddings(user_input['description'])\n",
    "    extracted_name_hidden = extract_last_hidden_state(name_embeddings)\n",
    "    extracted_description_hidden = extract_last_hidden_state(description_embeddings)\n",
    "\n",
    "    input_data = np.array([[user_input['type'], user_input['manufacturer']]])\n",
    "    predicted_labels_array = np.array(decoded_labels)[:3].reshape(1, -1)  \n",
    "    input_with_labels = np.hstack((input_data, predicted_labels_array))\n",
    "    \n",
    "\n",
    "    predicted_labels_one_hot = encoder_1.transform(input_with_labels)\n",
    "    \n",
    "\n",
    "    scaled_price_array = scaler_1.transform(np.array(user_input['price']).reshape(-1, 1))\n",
    "    \n",
    "    final_input_array_with_label = np.hstack((predicted_labels_one_hot.toarray(), scaled_price_array))\n",
    "    final_input_array_2 = np.concatenate((final_input_array_with_label, extracted_name_hidden, extracted_description_hidden), axis=1)\n",
    "    \n",
    "    return final_input_array_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "563bfe63-1943-41e0-af83-ff9d79c62a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_input_array_with_label_1 = prepare_input_2(user_input, encoder_1, decoded_labels, scaler_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "22d66894-835f-40fb-9299-b6257d47c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3755)\n"
     ]
    }
   ],
   "source": [
    "print(final_input_array_with_label_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "640460f3-5f37-4399-9c23-563641601928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_2(final_input_array_with_label_1, model_2):\n",
    "    predictions_1 = model_2.predict(final_input_array_with_label_1)\n",
    "    subcategory_pred_labels = np.argmax(predictions_1, axis=1)\n",
    "    print(predictions_1)\n",
    "    print(subcategory_pred_labels)\n",
    "    return subcategory_pred_labels\n",
    "\n",
    "def compare_predictions_2(subcategory_pred_labels, label_encoder_classes_1):\n",
    "    predicted_labels = []\n",
    "    for idx in subcategory_pred_labels:\n",
    "        if idx < len(label_encoder_classes_1):\n",
    "            predicted_labels.append(label_encoder_classes_1[idx])\n",
    "        else:\n",
    "            predicted_labels.append('unknown')  \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ac5ed18-461f-40eb-8a89-4cb9397e66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[3.6844229e-11 4.6887962e-04 4.8117687e-07 8.0186580e-10 8.8447871e-09\n",
      "  4.6524487e-08 2.0267258e-03 7.9313287e-07 5.7016885e-09 3.6593756e-05\n",
      "  3.0031963e-10 9.1216970e-07 4.0887610e-07 2.5195914e-07 1.0286867e-07\n",
      "  7.2678718e-06 5.0219811e-09 1.0779967e-07 7.1669768e-09 6.2127228e-08\n",
      "  1.0215353e-07 7.3317923e-12 6.2465574e-06 1.7766823e-07 1.6859884e-05\n",
      "  6.1551731e-07 5.1945452e-02 1.9249156e-07 2.0081163e-08 1.8394931e-07\n",
      "  1.5723835e-07 2.3924921e-08 5.2110154e-05 9.3451696e-08 1.1724555e-07\n",
      "  9.9461450e-10 6.0566734e-05 1.5919449e-08 2.5950538e-07 6.5691905e-09\n",
      "  8.2239580e-07 6.3074754e-07 3.0798282e-07 4.2065822e-06 2.9632092e-05\n",
      "  2.6438315e-08 5.4358775e-06 6.8670921e-08 5.1650879e-08 3.9680511e-08\n",
      "  6.7174363e-07 6.1103520e-08 1.3120189e-07 1.5817957e-03 9.4291294e-04\n",
      "  6.1525314e-09 1.3213876e-09 7.6963763e-10 3.5936068e-07 8.4888432e-03\n",
      "  2.1020906e-10 2.3361789e-08 5.4652620e-08 2.7075650e-05 1.9599214e-02\n",
      "  6.5320705e-07 8.2337165e-06 1.5607446e-06 7.0949656e-11 6.1895793e-07\n",
      "  3.7548232e-06 4.3814782e-07 2.9180912e-06 3.2194428e-05 4.1297564e-05\n",
      "  9.8197661e-06 1.9411660e-05 3.3048040e-05 4.0317176e-05 6.4645269e-07\n",
      "  4.2698648e-06 1.4559953e-06 1.0592020e-07 8.1173795e-01 7.5630140e-08\n",
      "  1.8971801e-10 7.4062458e-11 4.7186259e-06 9.7782016e-02 5.4641369e-08\n",
      "  4.2521460e-03 1.4246061e-06 1.0363148e-07 9.8432447e-06 1.3152467e-06\n",
      "  3.6700836e-09 6.5227482e-08 3.7294811e-08 2.8822782e-07 1.1160511e-08\n",
      "  6.3828304e-11 2.4984308e-08 2.4182993e-06 6.8818529e-09 1.2651836e-06\n",
      "  2.1814660e-06 4.4285032e-05 2.1939172e-06 1.7483551e-09 6.7665006e-11\n",
      "  6.0721683e-10 5.2864428e-09 6.3604367e-04 1.2564200e-05]]\n",
      "[83]\n",
      "[b' Recording Equipment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\OWNER\\Desktop\\AnyoneAI\\final_project\\venv2\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_input_array_with_label_1 = prepare_input_2(user_input, encoder_1, decoded_labels, scaler_1)\n",
    "subcategory_pred_labels = predict_model_2(final_input_array_with_label_1, model_2)\n",
    "predicted_labels = compare_predictions_2(subcategory_pred_labels, label_encoder_classes_1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f5ab2-e22d-482c-bcb8-cc19955cbaea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
